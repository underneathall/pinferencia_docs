{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"background/models/home/","text":"Model? \u00b6 What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"About Models"},{"location":"background/models/home/#model","text":"What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"Model?"},{"location":"get-started/install/","text":"Install Pinferencia \u00b6 Recommended \u00b6 It's recommended to install Pinferencia with streamlit . You will have the full power of Pinferencia . $ pip install \"pinferencia[streamlit]\" ---> 100% Alternatively \u00b6 You can also choose install Pinferencia without streamlit , in this mode, you can only run backend of pinferencia. $ pip install pinferencia ---> 100%","title":"Install"},{"location":"get-started/install/#install-pinferencia","text":"","title":"Install Pinferencia"},{"location":"get-started/install/#recommended","text":"It's recommended to install Pinferencia with streamlit . You will have the full power of Pinferencia . $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Recommended"},{"location":"get-started/install/#alternatively","text":"You can also choose install Pinferencia without streamlit , in this mode, you can only run backend of pinferencia. $ pip install pinferencia ---> 100%","title":"Alternatively"},{"location":"get-started/introduction/","text":"Get Started \u00b6 In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and some fun with the MNIST model","title":"Introduction"},{"location":"get-started/introduction/#get-started","text":"In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and some fun with the MNIST model","title":"Get Started"},{"location":"get-started/other-models/","text":"What's Next \u00b6 Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"get-started/other-models/#whats-next","text":"Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"get-started/pytorch-mnist/","text":"Serve PyTorch MNIST Model \u00b6 In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response. Prerequisite \u00b6 Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt Deploy Methods \u00b6 There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial. Directly Register a Function \u00b6 Create the App \u00b6 Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO! Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Frontend UI \u00b6 Open http://127.0.0.1:8501 , and the template Image to Text will be selected automatically. Use the image below: You will get: Backend API \u00b6 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1:8000 Register a Model Path, with a Handler \u00b6 Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers Create the App \u00b6 Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction. Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Run the test: $ python test.py Prediction: 4 No suprise, the same result. Finally \u00b6 Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model.","title":"Serve PyTorch MNIST Model"},{"location":"get-started/pytorch-mnist/#serve-pytorch-mnist-model","text":"In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response.","title":"Serve PyTorch MNIST Model"},{"location":"get-started/pytorch-mnist/#prerequisite","text":"Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"Prerequisite"},{"location":"get-started/pytorch-mnist/#deploy-methods","text":"There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial.","title":"Deploy Methods"},{"location":"get-started/pytorch-mnist/#directly-register-a-function","text":"","title":"Directly Register a Function"},{"location":"get-started/pytorch-mnist/#create-the-app","text":"Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO!","title":"Create the App"},{"location":"get-started/pytorch-mnist/#start-the-service","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"get-started/pytorch-mnist/#test-the-service","text":"Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4","title":"Test the Service"},{"location":"get-started/pytorch-mnist/#frontend-ui","text":"Open http://127.0.0.1:8501 , and the template Image to Text will be selected automatically. Use the image below: You will get:","title":"Frontend UI"},{"location":"get-started/pytorch-mnist/#backend-api","text":"Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1:8000","title":"Backend API"},{"location":"get-started/pytorch-mnist/#register-a-model-path-with-a-handler","text":"Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers","title":"Register a Model Path, with a Handler"},{"location":"get-started/pytorch-mnist/#create-the-app_1","text":"Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction.","title":"Create the App"},{"location":"get-started/pytorch-mnist/#start-the-service_1","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"get-started/pytorch-mnist/#test-the-service_1","text":"Run the test: $ python test.py Prediction: 4 No suprise, the same result.","title":"Test the Service"},{"location":"get-started/pytorch-mnist/#finally","text":"Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model.","title":"Finally"},{"location":"get-started/pytorch-mnist/bonus/","text":"Bonus \u00b6 If you still have time, let's try something fun. Extra: Sum Up the MNIST Images \u00b6 Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. Here, we first create a custom frontend template accepting two MNIST images and send them back to our backend for prediction. Custom Frontend \u00b6 How to Custom Template? You can find more info at Custom Templates . Custom Template \u00b6 First, we need a new template: sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) Here we split the content panel into two columns, each accepts a single MNIST image. Once the image is uploaded, append it to the image array for later prediction. Once the image is uploaded, append it to the image array for later prediction. If both images are uploaded, send them to backend to predict. Custom Frontend \u00b6 Based on the custom template file, we add some extra lines to define the custom frontend service. sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, ) Backend \u00b6 After we customize the frontend, we can directly use our custom template during model registration. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) Here we pre-process each image, predict its digit and sum up. Register our new template Sum Mnist as the default template. Start the Service \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Have fun with Pinferencia !","title":"MNIST Bonus"},{"location":"get-started/pytorch-mnist/bonus/#bonus","text":"If you still have time, let's try something fun.","title":"Bonus"},{"location":"get-started/pytorch-mnist/bonus/#extra-sum-up-the-mnist-images","text":"Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. Here, we first create a custom frontend template accepting two MNIST images and send them back to our backend for prediction.","title":"Extra: Sum Up the MNIST Images"},{"location":"get-started/pytorch-mnist/bonus/#custom-frontend","text":"How to Custom Template? You can find more info at Custom Templates .","title":"Custom Frontend"},{"location":"get-started/pytorch-mnist/bonus/#custom-template","text":"First, we need a new template: sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) Here we split the content panel into two columns, each accepts a single MNIST image. Once the image is uploaded, append it to the image array for later prediction. Once the image is uploaded, append it to the image array for later prediction. If both images are uploaded, send them to backend to predict.","title":"Custom Template"},{"location":"get-started/pytorch-mnist/bonus/#custom-frontend_1","text":"Based on the custom template file, we add some extra lines to define the custom frontend service. sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, )","title":"Custom Frontend"},{"location":"get-started/pytorch-mnist/bonus/#backend","text":"After we customize the frontend, we can directly use our custom template during model registration. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) Here we pre-process each image, predict its digit and sum up. Register our new template Sum Mnist as the default template.","title":"Backend"},{"location":"get-started/pytorch-mnist/bonus/#start-the-service","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"get-started/pytorch-mnist/bonus/#test-the-service","text":"Have fun with Pinferencia !","title":"Test the Service"},{"location":"get-started/serve-a-function/","text":"Serve a Function \u00b6 Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job. Mission \u00b6 We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end Create the Service and Register the Model \u00b6 Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Start the backend only? If you only want to start a backend, you can run: uvicorn app:service --reload or pinfer --mode = backend --reload app:service Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m. Further more \u00b6 So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Serve a Function"},{"location":"get-started/serve-a-function/#serve-a-function","text":"Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job.","title":"Serve a Function"},{"location":"get-started/serve-a-function/#mission","text":"We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end","title":"Mission"},{"location":"get-started/serve-a-function/#create-the-service-and-register-the-model","text":"Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"Create the Service and Register the Model"},{"location":"get-started/serve-a-function/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Start the backend only? If you only want to start a backend, you can run: uvicorn app:service --reload or pinfer --mode = backend --reload app:service","title":"Start the Server"},{"location":"get-started/serve-a-function/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m.","title":"Test the API"},{"location":"get-started/serve-a-function/#further-more","text":"So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Further more"},{"location":"get-started/serve-a-json-model/","text":"Run a JSON Model \u00b6 Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model Define the JSON Model \u00b6 Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) You can use Python 3 Type Hints to define the input and output of your model service. Check out how Pinferencia utilizes the usage of Type Hints at Define Request and Response Schema Create the Service and Register the Model \u00b6 First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , # (1) model = model , entrypoint = \"predict\" , # (2) metadata = { \"task\" : task . TEXT_TO_TEXT }, # (3) ) The name you'd like to give your model to display in the url. The function to use to perform predictions. Set the default task of this model. The frontend template will be automatically selected for this model according to the task defined here. What are the model_name, entrypoint and task here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. The task is the indication what kind of the task the model is performing. The corresponding frontend template will be chosen automatically if the task of the model is provided. More details of template can be find at Frontend Requirements Start the Server \u00b6 $ pinfer app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit: http://127.0.0.1:8501 to explore the graphic interface with built-in templates! http://127.0.0.1:8000 to explore the automatically generated API Documentation page! FastAPI and Starlette Pinferencia backends builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. The default address is at: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia frontend builds on Streamlit . The default address is at: http://127.0.0.1:8501 Test the API with requests and Postman \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Serve a Simple JSON Model"},{"location":"get-started/serve-a-json-model/#run-a-json-model","text":"Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model","title":"Run a JSON Model"},{"location":"get-started/serve-a-json-model/#define-the-json-model","text":"Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) You can use Python 3 Type Hints to define the input and output of your model service. Check out how Pinferencia utilizes the usage of Type Hints at Define Request and Response Schema","title":"Define the JSON Model"},{"location":"get-started/serve-a-json-model/#create-the-service-and-register-the-model","text":"First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , # (1) model = model , entrypoint = \"predict\" , # (2) metadata = { \"task\" : task . TEXT_TO_TEXT }, # (3) ) The name you'd like to give your model to display in the url. The function to use to perform predictions. Set the default task of this model. The frontend template will be automatically selected for this model according to the task defined here. What are the model_name, entrypoint and task here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. The task is the indication what kind of the task the model is performing. The corresponding frontend template will be chosen automatically if the task of the model is provided. More details of template can be find at Frontend Requirements","title":"Create the Service and Register the Model"},{"location":"get-started/serve-a-json-model/#start-the-server","text":"$ pinfer app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit: http://127.0.0.1:8501 to explore the graphic interface with built-in templates! http://127.0.0.1:8000 to explore the automatically generated API Documentation page! FastAPI and Starlette Pinferencia backends builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. The default address is at: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia frontend builds on Streamlit . The default address is at: http://127.0.0.1:8501","title":"Start the Server"},{"location":"get-started/serve-a-json-model/#test-the-api-with-requests-and-postman","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Test the API with requests and Postman"},{"location":"how-to-guides/custom-frontend/","text":"Custom Frontend Information \u00b6 Pinferencia frontend supports customization on: title of the web page using model display_name as title on the template short description and detail description First Let's Create a Simple Model Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) This will change the default templage title displayed on the right content area. Now start the service: $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get: Custom the Frontend \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) This will change the title displayed on the top of the left side panel. This will change the description below the title of the left side panel. This will change the about information of the page. Now start the service: $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"Custom Frontend"},{"location":"how-to-guides/custom-frontend/#custom-frontend-information","text":"Pinferencia frontend supports customization on: title of the web page using model display_name as title on the template short description and detail description","title":"Custom Frontend Information"},{"location":"how-to-guides/custom-frontend/#first-lets-create-a-simple-model-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) This will change the default templage title displayed on the right content area. Now start the service: $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"First Let's Create a Simple Model Service"},{"location":"how-to-guides/custom-frontend/#custom-the-frontend","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) This will change the title displayed on the top of the left side panel. This will change the description below the title of the left side panel. This will change the about information of the page. Now start the service: $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"Custom the Frontend"},{"location":"how-to-guides/custom-templates/","text":"Custom Templates \u00b6 Although there are built-in templates, it will never be enough to cover all the scenanrios. Pinferencia supports custom templates. It's easy to customize a template and use it in your service. First let's try to create a simple template: Input a list of numbers. Display the mean, max and min of the numbers. Model \u00b6 The model is simple, and the service can be defined as: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" }) Template \u00b6 Pinferencia provides a BaseTemplate to extend on to build a custom template. JSON Input Field \u00b6 First, we create a JSON input field and display field in two columns. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Start the Service \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... And open the browser you will see: Call Backend and Display Results \u00b6 Add the below highlighted codes to send request to backend and display the result. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Display a button to trigger prediction. Display a spinner while sending the request. Send the data to backend. Display the results in 3 columns. Start the Service again, and You Will Get: \u00b6","title":"Custom Templates"},{"location":"how-to-guides/custom-templates/#custom-templates","text":"Although there are built-in templates, it will never be enough to cover all the scenanrios. Pinferencia supports custom templates. It's easy to customize a template and use it in your service. First let's try to create a simple template: Input a list of numbers. Display the mean, max and min of the numbers.","title":"Custom Templates"},{"location":"how-to-guides/custom-templates/#model","text":"The model is simple, and the service can be defined as: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" })","title":"Model"},{"location":"how-to-guides/custom-templates/#template","text":"Pinferencia provides a BaseTemplate to extend on to build a custom template.","title":"Template"},{"location":"how-to-guides/custom-templates/#json-input-field","text":"First, we create a JSON input field and display field in two columns. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, )","title":"JSON Input Field"},{"location":"how-to-guides/custom-templates/#start-the-service","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... And open the browser you will see:","title":"Start the Service"},{"location":"how-to-guides/custom-templates/#call-backend-and-display-results","text":"Add the below highlighted codes to send request to backend and display the result. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Display a button to trigger prediction. Display a spinner while sending the request. Send the data to backend. Display the results in 3 columns.","title":"Call Backend and Display Results"},{"location":"how-to-guides/custom-templates/#start-the-service-again-and-you-will-get","text":"","title":"Start the Service again, and You Will Get:"},{"location":"how-to-guides/huggingface/dependencies/","text":"For mac users \u00b6 If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Install dependencies \u00b6 You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses transformers or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install Dependencies"},{"location":"how-to-guides/huggingface/dependencies/#for-mac-users","text":"If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"For mac users"},{"location":"how-to-guides/huggingface/dependencies/#install-dependencies","text":"You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses transformers or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install dependencies"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/","text":"Many of you must have heard of Bert , or transformers . And you may also know huggingface. In this tutorial, let's play with its pytorch transformer model and serve it through REST API How does the model work? \u00b6 With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now Prerequisite \u00b6 Please visit Dependencies Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Bert"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#how-does-the-model-work","text":"With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now","title":"How does the model work?"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/huggingface/pipeline/nlp/bert/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200aText Generation Transformer: How to Use & How to Serve \u00b6 What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself? How to Use \u00b6 The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time. How to Deploy \u00b6 Install Pinferencia \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% Create the Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Start the Server \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Text Generation - GPT2"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#gpt2-text-generation-transformer-how-to-use-how-to-serve","text":"What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself?","title":"GPT2\u200a-\u200aText Generation Transformer: How to Use &amp; How to\u00a0Serve"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#how-to-use","text":"The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time.","title":"How to\u00a0Use"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#how-to-deploy","text":"","title":"How to Deploy"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#install-pinferencia","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Install Pinferencia"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#create-the-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, )","title":"Create the Service"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#start-the-server","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Server"},{"location":"how-to-guides/huggingface/pipeline/nlp/text-generation/#test-the-service","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the Service"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/","text":"Google T5 Translation as a Service with Just 7 lines of Codes \u00b6 What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes? Install Dependencies \u00b6 HuggingFace \u00b6 pip install \"transformers[torch]\" If it doesn\u2019t work, please visit Installation and check their official documentations. Pinferencia \u00b6 pip install \"pinferencia[streamlit]\" Define the Service \u00b6 First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION }) Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Translation will be selected automatically. curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' Result: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [\"Guten Morgen, liebe Liebe.\"] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Translation - Google T5"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#google-t5-translation-as-a-service-with-just-7-lines-of-codes","text":"What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes?","title":"Google T5 Translation as a Service with Just 7 lines of Codes"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[torch]\" If it doesn\u2019t work, please visit Installation and check their official documentations.","title":"HuggingFace"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[streamlit]\"","title":"Pinferencia"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#define-the-service","text":"First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION })","title":"Define the Service"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#start-the-service","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"how-to-guides/huggingface/pipeline/nlp/translation/#test-the-service","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Translation will be selected automatically. curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' Result: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [\"Guten Morgen, liebe Liebe.\"] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the Service"},{"location":"how-to-guides/huggingface/pipeline/vision/","text":"In this tutorial, we will explore how to use Hugging Face pipeline, and how to deploy it with Pinferencia as REST API. Prerequisite \u00b6 Please visit Dependencies Download the model and predict \u00b6 The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Amazingly easy! Now let's try: Deploy the model \u00b6 Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Easy, right? Predict \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8501 , and you will have an interactive ui. You can send predict request just there! Improve it \u00b6 However, using the url of the image to predict sometimes is not always convenient. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, ) Predict Again \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Image Classification will be selected automatically. curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Image Classification"},{"location":"how-to-guides/huggingface/pipeline/vision/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"how-to-guides/huggingface/pipeline/vision/#download-the-model-and-predict","text":"The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Amazingly easy! Now let's try:","title":"Download the model and\u00a0predict"},{"location":"how-to-guides/huggingface/pipeline/vision/#deploy-the-model","text":"Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Easy, right?","title":"Deploy the\u00a0model"},{"location":"how-to-guides/huggingface/pipeline/vision/#predict","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8501 , and you will have an interactive ui. You can send predict request just there!","title":"Predict"},{"location":"how-to-guides/huggingface/pipeline/vision/#improve-it","text":"However, using the url of the image to predict sometimes is not always convenient. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, )","title":"Improve it"},{"location":"how-to-guides/huggingface/pipeline/vision/#predict-again","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Image Classification will be selected automatically. curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Predict Again"},{"location":"how-to-guides/paddlepaddle/dependencies/","text":"Install dependencies \u00b6 # Install the GPU version of paddlepaddle pip install paddlepaddle-gpu -U # Or install the CPU version of paddlepaddle pip install paddlepaddle -U pip install paddlehub Tips In addition to the above dependencies, the pre-trained model and preset data set of paddlehub need to be downloaded from the server. Please ensure the machine can access the network normally. If relevant data sets and pre-trained models already exist locally, paddlehub can run offline.","title":"Install Dependencies"},{"location":"how-to-guides/paddlepaddle/dependencies/#install-dependencies","text":"# Install the GPU version of paddlepaddle pip install paddlepaddle-gpu -U # Or install the CPU version of paddlepaddle pip install paddlepaddle -U pip install paddlehub Tips In addition to the above dependencies, the pre-trained model and preset data set of paddlehub need to be downloaded from the server. Please ensure the machine can access the network normally. If relevant data sets and pre-trained models already exist locally, paddlehub can run offline.","title":"Install dependencies"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/","text":"Model basic information \u00b6 Pyramid-Lite is a lightweight model developed by Baidu in 2018 in PyramBox of Computer Vision id 2018 ECCV. It is based on the main network FaceBoxes, measurement, environment, expression changes, meeting changes and other common problem models. The PaddleHub module is based on WIDER FACE data It can be used for face detection based on self-collected face datasets and Baidu self-collected face datasets, which supports prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001pyramidbox_lite_server dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install pyramidbox_lite_server Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' Response { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"Face Detection"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#model-basic-information","text":"Pyramid-Lite is a lightweight model developed by Baidu in 2018 in PyramBox of Computer Vision id 2018 ECCV. It is based on the main network FaceBoxes, measurement, environment, expression changes, meeting changes and other common problem models. The PaddleHub module is based on WIDER FACE data It can be used for face detection based on self-collected face datasets and Baidu self-collected face datasets, which supports prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#2pyramidbox_lite_server-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001pyramidbox_lite_server dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#3download-the-model","text":"hub install pyramidbox_lite_server","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/face_detection/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' Response { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/","text":"Mobile Net \u00b6 MobileNet V2 is a lightweight convolutional neural network. On the basis of MobileNet, it has made two major improvements: Inverted Residuals and Linear bottlenecks. The PaddleHub Module is trained on Baidu's self-built animal dataset and can be used for image classification and feature extraction. Currently, it supports the classification and recognition of 7978 animals. Details of the model can be found in the paper . Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Result: \u677e\u9f20 Let's try it out now Prerequisite \u00b6 1. Dependencies \u00b6 Please visit dependencies 2. mobilenet_v2_animals Prerequisites \u00b6 Package Version paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3. Install the Model \u00b6 hub install pyramidbox_lite_server Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' Response { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Image Classification"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#mobile-net","text":"MobileNet V2 is a lightweight convolutional neural network. On the basis of MobileNet, it has made two major improvements: Inverted Residuals and Linear bottlenecks. The PaddleHub Module is trained on Baidu's self-built animal dataset and can be used for image classification and feature extraction. Currently, it supports the classification and recognition of 7978 animals. Details of the model can be found in the paper . Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"Mobile Net"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Result: \u677e\u9f20 Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#1-dependencies","text":"Please visit dependencies","title":"1. Dependencies"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#2-mobilenet_v2_animals-prerequisites","text":"Package Version paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2. mobilenet_v2_animals Prerequisites"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#3-install-the-model","text":"hub install pyramidbox_lite_server","title":"3. Install the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/image_classification/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' Response { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/","text":"Model basic information \u00b6 This model is encapsulated from [the paddlepaddle version of the photo2cartoon project of Xiaoshi Technology] ( https://github.com/minivision-ai/photo2cartoon-paddle ). Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon Example \u00b6 Input Output Image Source ( https://www.pexels.com ) Let's try it out now Prerequisite \u00b6 1. environment dependent \u00b6 Please visit dependencies 2. mobilenet_v2_animals dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3. Download the model \u00b6 hub install Photo2Cartoon Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' Response { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Image Generation"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#model-basic-information","text":"This model is encapsulated from [the paddlepaddle version of the photo2cartoon project of Xiaoshi Technology] ( https://github.com/minivision-ai/photo2cartoon-paddle ). Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#example","text":"Input Output Image Source ( https://www.pexels.com ) Let's try it out now","title":"Example"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#1-environment-dependent","text":"Please visit dependencies","title":"1. environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#2-mobilenet_v2_animals-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2. mobilenet_v2_animals dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#3-download-the-model","text":"hub install Photo2Cartoon","title":"3. Download the model"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/image_generation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' Response { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/","text":"Model basic information \u00b6 Vehicle detection is a very important and challenging task in urban traffic monitoring. The difficulty of this task lies in accurately localizing and classifying relatively small vehicles in complex scenes. The network of the PaddleHub Module is YOLOv3, of which the backbone is DarkNet53, which is trained by Baidu's self-built large-scale vehicle data set, and supports the recognition of car, truck, bus, motorbike, tricycle and other models. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001yolov3_darknet53_vehicles dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install yolov3_darknet53_vehicles Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' Response { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Index"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#model-basic-information","text":"Vehicle detection is a very important and challenging task in urban traffic monitoring. The difficulty of this task lies in accurately localizing and classifying relatively small vehicles in complex scenes. The network of the PaddleHub Module is YOLOv3, of which the backbone is DarkNet53, which is trained by Baidu's self-built large-scale vehicle data set, and supports the recognition of car, truck, bus, motorbike, tricycle and other models. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#2yolov3_darknet53_vehicles-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#3download-the-model","text":"hub install yolov3_darknet53_vehicles","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/object_detection/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' Response { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/","text":"Model basic information \u00b6 A lightweight portrait segmentation model based on the ExtremeC3 model. For more details, please refer to: ExtremeC3_Portrait_Segmentation project. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001ExtremeC3_Portrait_Segmentation dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001Download the model \u00b6 hub install ExtremeC3_Portrait_Segmentation Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' Response { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Semantic Segmentation"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#model-basic-information","text":"A lightweight portrait segmentation model based on the ExtremeC3 model. For more details, please refer to: ExtremeC3_Portrait_Segmentation project. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#2extremec3_portrait_segmentation-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001ExtremeC3_Portrait_Segmentation dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#3download-the-model","text":"hub install ExtremeC3_Portrait_Segmentation","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' Response { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/","text":"Model basic information \u00b6 The chinese_ocr_db_crnn_mobile Module is used to identify the Chinese characters in the picture, identify the Chinese characters in the text box, and then classify the angle of the detected text box. The final text recognition algorithm adopts CRNN (Convolutional Recurrent Neural Network), namely Convolutional Recurrent Neural Network. This Module is an ultra-lightweight Chinese OCR model that supports direct prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001yolov3_darknet53_vehicles dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install chinese_ocr_db_crnn_mobile Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' Response {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Index"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#model-basic-information","text":"The chinese_ocr_db_crnn_mobile Module is used to identify the Chinese characters in the picture, identify the Chinese characters in the text box, and then classify the angle of the detected text box. The final text recognition algorithm adopts CRNN (Convolutional Recurrent Neural Network), namely Convolutional Recurrent Neural Network. This Module is an ultra-lightweight Chinese OCR model that supports direct prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#2yolov3_darknet53_vehicles-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles dependent"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#3download-the-model","text":"hub install chinese_ocr_db_crnn_mobile","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/image/text_recognition/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' Response {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/","text":"Model basic information \u00b6 Emotion Detection (EmoTect for short) focuses on identifying the emotions of users in intelligent dialogue scenes. For user texts in intelligent dialogue scenes, it automatically determines the emotional category of the text and gives the corresponding confidence. The emotional type is divided into positive , Negative, Neutral. The model is based on TextCNN (Multiple Convolutional Kernel CNN model), which can better capture sentence local correlation. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn Sample result example \u00b6 Input Output [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001emotion_detection_textcnn dependent \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001Download the model \u00b6 hub install emotion_detection_textcnn Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Raw Request will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' Response { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Emotion Detection Textcnn"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#model-basic-information","text":"Emotion Detection (EmoTect for short) focuses on identifying the emotions of users in intelligent dialogue scenes. For user texts in intelligent dialogue scenes, it automatically determines the emotional category of the text and gives the corresponding confidence. The emotional type is divided into positive , Negative, Neutral. The model is based on TextCNN (Multiple Convolutional Kernel CNN model), which can better capture sentence local correlation. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#sample-result-example","text":"Input Output [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#2emotion_detection_textcnn-dependent","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001emotion_detection_textcnn dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#3download-the-model","text":"hub install emotion_detection_textcnn","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Raw Request will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' Response { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/","text":"Model basic information \u00b6 This Module is a word segmentation network (bidirectional GRU) built by jieba using the PaddlePaddle deep learning framework. At the same time, it also supports jieba's traditional word segmentation methods, such as precise mode, full mode, search engine mode and other word segmentation modes. The usage methods are consistent with jieba. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle Sample result example \u00b6 Input Output \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001jieba_paddle dependent \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001Download the model \u00b6 hub install jieba_paddle Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' Response { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Lexical analysis"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#model-basic-information","text":"This Module is a word segmentation network (bidirectional GRU) built by jieba using the PaddlePaddle deep learning framework. At the same time, it also supports jieba's traditional word segmentation methods, such as precise mode, full mode, search engine mode and other word segmentation modes. The usage methods are consistent with jieba. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#sample-result-example","text":"Input Output \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#2jieba_paddle-dependent","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001jieba_paddle dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#3download-the-model","text":"hub install jieba_paddle","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/text/lexical_analysis/#test-the","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' Response { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/","text":"Model basic information \u00b6 Simultaneous interpretation, that is, translation before the sentence is completed, the goal of simultaneous interpretation is to automate simultaneous interpretation, which can be translated at the same time as the source language, with a delay of only a few seconds. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1 Sample result example \u00b6 Input Output [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001transformer_nist_wait_1 dependent \u00b6 paddlepaddle >= 2.1.0 paddlehub >= 2.1.0 3\u3001Download the model \u00b6 hub install transformer_nist_wait_1 Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' Response on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Create the test.py . test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) Run the script and check the result. $ python test.py # on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Index"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#model-basic-information","text":"Simultaneous interpretation, that is, translation before the sentence is completed, the goal of simultaneous interpretation is to automate simultaneous interpretation, which can be translated at the same time as the source language, with a delay of only a few seconds. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#sample-result-example","text":"Input Output [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#2transformer_nist_wait_1-dependent","text":"paddlepaddle >= 2.1.0 paddlehub >= 2.1.0","title":"2\u3001transformer_nist_wait_1 dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#3download-the-model","text":"hub install transformer_nist_wait_1","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#test-the-service","text":"curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' Response on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Create the test.py . test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) Run the script and check the result. $ python test.py # on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Test the service"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/","text":"Model basic information \u00b6 ERNIE-GEN is a pre-training-fine-tuning framework for generation tasks. For the first time, span-by-span generation tasks are added to the pre-training stage, so that the model can generate a semantically complete segment each time. The exposure bias problem is mitigated by a padding generative mechanism and a noise-aware mechanism in pre-training and fine-tuning. In addition, ERNIE-GEN samples multi-segment-multi-granularity target text sampling strategy, which enhances the correlation between source text and target text, and strengthens the interaction between encoder and decoder. ernie_gen_poetry is fine-tuned on the open source poetry dataset and can be used to generate poetry. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry Sample result example \u00b6 Input Output [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001ernie_gen_poetry dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp 3\u3001Download the model \u00b6 hub install ernie_gen_poetry Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia \u3002 pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template TEXT_TO_TEXT will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' Response { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Text Generation"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#model-basic-information","text":"ERNIE-GEN is a pre-training-fine-tuning framework for generation tasks. For the first time, span-by-span generation tasks are added to the pre-training stage, so that the model can generate a semantically complete segment each time. The exposure bias problem is mitigated by a padding generative mechanism and a noise-aware mechanism in pre-training and fine-tuning. In addition, ERNIE-GEN samples multi-segment-multi-granularity target text sampling strategy, which enhances the correlation between source text and target text, and strengthens the interaction between encoder and decoder. ernie_gen_poetry is fine-tuned on the open source poetry dataset and can be used to generate poetry. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry","title":"Model basic information"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#sample-result-example","text":"Input Output [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] Let's try it out now","title":"Sample result example"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#prerequisite","text":"","title":"Prerequisite"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#2ernie_gen_poetry-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp","title":"2\u3001ernie_gen_poetry dependent"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#3download-the-model","text":"hub install ernie_gen_poetry","title":"3\u3001Download the model"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#install-pinferencia","text":"First, let's install Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"how-to-guides/paddlepaddle/modules/text/text_generation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template TEXT_TO_TEXT will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' Response { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"how-to-guides/schema/","text":"How to Define the Schema of Request and Response of Your Service? \u00b6 Imagine this: Your service calculates the sum of posted data. It requires a request body: [ 1 , 2 , 3 ] and returns a response body: 6 How do you let the user know what does your request and response body look like? And what if you want to validate/parse the request/response body automatically? In this article, we will walk throught how to define the schema of the request and response of your service in Pinferencia. Python 3 Type Hint \u00b6 Have you heard of type hint in python? If not, you better check it out now at Python Typing . Since Python 3.5, Python starts to support type hint in your function definition. You can declare the type of the arguments and return. Pinferencia use the type hint of your function to define the schema of your request and response. So, you don't need to learn another format, and you can just stay with python. Not all type hints are supported! Not all the type hints in python can be used to define the schema. The type hints need be able to be correctly represented in the json schema. A Dummy Service \u00b6 Let's create a dummy service to show you how everything works. dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) Start the service, and visit the backend documentation page, you will find examples of the request and response: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Here type hint list of the argument of the function will be used to define the data field in request body. Type hint str of the return of the function will be used to define the data field in the response body. The Sum Service \u00b6 Now let's get back to the service at the start of this article, a sum service with: Request Example Response Example [ 1 , 2 , 3 ] 6 Let's rewrite our dummy service Python 3.6 and above Python 3.9 and above dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) Now visit the backend documentation page, the examples will be: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } Besides displaying the schema, Pinferencia also validates and tries to parse the data into the desired types. Let's try out the API on the backend documentation page. Normal Data Invalid Type Data Invalid Data Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's change one of the number in the request to string type. And the number will be converted to integer according to the schema. Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's post some invalid data type, and you will receive a 422 error Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] } Complicated Schema \u00b6 It's possible to define complicated schema in Pinferencia with the help of pydantic . Let's assume a service receive a persion's information: request [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] and simply reply a welcome message: response \"Hello, Will! Hello, Elise!\" Let's define the service: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) Now start the service and visit the backend documentation page, you will find the request and response example as below: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Mission Completed \u00b6 You have learned how to define request and response schema with Pinferencia . You can now try out more schemas you're interested. Have fun!","title":"Define Request and Response Schema"},{"location":"how-to-guides/schema/#how-to-define-the-schema-of-request-and-response-of-your-service","text":"Imagine this: Your service calculates the sum of posted data. It requires a request body: [ 1 , 2 , 3 ] and returns a response body: 6 How do you let the user know what does your request and response body look like? And what if you want to validate/parse the request/response body automatically? In this article, we will walk throught how to define the schema of the request and response of your service in Pinferencia.","title":"How to Define the Schema of Request and Response of Your Service?"},{"location":"how-to-guides/schema/#python-3-type-hint","text":"Have you heard of type hint in python? If not, you better check it out now at Python Typing . Since Python 3.5, Python starts to support type hint in your function definition. You can declare the type of the arguments and return. Pinferencia use the type hint of your function to define the schema of your request and response. So, you don't need to learn another format, and you can just stay with python. Not all type hints are supported! Not all the type hints in python can be used to define the schema. The type hints need be able to be correctly represented in the json schema.","title":"Python 3 Type Hint"},{"location":"how-to-guides/schema/#a-dummy-service","text":"Let's create a dummy service to show you how everything works. dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) Start the service, and visit the backend documentation page, you will find examples of the request and response: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Here type hint list of the argument of the function will be used to define the data field in request body. Type hint str of the return of the function will be used to define the data field in the response body.","title":"A Dummy Service"},{"location":"how-to-guides/schema/#the-sum-service","text":"Now let's get back to the service at the start of this article, a sum service with: Request Example Response Example [ 1 , 2 , 3 ] 6 Let's rewrite our dummy service Python 3.6 and above Python 3.9 and above dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) Now visit the backend documentation page, the examples will be: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } Besides displaying the schema, Pinferencia also validates and tries to parse the data into the desired types. Let's try out the API on the backend documentation page. Normal Data Invalid Type Data Invalid Data Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's change one of the number in the request to string type. And the number will be converted to integer according to the schema. Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's post some invalid data type, and you will receive a 422 error Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] }","title":"The Sum Service"},{"location":"how-to-guides/schema/#complicated-schema","text":"It's possible to define complicated schema in Pinferencia with the help of pydantic . Let's assume a service receive a persion's information: request [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] and simply reply a welcome message: response \"Hello, Will! Hello, Elise!\" Let's define the service: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) Now start the service and visit the backend documentation page, you will find the request and response example as below: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" }","title":"Complicated Schema"},{"location":"how-to-guides/schema/#mission-completed","text":"You have learned how to define request and response schema with Pinferencia . You can now try out more schemas you're interested. Have fun!","title":"Mission Completed"},{"location":"introduction/overview/","text":"Welcome to Pinferencia \u00b6 What is Pinferencia? \u00b6 Straight forward. Simple. Powerful. Three extra lines and your model goes online . Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your machine learning models with a fully functioning Rest API. Features \u00b6 Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served. Try it now! \u00b6 Install \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% Create the App \u00b6 Any Model Any Function Scikit-Learn PyTorch Tensorflow HuggingFace Transformer app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) Run! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1:8000/ and have fun. Remember to come back to our Get Started class!","title":"Overview"},{"location":"introduction/overview/#welcome-to-pinferencia","text":"","title":"Welcome to Pinferencia"},{"location":"introduction/overview/#what-is-pinferencia","text":"Straight forward. Simple. Powerful. Three extra lines and your model goes online . Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your machine learning models with a fully functioning Rest API.","title":"What is Pinferencia?"},{"location":"introduction/overview/#features","text":"Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served.","title":"Features"},{"location":"introduction/overview/#try-it-now","text":"","title":"Try it now!"},{"location":"introduction/overview/#install","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Install"},{"location":"introduction/overview/#create-the-app","text":"Any Model Any Function Scikit-Learn PyTorch Tensorflow HuggingFace Transformer app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict )","title":"Create the App"},{"location":"introduction/overview/#run","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1:8000/ and have fun. Remember to come back to our Get Started class!","title":"Run!"},{"location":"introduction/pinferencia-is-different/","text":"Why is Pinferencia different? \u00b6 Different? \u00b6 Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now? No way!!!!!!!!!!!! You have your model, you train it in python , and you predict in python . You even write complicated python codes to perform more difficult tasks . How many changes you need to make and how many extra codes you need to write to get your model served using those tools or platforms? The answer is A lot . With Pinferencia \u00b6 You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected . Simple, and Powerful \u00b6 Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Pinferencia is different?"},{"location":"introduction/pinferencia-is-different/#why-is-pinferencia-different","text":"","title":"Why is Pinferencia different?"},{"location":"introduction/pinferencia-is-different/#different","text":"Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now?","title":"Different?"},{"location":"introduction/pinferencia-is-different/#with-pinferencia","text":"You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected .","title":"With Pinferencia"},{"location":"introduction/pinferencia-is-different/#simple-and-powerful","text":"Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Simple, and Powerful"},{"location":"reference/cli/","text":"CLI \u00b6 Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"reference/cli/#cli","text":"Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"reference/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image. Output \u00b6 If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Frontend Requirements"},{"location":"reference/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"reference/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"reference/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"reference/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"reference/frontend/requirements/#input","text":"Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image.","title":"Input"},{"location":"reference/frontend/requirements/#output","text":"If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Output"},{"location":"reference/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path PickleHandler \u00b6 The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"reference/handlers/#handlers","text":"","title":"Handlers"},{"location":"reference/handlers/#basehandler","text":"BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path","title":"BaseHandler"},{"location":"reference/handlers/#picklehandler","text":"The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"reference/models/machine-learning/","text":"Machine Learning Frameworks \u00b6 Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Other Machine Learning Frameworks"},{"location":"reference/models/machine-learning/#machine-learning-frameworks","text":"Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Machine Learning Frameworks"},{"location":"reference/models/register/","text":"Register \u00b6 Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) Parameters \u00b6 Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration Examples \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version Name \u00b6 Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict. Metadata \u00b6 Default API \u00b6 Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure. Handler \u00b6 Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Register Models"},{"location":"reference/models/register/#register","text":"Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"Register"},{"location":"reference/models/register/#parameters","text":"Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration","title":"Parameters"},{"location":"reference/models/register/#examples","text":"","title":"Examples"},{"location":"reference/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"reference/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"reference/models/register/#version-name","text":"Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version Name"},{"location":"reference/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict.","title":"Entrypoint"},{"location":"reference/models/register/#metadata","text":"","title":"Metadata"},{"location":"reference/models/register/#default-api","text":"Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"Default API"},{"location":"reference/models/register/#kserve-api","text":"Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure.","title":"Kserve API"},{"location":"reference/models/register/#handler","text":"Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"reference/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"reference/restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"reference/restapi/#rest-api","text":"","title":"REST API"},{"location":"reference/restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"reference/restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"reference/restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"reference/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"en/reference/cli/","text":"CLI \u00b6 Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"en/reference/cli/#cli","text":"Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"en/reference/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image. Output \u00b6 If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Frontend Requirements"},{"location":"en/reference/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"en/reference/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"en/reference/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"en/reference/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"en/reference/frontend/requirements/#input","text":"Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image.","title":"Input"},{"location":"en/reference/frontend/requirements/#output","text":"If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Output"},{"location":"en/reference/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path PickleHandler \u00b6 The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"en/reference/handlers/#handlers","text":"","title":"Handlers"},{"location":"en/reference/handlers/#basehandler","text":"BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path","title":"BaseHandler"},{"location":"en/reference/handlers/#picklehandler","text":"The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"en/reference/models/machine-learning/","text":"Machine Learning Frameworks \u00b6 Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Other Machine Learning Frameworks"},{"location":"en/reference/models/machine-learning/#machine-learning-frameworks","text":"Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Machine Learning Frameworks"},{"location":"en/reference/models/register/","text":"Register \u00b6 Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) Parameters \u00b6 Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration Examples \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version Name \u00b6 Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict. Metadata \u00b6 Default API \u00b6 Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure. Handler \u00b6 Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Register Models"},{"location":"en/reference/models/register/#register","text":"Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"Register"},{"location":"en/reference/models/register/#parameters","text":"Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration","title":"Parameters"},{"location":"en/reference/models/register/#examples","text":"","title":"Examples"},{"location":"en/reference/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"en/reference/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"en/reference/models/register/#version-name","text":"Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version Name"},{"location":"en/reference/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict.","title":"Entrypoint"},{"location":"en/reference/models/register/#metadata","text":"","title":"Metadata"},{"location":"en/reference/models/register/#default-api","text":"Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"Default API"},{"location":"en/reference/models/register/#kserve-api","text":"Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure.","title":"Kserve API"},{"location":"en/reference/models/register/#handler","text":"Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"en/reference/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"en/reference/restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"en/reference/restapi/#rest-api","text":"","title":"REST API"},{"location":"en/reference/restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"en/reference/restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"en/reference/restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"en/reference/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"re/","text":"","title":"Home"},{"location":"re/background/models/home/","text":"Model? \u00b6 What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"About Models"},{"location":"re/background/models/home/#model","text":"What is a Model ? Generally, it is a way to calculate something that is more complicated than a equation. Should it be a file? Perhaps. Could it be a python object? Of course. In Pinferencia , a model is just a piece of codes that can get called. A function, or an instance of a class just like those pytorch models.","title":"Model?"},{"location":"re/get-started/install/","text":"Install Pinferencia \u00b6 Recommended \u00b6 It's recommended to install Pinferencia with streamlit . You will have the full power of Pinferencia . $ pip install \"pinferencia[streamlit]\" ---> 100% Alternatively \u00b6 You can also choose install Pinferencia without streamlit , in this mode, you can only run backend of pinferencia. $ pip install pinferencia ---> 100%","title":"Install"},{"location":"re/get-started/install/#install-pinferencia","text":"","title":"Install Pinferencia"},{"location":"re/get-started/install/#recommended","text":"It's recommended to install Pinferencia with streamlit . You will have the full power of Pinferencia . $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Recommended"},{"location":"re/get-started/install/#alternatively","text":"You can also choose install Pinferencia without streamlit , in this mode, you can only run backend of pinferencia. $ pip install pinferencia ---> 100%","title":"Alternatively"},{"location":"re/get-started/introduction/","text":"Solve the Riddle \u00b6 In this world there are so many tools, however you never learn them at schools. Want to has some API for you model? It's not easy, like a myth, like a riddle. All I want are some predictions, and the way led by pinferencia, there'll be no more frictions. In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and have some fun with the MNIST model","title":"Introduction"},{"location":"re/get-started/introduction/#solve-the-riddle","text":"In this world there are so many tools, however you never learn them at schools. Want to has some API for you model? It's not easy, like a myth, like a riddle. All I want are some predictions, and the way led by pinferencia, there'll be no more frictions. In this series of tutorials, you will master how to serve: a custom model : a simple JSON model a custom function PyTorch MNIST model with two methods and have some fun with the MNIST model","title":"Solve the Riddle"},{"location":"re/get-started/other-models/","text":"What's Next \u00b6 Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"re/get-started/other-models/#whats-next","text":"Well, I bet you had some fun with the PyTorch MNIST model in the previous tutorial. I think you are familiar with Pinferencia now. Pinferencia can serve any callable object in a very straight-forward way. No complications. And it's easy to integrate with your existing codes. That is what Pinferencia is designed for. Minimum codes modifications . Now you can serve models from any framework , and you can even mix up them together . You can have an API using different models from different frameworks at the same time ! Enjoy yourself! If you like Pinferencia , don't forget to go to Github and give a star. Thank you. If you want to explore more examples on different machine learning models, you can find the inside the Example section from the navigation panel on your left.","title":"What's Next"},{"location":"re/get-started/pytorch-mnist/","text":"Serve PyTorch MNIST Model \u00b6 In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response. Prerequisite \u00b6 Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt Deploy Methods \u00b6 There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial. Directly Register a Function \u00b6 Create the App \u00b6 Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO! Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 Frontend UI \u00b6 Open http://127.0.0.1:8501 , and the template Image to Text will be selected automatically. Use the image below: You will get: Backend API \u00b6 Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1:8000 Register a Model Path, with a Handler \u00b6 Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers Create the App \u00b6 Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction. Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Run the test: $ python test.py Prediction: 4 No suprise, the same result. Finally \u00b6 Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model.","title":"Serve PyTorch MNIST Model"},{"location":"re/get-started/pytorch-mnist/#serve-pytorch-mnist-model","text":"In this tutorial, we will serve a PyTorch MNIST model. It receives a Base64 encoded image as request data, and return the prediction in the response.","title":"Serve PyTorch MNIST Model"},{"location":"re/get-started/pytorch-mnist/#prerequisite","text":"Visit PyTorch Examples - MNIST , download the files. Run below commands to install and train the model: pip install -r requirements.txt python main.py --save-model After the training is finished, you will have a folder structure as below. A mnist_cnn.pt file is created . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"Prerequisite"},{"location":"re/get-started/pytorch-mnist/#deploy-methods","text":"There are two methods you can deploy the model. Directly register a function. Only register a model path, with an additioanl handler. We will cover both methods step by step in this tutorial.","title":"Deploy Methods"},{"location":"re/get-started/pytorch-mnist/#directly-register-a-function","text":"","title":"Directly Register a Function"},{"location":"re/get-started/pytorch-mnist/#create-the-app","text":"Let's create a file func_app.py in the same folder. func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Make suer you can import the Net Model. Preprocessing transformation codes. The example script only save the state_dict . Here we need to initialize the model and load the state_dict . Get ready, 3, 2, 1. GO!","title":"Create the App"},{"location":"re/get-started/pytorch-mnist/#start-the-service","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"re/get-started/pytorch-mnist/#test-the-service","text":"Test Data? Because our input is a base64 encoded MNIST image, where can we get these data? You can make use of PyTorch's datasets. Create a file with in the same folder named get-base64-img . get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) This is the MNIST dataset used during training. Let's use a random image. The string and the target are printed to stdout. Run the script and copy the string. python get-base64-img.py Output: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4","title":"Test the Service"},{"location":"re/get-started/pytorch-mnist/#frontend-ui","text":"Open http://127.0.0.1:8501 , and the template Image to Text will be selected automatically. Use the image below: You will get:","title":"Frontend UI"},{"location":"re/get-started/pytorch-mnist/#backend-api","text":"Let's create a file test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run the test: $ python test.py Prediction: 4 You can try out the API with more images, or even using the interactive API documentation page http://127.0.0.1:8000","title":"Backend API"},{"location":"re/get-started/pytorch-mnist/#register-a-model-path-with-a-handler","text":"Handler If you prefer the old classical way of serving a model with a file, using a handler is your choice. For details of handlers, please visit Handlers","title":"Register a Model Path, with a Handler"},{"location":"re/get-started/pytorch-mnist/#create-the-app_1","text":"Let's create a file func_app.py in the same folder. The codes below are refactored into a handle class. It looks cleaner! path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) We move the codes of loading the model into the load_model function. The model path can be accessed by self.model_path . We move the codes of predicting into the predict function. The model can be accessed by self.model . model_dir is where Pinferencia will look for your model files. Set the model_dir to the folder having the mnist_cnn.pt and this script. load_now determine if the model will be get loaded immediately during registration. The default is True . If set to False , you need to call the load API to load the model before prediction.","title":"Create the App"},{"location":"re/get-started/pytorch-mnist/#start-the-service_1","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"re/get-started/pytorch-mnist/#test-the-service_1","text":"Run the test: $ python test.py Prediction: 4 No suprise, the same result.","title":"Test the Service"},{"location":"re/get-started/pytorch-mnist/#finally","text":"Using Pinferencia , you can serve any model. You can load the models by yourself, just what you have done to do a offline prediction. The codes are already there. Then, just register the model using Pinferencia , and your model is alive. Alternatively, you can choose to refactor your codes into a Handler Class . The old classic way also works with Pinferencia . Both worlds work for your model, classic music and rock'n'roll . Isn't it great! Now you have mastered how to use Pinferencia to: Register any model, any function and serve them. Use your custom handler to serve your machine learning model.","title":"Finally"},{"location":"re/get-started/pytorch-mnist/bonus/","text":"Bonus \u00b6 If you still have time, let's try something fun. Extra: Sum Up the MNIST Images \u00b6 Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. Here, we first create a custom frontend template accepting two MNIST images and send them back to our backend for prediction. Custom Frontend \u00b6 How to Custom Template? You can find more info at Custom Templates . Custom Template \u00b6 First, we need a new template: sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) Here we split the content panel into two columns, each accepts a single MNIST image. Once the image is uploaded, append it to the image array for later prediction. Once the image is uploaded, append it to the image array for later prediction. If both images are uploaded, send them to backend to predict. Custom Frontend \u00b6 Based on the custom template file, we add some extra lines to define the custom frontend service. sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, ) Backend \u00b6 After we customize the frontend, we can directly use our custom template during model registration. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) Here we pre-process each image, predict its digit and sum up. Register our new template Sum Mnist as the default template. Start the Service \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 Have fun with Pinferencia !","title":"MNIST Bonus"},{"location":"re/get-started/pytorch-mnist/bonus/#bonus","text":"If you still have time, let's try something fun.","title":"Bonus"},{"location":"re/get-started/pytorch-mnist/bonus/#extra-sum-up-the-mnist-images","text":"Let's create a sum_mnist.py . It accepts an array of images, predicts their digits and sum up them. Here, we first create a custom frontend template accepting two MNIST images and send them back to our backend for prediction.","title":"Extra: Sum Up the MNIST Images"},{"location":"re/get-started/pytorch-mnist/bonus/#custom-frontend","text":"How to Custom Template? You can find more info at Custom Templates .","title":"Custom Frontend"},{"location":"re/get-started/pytorch-mnist/bonus/#custom-template","text":"First, we need a new template: sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) Here we split the content panel into two columns, each accepts a single MNIST image. Once the image is uploaded, append it to the image array for later prediction. Once the image is uploaded, append it to the image array for later prediction. If both images are uploaded, send them to backend to predict.","title":"Custom Template"},{"location":"re/get-started/pytorch-mnist/bonus/#custom-frontend_1","text":"Based on the custom template file, we add some extra lines to define the custom frontend service. sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, )","title":"Custom Frontend"},{"location":"re/get-started/pytorch-mnist/bonus/#backend","text":"After we customize the frontend, we can directly use our custom template during model registration. sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) Here we pre-process each image, predict its digit and sum up. Register our new template Sum Mnist as the default template.","title":"Backend"},{"location":"re/get-started/pytorch-mnist/bonus/#start-the-service","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"re/get-started/pytorch-mnist/bonus/#test-the-service","text":"Have fun with Pinferencia !","title":"Test the Service"},{"location":"re/get-started/serve-a-function/","text":"Serve a Function \u00b6 Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job. Mission \u00b6 We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end Create the Service and Register the Model \u00b6 Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) Start the Server \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Start the backend only? If you only want to start a backend, you can run: uvicorn app:service --reload or pinfer --mode = backend --reload app:service Test the API \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m. Further more \u00b6 So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Serve a Function"},{"location":"re/get-started/serve-a-function/#serve-a-function","text":"Well, serving a function? Is it useful? Of course it is. If you have a whole workflow of inferences , it consists of many steps. Most of the time, you will implement a function to do this job. Now you can register the function immediately. If you want to share some pre-processing or post-processing functions, now you've got your Robin, Batman ! Or a function is just enough for your job.","title":"Serve a Function"},{"location":"re/get-started/serve-a-function/#mission","text":"We're given a list of mountains' heights. We need to find out the highest, the loweset, and the difference between the highest and the lowest. It's a simple problem, let's solve it in a function to get you familiar with the concept: graph LR heights(Mountains' Heights) --> max(Find Out the Highest) heights --> min(Find Out the Lowest) min --> diff(Calculate the Difference) max --> diff diff --> output(Output) subgraph Workflow max min diff end","title":"Mission"},{"location":"re/get-started/serve-a-function/#create-the-service-and-register-the-model","text":"Save the following codes in app.py . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"Create the Service and Register the Model"},{"location":"re/get-started/serve-a-function/#start-the-server","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Start the backend only? If you only want to start a backend, you can run: uvicorn app:service --reload or pinfer --mode = backend --reload app:service","title":"Start the Server"},{"location":"re/get-started/serve-a-function/#test-the-api","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) Run the script and check the result. $ python test.py Difference between the highest and lowest is 2000m.","title":"Test the API"},{"location":"re/get-started/serve-a-function/#further-more","text":"So now you have learned how to serve a model define as a Class or a Function . If you have just a single model to serve, it's easy-peasy. But in real world, you have custom codes like pre-processing and post-processing. And some tasks need multiple models to work together. For example, if you want to predict an animal's breed, you may need the below workflow: graph LR pic(Picture) --> species(Species Classification) species --> cat(Cat) --> cat_breed(Cat Breed Classification) --> Persian(Persian) species --> dog(Dog) --> dog_breed(Dog Breed Classification) --> Labrador(Labrador) species --> monkey(Monkey) --> monkey_breed(Monkey Breed Classification) --> spider(Spider Monkeys) Deploying this on many platform or tools aren't that easy. However, now you have Pinferencia , you have a choice!","title":"Further more"},{"location":"re/get-started/serve-a-json-model/","text":"Run a JSON Model \u00b6 Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model Define the JSON Model \u00b6 Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) You can use Python 3 Type Hints to define the input and output of your model service. Check out how Pinferencia utilizes the usage of Type Hints at Define Request and Response Schema Create the Service and Register the Model \u00b6 First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , # (1) model = model , entrypoint = \"predict\" , # (2) metadata = { \"task\" : task . TEXT_TO_TEXT }, # (3) ) The name you'd like to give your model to display in the url. The function to use to perform predictions. Set the default task of this model. The frontend template will be automatically selected for this model according to the task defined here. What are the model_name, entrypoint and task here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. The task is the indication what kind of the task the model is performing. The corresponding frontend template will be chosen automatically if the task of the model is provided. More details of template can be find at Frontend Requirements Start the Server \u00b6 $ pinfer app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit: http://127.0.0.1:8501 to explore the graphic interface with built-in templates! http://127.0.0.1:8000 to explore the automatically generated API Documentation page! FastAPI and Starlette Pinferencia backends builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. The default address is at: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia frontend builds on Streamlit . The default address is at: http://127.0.0.1:8501 Test the API with requests and Postman \u00b6 Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Serve a Simple JSON Model"},{"location":"re/get-started/serve-a-json-model/#run-a-json-model","text":"Now let's first try something easy to get you familiar with Pinferecia . TL;DR It's important for you to understand how to register and serve a model in Pinferencia . However, if you want to try machine learning model now, you can jump to Serve Pytorch MNIST Model","title":"Run a JSON Model"},{"location":"re/get-started/serve-a-json-model/#define-the-json-model","text":"Let's create a file named app.py . Below is a JSON Model. It simply return 1 for input a , 2 for input b , and 0 for other inputs. app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) You can use Python 3 Type Hints to define the input and output of your model service. Check out how Pinferencia utilizes the usage of Type Hints at Define Request and Response Schema","title":"Define the JSON Model"},{"location":"re/get-started/serve-a-json-model/#create-the-service-and-register-the-model","text":"First we import Server from pinferencia , then create an instance and register a instance of our JSON Model . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , # (1) model = model , entrypoint = \"predict\" , # (2) metadata = { \"task\" : task . TEXT_TO_TEXT }, # (3) ) The name you'd like to give your model to display in the url. The function to use to perform predictions. Set the default task of this model. The frontend template will be automatically selected for this model according to the task defined here. What are the model_name, entrypoint and task here? model_name is the name you give to the model for later API access. Here we give the model a name json , and the url for this model is http://127.0.0.1:8000/v1/models/json . If you have any confusion about the APIs, you can always visit the documentation page mentioned in the next part. The entrypoint predict means we will use the predict function of JSON Model to predict the data. The task is the indication what kind of the task the model is performing. The corresponding frontend template will be chosen automatically if the task of the model is provided. More details of template can be find at Frontend Requirements","title":"Create the Service and Register the Model"},{"location":"re/get-started/serve-a-json-model/#start-the-server","text":"$ pinfer app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Open your browser and visit: http://127.0.0.1:8501 to explore the graphic interface with built-in templates! http://127.0.0.1:8000 to explore the automatically generated API Documentation page! FastAPI and Starlette Pinferencia backends builds on FastAPI which is built on Starlette . Thanks to them, you will have an API with OpenAPI Specification. It means you will have an automatic documentation webpage and client codes can also be generated automatically. The default address is at: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia frontend builds on Streamlit . The default address is at: http://127.0.0.1:8501","title":"Start the Server"},{"location":"re/get-started/serve-a-json-model/#test-the-api-with-requests-and-postman","text":"Create a test.py with the codes below. Tips You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'json', 'data': 1} Now let's add two more inputs and make the print pretty. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) Run the script again and check the result. $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"Test the API with requests and Postman"},{"location":"re/how-to-guides/custom-frontend/","text":"Custom Frontend Information \u00b6 Pinferencia frontend supports customization on: title of the web page using model display_name as title on the template short description and detail description First Let's Create a Simple Model Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) This will change the default templage title displayed on the right content area. Now start the service: $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get: Custom the Frontend \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) This will change the title displayed on the top of the left side panel. This will change the description below the title of the left side panel. This will change the about information of the page. Now start the service: $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"Custom Frontend"},{"location":"re/how-to-guides/custom-frontend/#custom-frontend-information","text":"Pinferencia frontend supports customization on: title of the web page using model display_name as title on the template short description and detail description","title":"Custom Frontend Information"},{"location":"re/how-to-guides/custom-frontend/#first-lets-create-a-simple-model-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) This will change the default templage title displayed on the right content area. Now start the service: $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"First Let's Create a Simple Model Service"},{"location":"re/how-to-guides/custom-frontend/#custom-the-frontend","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) This will change the title displayed on the top of the left side panel. This will change the description below the title of the left side panel. This will change the about information of the page. Now start the service: $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... you will get:","title":"Custom the Frontend"},{"location":"re/how-to-guides/custom-templates/","text":"Custom Templates \u00b6 Although there are built-in templates, it will never be enough to cover all the scenanrios. Pinferencia supports custom templates. It's easy to customize a template and use it in your service. First let's try to create a simple template: Input a list of numbers. Display the mean, max and min of the numbers. Model \u00b6 The model is simple, and the service can be defined as: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" }) Template \u00b6 Pinferencia provides a BaseTemplate to extend on to build a custom template. JSON Input Field \u00b6 First, we create a JSON input field and display field in two columns. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Start the Service \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... And open the browser you will see: Call Backend and Display Results \u00b6 Add the below highlighted codes to send request to backend and display the result. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Display a button to trigger prediction. Display a spinner while sending the request. Send the data to backend. Display the results in 3 columns. Start the Service again, and You Will Get: \u00b6","title":"Custom Templates"},{"location":"re/how-to-guides/custom-templates/#custom-templates","text":"Although there are built-in templates, it will never be enough to cover all the scenanrios. Pinferencia supports custom templates. It's easy to customize a template and use it in your service. First let's try to create a simple template: Input a list of numbers. Display the mean, max and min of the numbers.","title":"Custom Templates"},{"location":"re/how-to-guides/custom-templates/#model","text":"The model is simple, and the service can be defined as: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" })","title":"Model"},{"location":"re/how-to-guides/custom-templates/#template","text":"Pinferencia provides a BaseTemplate to extend on to build a custom template.","title":"Template"},{"location":"re/how-to-guides/custom-templates/#json-input-field","text":"First, we create a JSON input field and display field in two columns. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, )","title":"JSON Input Field"},{"location":"re/how-to-guides/custom-templates/#start-the-service","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... And open the browser you will see:","title":"Start the Service"},{"location":"re/how-to-guides/custom-templates/#call-backend-and-display-results","text":"Add the below highlighted codes to send request to backend and display the result. frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) Display a button to trigger prediction. Display a spinner while sending the request. Send the data to backend. Display the results in 3 columns.","title":"Call Backend and Display Results"},{"location":"re/how-to-guides/custom-templates/#start-the-service-again-and-you-will-get","text":"","title":"Start the Service again, and You Will Get:"},{"location":"re/how-to-guides/huggingface/dependencies/","text":"For mac users \u00b6 If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Install dependencies \u00b6 You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses transformers or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install Dependencies"},{"location":"re/how-to-guides/huggingface/dependencies/#for-mac-users","text":"If you're working on a M1 Mac like me, you need install cmake and rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"For mac users"},{"location":"re/how-to-guides/huggingface/dependencies/#install-dependencies","text":"You can install dependencies using pip. pip install tqdm boto3 requests regex sentencepiece sacremoses transformers or you can use a docker image instead: docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"Install dependencies"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/","text":"Many of you must have heard of Bert , or transformers . And you may also know huggingface. In this tutorial, let's play with its pytorch transformer model and serve it through REST API How does the model work? \u00b6 With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now Prerequisite \u00b6 Please visit Dependencies Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Bert"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#how-does-the-model-work","text":"With an input of an incomplete sentence, the model will give its prediction: Input Output Paris is the [MASK] of France. Paris is the capital of France. Let's try it out now","title":"How does the model work?"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/bert/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' Response { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200aText Generation Transformer: How to Use & How to Serve \u00b6 What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself? How to Use \u00b6 The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time. How to Deploy \u00b6 Install Pinferencia \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% Create the Service \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Start the Server \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Text Generation - GPT2"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#gpt2-text-generation-transformer-how-to-use-how-to-serve","text":"What is text generation? Input some texts, and the model will predict what the following texts will be. Sounds interesting. How can it be interesting without trying out the model by ourself?","title":"GPT2\u200a-\u200aText Generation Transformer: How to Use &amp; How to\u00a0Serve"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#how-to-use","text":"The model will be downloaded automatically from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) That's it! Let's try it out a little bit: predict ( \"You look amazing today,\" ) And the result: [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] Let's have a look at the first result. You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 That's the thing we're looking for! If you run the prediction again, it'll give different results every time.","title":"How to\u00a0Use"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#how-to-deploy","text":"","title":"How to Deploy"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#install-pinferencia","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Install Pinferencia"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#create-the-service","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, )","title":"Create the Service"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#start-the-server","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Server"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/text-generation/#test-the-service","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' Result: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the Service"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/","text":"Google T5 Translation as a Service with Just 7 lines of Codes \u00b6 What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes? Install Dependencies \u00b6 HuggingFace \u00b6 pip install \"transformers[torch]\" If it doesn\u2019t work, please visit Installation and check their official documentations. Pinferencia \u00b6 pip install \"pinferencia[streamlit]\" Define the Service \u00b6 First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION }) Start the Service \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the Service \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Translation will be selected automatically. curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' Result: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [\"Guten Morgen, liebe Liebe.\"] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Translation - Google T5"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#google-t5-translation-as-a-service-with-just-7-lines-of-codes","text":"What is T5? Text-To-Text Transfer Transformer (T5) from Google gives the power of translation. In the article, we will deploy Google T5 model as a REST API service. Difficult? What about I\u2019ll tell you: you just need to write 7 lines of codes?","title":"Google T5 Translation as a Service with Just 7 lines of Codes"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#install-dependencies","text":"","title":"Install Dependencies"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[torch]\" If it doesn\u2019t work, please visit Installation and check their official documentations.","title":"HuggingFace"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[streamlit]\"","title":"Pinferencia"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#define-the-service","text":"First let\u2019s create the app.py to define the service: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION })","title":"Define the Service"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#start-the-service","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Start the Service"},{"location":"re/how-to-guides/huggingface/pipeline/nlp/translation/#test-the-service","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Translation will be selected automatically. curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' Result: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and print the result: Prediction: [\"Guten Morgen, liebe Liebe.\"] Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the Service"},{"location":"re/how-to-guides/huggingface/pipeline/vision/","text":"In this tutorial, we will explore how to use Hugging Face pipeline, and how to deploy it with Pinferencia as REST API. Prerequisite \u00b6 Please visit Dependencies Download the model and predict \u00b6 The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Amazingly easy! Now let's try: Deploy the model \u00b6 Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Easy, right? Predict \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8501 , and you will have an interactive ui. You can send predict request just there! Improve it \u00b6 However, using the url of the image to predict sometimes is not always convenient. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, ) Predict Again \u00b6 UI Curl Python requests Open http://127.0.0.1:8501 , and the template Image Classification will be selected automatically. curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Image Classification"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#prerequisite","text":"Please visit Dependencies","title":"Prerequisite"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#download-the-model-and-predict","text":"The model will be automatically downloaded. 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) Result: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] Amazingly easy! Now let's try:","title":"Download the model and\u00a0predict"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#deploy-the-model","text":"Without deployment, how could a machine learning tutorial be complete? First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Now let's create an app.py file with the codes: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Easy, right?","title":"Deploy the\u00a0model"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#predict","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] Even cooler, go to http://127.0.0.1:8501 , and you will have an interactive ui. You can send predict request just there!","title":"Predict"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#improve-it","text":"However, using the url of the image to predict sometimes is not always convenient. Let's modify the app.py a little bit to accept Base64 Encoded String as the input. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, )","title":"Improve it"},{"location":"re/how-to-guides/huggingface/pipeline/vision/#predict-again","text":"UI Curl Python requests Open http://127.0.0.1:8501 , and the template Image Classification will be selected automatically. curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' Result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) Run python test.py and result: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"Predict Again"},{"location":"re/how-to-guides/paddlepaddle/dependencies/","text":"Install dependencies \u00b6 # Install the GPU version of paddlepaddle pip install paddlepaddle-gpu -U # Or install the CPU version of paddlepaddle pip install paddlepaddle -U pip install paddlehub Tips In addition to the above dependencies, the pre-trained model and preset data set of paddlehub need to be downloaded from the server. Please ensure the machine can access the network normally. If relevant data sets and pre-trained models already exist locally, paddlehub can run offline.","title":"Install Dependencies"},{"location":"re/how-to-guides/paddlepaddle/dependencies/#install-dependencies","text":"# Install the GPU version of paddlepaddle pip install paddlepaddle-gpu -U # Or install the CPU version of paddlepaddle pip install paddlepaddle -U pip install paddlehub Tips In addition to the above dependencies, the pre-trained model and preset data set of paddlehub need to be downloaded from the server. Please ensure the machine can access the network normally. If relevant data sets and pre-trained models already exist locally, paddlehub can run offline.","title":"Install dependencies"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/","text":"Model basic information \u00b6 Pyramid-Lite is a lightweight model developed by Baidu in 2018 in PyramBox of Computer Vision id 2018 ECCV. It is based on the main network FaceBoxes, measurement, environment, expression changes, meeting changes and other common problem models. The PaddleHub module is based on WIDER FACE data It can be used for face detection based on self-collected face datasets and Baidu self-collected face datasets, which supports prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001pyramidbox_lite_server dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install pyramidbox_lite_server Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' Response { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"Face Detection"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#model-basic-information","text":"Pyramid-Lite is a lightweight model developed by Baidu in 2018 in PyramBox of Computer Vision id 2018 ECCV. It is based on the main network FaceBoxes, measurement, environment, expression changes, meeting changes and other common problem models. The PaddleHub module is based on WIDER FACE data It can be used for face detection based on self-collected face datasets and Baidu self-collected face datasets, which supports prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#2pyramidbox_lite_server-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001pyramidbox_lite_server dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#3download-the-model","text":"hub install pyramidbox_lite_server","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/face_detection/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' Response { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/","text":"Mobile Net \u00b6 MobileNet V2 is a lightweight convolutional neural network. On the basis of MobileNet, it has made two major improvements: Inverted Residuals and Linear bottlenecks. The PaddleHub Module is trained on Baidu's self-built animal dataset and can be used for image classification and feature extraction. Currently, it supports the classification and recognition of 7978 animals. Details of the model can be found in the paper . Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Result: \u677e\u9f20 Let's try it out now Prerequisite \u00b6 1. Dependencies \u00b6 Please visit dependencies 2. mobilenet_v2_animals Prerequisites \u00b6 Package Version paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3. Install the Model \u00b6 hub install pyramidbox_lite_server Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' Response { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Image Classification"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#mobile-net","text":"MobileNet V2 is a lightweight convolutional neural network. On the basis of MobileNet, it has made two major improvements: Inverted Residuals and Linear bottlenecks. The PaddleHub Module is trained on Baidu's self-built animal dataset and can be used for image classification and feature extraction. Currently, it supports the classification and recognition of 7978 animals. Details of the model can be found in the paper . Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"Mobile Net"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Image Source ( https://www.pexels.com ) Result: \u677e\u9f20 Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#1-dependencies","text":"Please visit dependencies","title":"1. Dependencies"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#2-mobilenet_v2_animals-prerequisites","text":"Package Version paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2. mobilenet_v2_animals Prerequisites"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#3-install-the-model","text":"hub install pyramidbox_lite_server","title":"3. Install the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_classification/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' Response { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/","text":"Model basic information \u00b6 This model is encapsulated from [the paddlepaddle version of the photo2cartoon project of Xiaoshi Technology] ( https://github.com/minivision-ai/photo2cartoon-paddle ). Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon Example \u00b6 Input Output Image Source ( https://www.pexels.com ) Let's try it out now Prerequisite \u00b6 1. environment dependent \u00b6 Please visit dependencies 2. mobilenet_v2_animals dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3. Download the model \u00b6 hub install Photo2Cartoon Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' Response { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Image Generation"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#model-basic-information","text":"This model is encapsulated from [the paddlepaddle version of the photo2cartoon project of Xiaoshi Technology] ( https://github.com/minivision-ai/photo2cartoon-paddle ). Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#example","text":"Input Output Image Source ( https://www.pexels.com ) Let's try it out now","title":"Example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#1-environment-dependent","text":"Please visit dependencies","title":"1. environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#2-mobilenet_v2_animals-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2. mobilenet_v2_animals dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#3-download-the-model","text":"hub install Photo2Cartoon","title":"3. Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/image_generation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' Response { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/","text":"Model basic information \u00b6 Vehicle detection is a very important and challenging task in urban traffic monitoring. The difficulty of this task lies in accurately localizing and classifying relatively small vehicles in complex scenes. The network of the PaddleHub Module is YOLOv3, of which the backbone is DarkNet53, which is trained by Baidu's self-built large-scale vehicle data set, and supports the recognition of car, truck, bus, motorbike, tricycle and other models. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001yolov3_darknet53_vehicles dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install yolov3_darknet53_vehicles Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' Response { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Index"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#model-basic-information","text":"Vehicle detection is a very important and challenging task in urban traffic monitoring. The difficulty of this task lies in accurately localizing and classifying relatively small vehicles in complex scenes. The network of the PaddleHub Module is YOLOv3, of which the backbone is DarkNet53, which is trained by Baidu's self-built large-scale vehicle data set, and supports the recognition of car, truck, bus, motorbike, tricycle and other models. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#2yolov3_darknet53_vehicles-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#3download-the-model","text":"hub install yolov3_darknet53_vehicles","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/object_detection/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' Response { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/","text":"Model basic information \u00b6 A lightweight portrait segmentation model based on the ExtremeC3 model. For more details, please refer to: ExtremeC3_Portrait_Segmentation project. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001ExtremeC3_Portrait_Segmentation dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001Download the model \u00b6 hub install ExtremeC3_Portrait_Segmentation Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' Response { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Semantic Segmentation"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#model-basic-information","text":"A lightweight portrait segmentation model based on the ExtremeC3 model. For more details, please refer to: ExtremeC3_Portrait_Segmentation project. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Output Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#2extremec3_portrait_segmentation-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001ExtremeC3_Portrait_Segmentation dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#3download-the-model","text":"hub install ExtremeC3_Portrait_Segmentation","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Url Image To Image will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' Response { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Create the test.py . test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/","text":"Model basic information \u00b6 The chinese_ocr_db_crnn_mobile Module is used to identify the Chinese characters in the picture, identify the Chinese characters in the text box, and then classify the angle of the detected text box. The final text recognition algorithm adopts CRNN (Convolutional Recurrent Neural Network), namely Convolutional Recurrent Neural Network. This Module is an ultra-lightweight Chinese OCR model that supports direct prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile Sample result example \u00b6 Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001yolov3_darknet53_vehicles dependent \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001Download the model \u00b6 hub install chinese_ocr_db_crnn_mobile Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' Response {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Index"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#model-basic-information","text":"The chinese_ocr_db_crnn_mobile Module is used to identify the Chinese characters in the picture, identify the Chinese characters in the text box, and then classify the angle of the detected text box. The final text recognition algorithm adopts CRNN (Convolutional Recurrent Neural Network), namely Convolutional Recurrent Neural Network. This Module is an ultra-lightweight Chinese OCR model that supports direct prediction. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#sample-result-example","text":"Enter the file path and the model will give its predictions\uff1a Input Image Source ( https://www.pexels.com ) Output Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#2yolov3_darknet53_vehicles-dependent","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#3download-the-model","text":"hub install chinese_ocr_db_crnn_mobile","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/image/text_recognition/#test-the-service","text":"Tips The image exists on the service machine, you can enter the relative path of the service file or the absolute path of the file curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' Response {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) Run the script and check the result. $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/","text":"Model basic information \u00b6 Emotion Detection (EmoTect for short) focuses on identifying the emotions of users in intelligent dialogue scenes. For user texts in intelligent dialogue scenes, it automatically determines the emotional category of the text and gives the corresponding confidence. The emotional type is divided into positive , Negative, Neutral. The model is based on TextCNN (Multiple Convolutional Kernel CNN model), which can better capture sentence local correlation. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn Sample result example \u00b6 Input Output [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001emotion_detection_textcnn dependent \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001Download the model \u00b6 hub install emotion_detection_textcnn Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Raw Request will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' Response { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Emotion Detection Textcnn"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#model-basic-information","text":"Emotion Detection (EmoTect for short) focuses on identifying the emotions of users in intelligent dialogue scenes. For user texts in intelligent dialogue scenes, it automatically determines the emotional category of the text and gives the corresponding confidence. The emotional type is divided into positive , Negative, Neutral. The model is based on TextCNN (Multiple Convolutional Kernel CNN model), which can better capture sentence local correlation. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#sample-result-example","text":"Input Output [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#2emotion_detection_textcnn-dependent","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001emotion_detection_textcnn dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#3download-the-model","text":"hub install emotion_detection_textcnn","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Raw Request will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' Response { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/","text":"Model basic information \u00b6 This Module is a word segmentation network (bidirectional GRU) built by jieba using the PaddlePaddle deep learning framework. At the same time, it also supports jieba's traditional word segmentation methods, such as precise mode, full mode, search engine mode and other word segmentation modes. The usage methods are consistent with jieba. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle Sample result example \u00b6 Input Output \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001jieba_paddle dependent \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001Download the model \u00b6 hub install jieba_paddle Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' Response { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Lexical analysis"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#model-basic-information","text":"This Module is a word segmentation network (bidirectional GRU) built by jieba using the PaddlePaddle deep learning framework. At the same time, it also supports jieba's traditional word segmentation methods, such as precise mode, full mode, search engine mode and other word segmentation modes. The usage methods are consistent with jieba. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#sample-result-example","text":"Input Output \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#2jieba_paddle-dependent","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001jieba_paddle dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#3download-the-model","text":"hub install jieba_paddle","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#test-the","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template Text to Text will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' Response { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/","text":"Model basic information \u00b6 Simultaneous interpretation, that is, translation before the sentence is completed, the goal of simultaneous interpretation is to automate simultaneous interpretation, which can be translated at the same time as the source language, with a delay of only a few seconds. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1 Sample result example \u00b6 Input Output [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001transformer_nist_wait_1 dependent \u00b6 paddlepaddle >= 2.1.0 paddlehub >= 2.1.0 3\u3001Download the model \u00b6 hub install transformer_nist_wait_1 Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia . pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Test the service \u00b6 curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' Response on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Create the test.py . test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) Run the script and check the result. $ python test.py # on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Index"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#model-basic-information","text":"Simultaneous interpretation, that is, translation before the sentence is completed, the goal of simultaneous interpretation is to automate simultaneous interpretation, which can be translated at the same time as the source language, with a delay of only a few seconds. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#sample-result-example","text":"Input Output [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#2transformer_nist_wait_1-dependent","text":"paddlepaddle >= 2.1.0 paddlehub >= 2.1.0","title":"2\u3001transformer_nist_wait_1 dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#3download-the-model","text":"hub install transformer_nist_wait_1","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#install-pinferencia","text":"First, let's install Pinferencia . pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) Run the service, and wait for it to load the model and start the server: $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#test-the-service","text":"curl Python Requests Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' Response on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . Create the test.py . test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) Run the script and check the result. $ python test.py # on the server page input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Test the service"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/","text":"Model basic information \u00b6 ERNIE-GEN is a pre-training-fine-tuning framework for generation tasks. For the first time, span-by-span generation tasks are added to the pre-training stage, so that the model can generate a semantically complete segment each time. The exposure bias problem is mitigated by a padding generative mechanism and a noise-aware mechanism in pre-training and fine-tuning. In addition, ERNIE-GEN samples multi-segment-multi-granularity target text sampling strategy, which enhances the correlation between source text and target text, and strengthens the interaction between encoder and decoder. ernie_gen_poetry is fine-tuned on the open source poetry dataset and can be used to generate poetry. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry Sample result example \u00b6 Input Output [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] Let's try it out now Prerequisite \u00b6 1\u3001environment dependent \u00b6 Please visit dependencies 2\u3001ernie_gen_poetry dependent \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp 3\u3001Download the model \u00b6 hub install ernie_gen_poetry Serve the Model \u00b6 Install Pinferencia \u00b6 First, let's install Pinferencia \u3002 pip install \"pinferencia[streamlit]\" Create app.py \u00b6 Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... Test the service \u00b6 UI curl Python Requests Open http://127.0.0.1:8501 , and the template TEXT_TO_TEXT will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' Response { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Text Generation"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#model-basic-information","text":"ERNIE-GEN is a pre-training-fine-tuning framework for generation tasks. For the first time, span-by-span generation tasks are added to the pre-training stage, so that the model can generate a semantically complete segment each time. The exposure bias problem is mitigated by a padding generative mechanism and a noise-aware mechanism in pre-training and fine-tuning. In addition, ERNIE-GEN samples multi-segment-multi-granularity target text sampling strategy, which enhances the correlation between source text and target text, and strengthens the interaction between encoder and decoder. ernie_gen_poetry is fine-tuned on the open source poetry dataset and can be used to generate poetry. Reference\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry","title":"Model basic information"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#sample-result-example","text":"Input Output [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] Let's try it out now","title":"Sample result example"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#prerequisite","text":"","title":"Prerequisite"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#1environment-dependent","text":"Please visit dependencies","title":"1\u3001environment dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#2ernie_gen_poetry-dependent","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp","title":"2\u3001ernie_gen_poetry dependent"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#3download-the-model","text":"hub install ernie_gen_poetry","title":"3\u3001Download the model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#serve-the-model","text":"","title":"Serve the Model"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#install-pinferencia","text":"First, let's install Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"Install Pinferencia"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#create-apppy","text":"Let's save our predict function into a file app.py and add some lines to register it. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) Run the service, and wait for it to load the model and start the server: Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"Create app.py"},{"location":"re/how-to-guides/paddlepaddle/modules/text/text_generation/#test-the-service","text":"UI curl Python Requests Open http://127.0.0.1:8501 , and the template TEXT_TO_TEXT will be selected automatically. Request curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' Response { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Create the test.py . test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) Run the script and check the result. $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } Even cooler, go to http://127.0.0.1:8000 , and you will have a full documentation of your APIs. You can also send predict requests just there!","title":"Test the service"},{"location":"re/how-to-guides/schema/","text":"How to Define the Schema of Request and Response of Your Service? \u00b6 Imagine this: Your service calculates the sum of posted data. It requires a request body: [ 1 , 2 , 3 ] and returns a response body: 6 How do you let the user know what does your request and response body look like? And what if you want to validate/parse the request/response body automatically? In this article, we will walk throught how to define the schema of the request and response of your service in Pinferencia. Python 3 Type Hint \u00b6 Have you heard of type hint in python? If not, you better check it out now at Python Typing . Since Python 3.5, Python starts to support type hint in your function definition. You can declare the type of the arguments and return. Pinferencia use the type hint of your function to define the schema of your request and response. So, you don't need to learn another format, and you can just stay with python. Not all type hints are supported! Not all the type hints in python can be used to define the schema. The type hints need be able to be correctly represented in the json schema. A Dummy Service \u00b6 Let's create a dummy service to show you how everything works. dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) Start the service, and visit the backend documentation page, you will find examples of the request and response: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Here type hint list of the argument of the function will be used to define the data field in request body. Type hint str of the return of the function will be used to define the data field in the response body. The Sum Service \u00b6 Now let's get back to the service at the start of this article, a sum service with: Request Example Response Example [ 1 , 2 , 3 ] 6 Let's rewrite our dummy service Python 3.6 and above Python 3.9 and above dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) Now visit the backend documentation page, the examples will be: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } Besides displaying the schema, Pinferencia also validates and tries to parse the data into the desired types. Let's try out the API on the backend documentation page. Normal Data Invalid Type Data Invalid Data Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's change one of the number in the request to string type. And the number will be converted to integer according to the schema. Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's post some invalid data type, and you will receive a 422 error Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] } Complicated Schema \u00b6 It's possible to define complicated schema in Pinferencia with the help of pydantic . Let's assume a service receive a persion's information: request [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] and simply reply a welcome message: response \"Hello, Will! Hello, Elise!\" Let's define the service: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) Now start the service and visit the backend documentation page, you will find the request and response example as below: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Mission Completed \u00b6 You have learned how to define request and response schema with Pinferencia . You can now try out more schemas you're interested. Have fun!","title":"Define Request and Response Schema"},{"location":"re/how-to-guides/schema/#how-to-define-the-schema-of-request-and-response-of-your-service","text":"Imagine this: Your service calculates the sum of posted data. It requires a request body: [ 1 , 2 , 3 ] and returns a response body: 6 How do you let the user know what does your request and response body look like? And what if you want to validate/parse the request/response body automatically? In this article, we will walk throught how to define the schema of the request and response of your service in Pinferencia.","title":"How to Define the Schema of Request and Response of Your Service?"},{"location":"re/how-to-guides/schema/#python-3-type-hint","text":"Have you heard of type hint in python? If not, you better check it out now at Python Typing . Since Python 3.5, Python starts to support type hint in your function definition. You can declare the type of the arguments and return. Pinferencia use the type hint of your function to define the schema of your request and response. So, you don't need to learn another format, and you can just stay with python. Not all type hints are supported! Not all the type hints in python can be used to define the schema. The type hints need be able to be correctly represented in the json schema.","title":"Python 3 Type Hint"},{"location":"re/how-to-guides/schema/#a-dummy-service","text":"Let's create a dummy service to show you how everything works. dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) Start the service, and visit the backend documentation page, you will find examples of the request and response: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } Here type hint list of the argument of the function will be used to define the data field in request body. Type hint str of the return of the function will be used to define the data field in the response body.","title":"A Dummy Service"},{"location":"re/how-to-guides/schema/#the-sum-service","text":"Now let's get back to the service at the start of this article, a sum service with: Request Example Response Example [ 1 , 2 , 3 ] 6 Let's rewrite our dummy service Python 3.6 and above Python 3.9 and above dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) Now visit the backend documentation page, the examples will be: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } Besides displaying the schema, Pinferencia also validates and tries to parse the data into the desired types. Let's try out the API on the backend documentation page. Normal Data Invalid Type Data Invalid Data Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's change one of the number in the request to string type. And the number will be converted to integer according to the schema. Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } Let's post some invalid data type, and you will receive a 422 error Request Response { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] }","title":"The Sum Service"},{"location":"re/how-to-guides/schema/#complicated-schema","text":"It's possible to define complicated schema in Pinferencia with the help of pydantic . Let's assume a service receive a persion's information: request [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] and simply reply a welcome message: response \"Hello, Will! Hello, Elise!\" Let's define the service: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) Now start the service and visit the backend documentation page, you will find the request and response example as below: Request Example Response Example { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" }","title":"Complicated Schema"},{"location":"re/how-to-guides/schema/#mission-completed","text":"You have learned how to define request and response schema with Pinferencia . You can now try out more schemas you're interested. Have fun!","title":"Mission Completed"},{"location":"re/introduction/overview/","text":"Welcome to Pinferencia \u00b6 What is Pinferencia? \u00b6 Never heard of Pinferencia ? No one's gonna blame ya. No coins in my pocket, can't put this in the rocket. Lots of models you have got, serve them online, not a easy job. Now you've got Pinferencia, all you need is to say \"abracadabra\". Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your deep learning models with a fully functioning Rest API. Straight forward. Simple. Powerful. $ pip install \"pinferencia[streamlit]\" ---> 100% Features \u00b6 Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served. Try it now! \u00b6 Create the App \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) Run! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1/ and have fun.","title":"Overview"},{"location":"re/introduction/overview/#welcome-to-pinferencia","text":"","title":"Welcome to Pinferencia"},{"location":"re/introduction/overview/#what-is-pinferencia","text":"Never heard of Pinferencia ? No one's gonna blame ya. No coins in my pocket, can't put this in the rocket. Lots of models you have got, serve them online, not a easy job. Now you've got Pinferencia, all you need is to say \"abracadabra\". Pinferencia ( python + inference ) aims to provide the simplest way to serve any of your deep learning models with a fully functioning Rest API. Straight forward. Simple. Powerful. $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"What is Pinferencia?"},{"location":"re/introduction/overview/#features","text":"Pinferencia features include: Fast to code, fast to go alive . Minimal codes to write, minimum codes modifications needed. Just based on what you have. 100% Test Coverage : Both statement and branch coverages, no kidding. Easy to use, easy to understand . Automatic API documentation page . All API explained in details with online try-out feature. Thanks to FastAPI and Starlette . Serve any model , even a single function can be served.","title":"Features"},{"location":"re/introduction/overview/#try-it-now","text":"","title":"Try it now!"},{"location":"re/introduction/overview/#create-the-app","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Create the App"},{"location":"re/introduction/overview/#run","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Hooray , your service is alive. Go to http://127.0.0.1/ and have fun.","title":"Run!"},{"location":"re/introduction/pinferencia-is-different/","text":"Why is Pinferencia different? \u00b6 Different? \u00b6 Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now? No way!!!!!!!!!!!! You have your model, you train it in python , and you predict in python . You even write complicated python codes to perform more difficult tasks . How many changes you need to make and how many extra codes you need to write to get your model served using those tools or platforms? The answer is A lot . With Pinferencia \u00b6 You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected . Simple, and Powerful \u00b6 Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Pinferencia is different?"},{"location":"re/introduction/pinferencia-is-different/#why-is-pinferencia-different","text":"","title":"Why is Pinferencia different?"},{"location":"re/introduction/pinferencia-is-different/#different","text":"Actually, it is not something different. It is something more intuitive, more straight forward or just more simple. How do you serve a model yesterday? Write some script, save a model file, or do something else according to the tools' requirements . And you spend a lot of time to understand those requirements. And a lot time to get it right. Once finished, you're so relieved. However, after almost half a year, you've got new and more complicated models and serve them again using your previous tool. What's in your mind now?","title":"Different?"},{"location":"re/introduction/pinferencia-is-different/#with-pinferencia","text":"You don't need to do any of these. You just use the model in your own python code. It doesn't matter whether the model is - a PyTorch model or - a Tensorflow model or - any machine learning model or - simply your own codes or - just your own functions . Register the model/function, and Pinferencia will use it to predict, in the way just as expected .","title":"With Pinferencia"},{"location":"re/introduction/pinferencia-is-different/#simple-and-powerful","text":"Pinferencia aims to be the simplest AI model inference server! Serving a model has never been so easy. If you want to find a simple but robust way to serve your model write minimal codes while maintain controls over you service avoid those heavy tools or platforms You're at the right place.","title":"Simple, and Powerful"},{"location":"re/reference/cli/","text":"CLI \u00b6 Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"re/reference/cli/#cli","text":"Pinfenrecia provides the command pinfer to simplify starting frontend and backend service. You can use pinfer --help to view available options: Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"CLI"},{"location":"re/reference/frontend/requirements/","text":"Requirements \u00b6 To use Pinferencia's frontend with your model, there are some requirements of your model's predict function. Templates \u00b6 Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future. Base Templates \u00b6 Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image Derived Templates \u00b6 Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image Input \u00b6 Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image. Output \u00b6 If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Frontend Requirements"},{"location":"re/reference/frontend/requirements/#requirements","text":"To use Pinferencia's frontend with your model, there are some requirements of your model's predict function.","title":"Requirements"},{"location":"re/reference/frontend/requirements/#templates","text":"Currently, there are mainly two major catogory of the template's inputs and outputs. More (audio and video) will be supported in the future.","title":"Templates"},{"location":"re/reference/frontend/requirements/#base-templates","text":"Template Input Output Text to Text Text Text Text to Image Text Image Image to Text Image Text Camera to Text Image Text Image to Image Image Image Camera to Image Image Image","title":"Base Templates"},{"location":"re/reference/frontend/requirements/#derived-templates","text":"Template Input Output Translation Text Text Image Classification Image Text Image Style Transfer Image Image","title":"Derived Templates"},{"location":"re/reference/frontend/requirements/#input","text":"Based on the schema of the request, frontend end may parse the input into a list or simply a single string. Define Schema About how to define schema of request and response, please visit How to Define the Schema of Request and Response of Your Service? If you define your schema request as a list, for example, List[str], or simply list: The predict function must be able to accept a list of data as inputs. For text input, the input will be a list of strings. For image input, the input will be a list of strings representing the base64 encoded images. Otherwise, The predict function must be able to accept a single data as the input. For text input, the input will be a single string. For image input, the input will be a single string representing the base64 encoded image.","title":"Input"},{"location":"re/reference/frontend/requirements/#output","text":"If you define your schema response as a list, for example, List[str], or simply list: The predict function must produce a list of data as outputs. For text output, the output must be a list. For image output, the output must be a list of strings representing the base64 encoded images. Otherwise, The predict function must produce a single data as the output. For text output, the output should be a single string. For image output, the output should be a single string representing the base64 encoded image. Text Output The frontend will try to parse the text outputs into a table, a json or pure texts. Table Text JSON If the output is similar to below: If the schema of the response is a list [ [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] ] or If the schema of the response is a not list [ { \"a\" : 1 , \"b\" : 2 }, { \"a\" : 3 , \"b\" : 4 }, { \"a\" : 5 , \"b\" : 6 } ] It will be displayed as a table. If the output is similar to below: If the schema of the response is a list [ \"Text output.\" ] or If the schema of the response is a not list \"Text output.\" It will be displayed as a text. All other format of outputs will be displayed as a JSON. For example, [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] or { \"a\" : 1 , \"b\" : 2 }","title":"Output"},{"location":"re/reference/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path PickleHandler \u00b6 The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"re/reference/handlers/#handlers","text":"","title":"Handlers"},{"location":"re/reference/handlers/#basehandler","text":"BaseHandler is only an abstract base class. You can't use it directly. Let's Take a look at some of its functions: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) The default codes do nothing . You can override this function to provide your own pre-processing codes. The default codes do nothing . You can override this function to provide your own post-processing codes. Get predict function from entrypoint name and the model object. Model can be accessed by self.model , the entrypoint registered can be accessed by self.entrypoint . You need to implement this function. Model path can be accessed by self.model_path","title":"BaseHandler"},{"location":"re/reference/handlers/#picklehandler","text":"The default handler is PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"re/reference/models/machine-learning/","text":"Machine Learning Frameworks \u00b6 Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Other Machine Learning Frameworks"},{"location":"re/reference/models/machine-learning/#machine-learning-frameworks","text":"Here is how to load models from different framework: Scikit-Learn PyTorch Tensorflow Any Model Any Function app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"Machine Learning Frameworks"},{"location":"re/reference/models/register/","text":"Register \u00b6 Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) Parameters \u00b6 Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration Examples \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version Name \u00b6 Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict. Metadata \u00b6 Default API \u00b6 Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure. Handler \u00b6 Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Register Models"},{"location":"re/reference/models/register/#register","text":"Registering a model is as easy as: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) Register Multiple Model and Multiple Versions? You can register multiple models with multiple versions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"Register"},{"location":"re/reference/models/register/#parameters","text":"Parameter Type Default Details model_name str Name of the model model object Model object or path version_name str None Name of the version entrypoint str None Name of the function to use metadata dict None Metadata of the model handler object None A class to handler model loading and predicting load_now bool True Whether loading the model on registration","title":"Parameters"},{"location":"re/reference/models/register/#examples","text":"","title":"Examples"},{"location":"re/reference/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"re/reference/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"re/reference/models/register/#version-name","text":"Model without version name will be registered as default version. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version Name"},{"location":"re/reference/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/add/predict The predicting endpoint will be http://127.0.0.1/v1/models/mymodel/versions/substract/predict add function of the model will be used to predict. substract function of the model will be used to predict.","title":"Entrypoint"},{"location":"re/reference/models/register/#metadata","text":"","title":"Metadata"},{"location":"re/reference/models/register/#default-api","text":"Pinferencia default metadata schema supports platform and device These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"Default API"},{"location":"re/reference/models/register/#kserve-api","text":"Pinferencia also supports Kserve API. For Kserve V2, the metadata supports: - platform - inputs - outputs The inputs and outputs metadata will determine the data and datatype model received and returned. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) If you want to use kserve API, you need to set api=\"kserve\" when initializing the service. In the request, if there are multiple inputs, only input with name intergers will be passed to the model. Output data will be converted into int64 . The datatype field only supports numpy data type. If the data cannot be converted, there will be an extra error field in the output, indicating the reason of the failure.","title":"Kserve API"},{"location":"re/reference/models/register/#handler","text":"Details of handlers can be found at Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"re/reference/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"re/reference/restapi/","text":"REST API \u00b6 Overview \u00b6 Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 No Pain, Just Gain \u00b6 As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain. Default API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict Kserve API \u00b6 Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"REST API"},{"location":"re/reference/restapi/#rest-api","text":"","title":"REST API"},{"location":"re/reference/restapi/#overview","text":"Pinferencia has two built-in API sets: Default API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) Are you using other serving tools now? If you also use other model serving tools, here are the Kserve API versions the tools support: Name API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"Overview"},{"location":"re/reference/restapi/#no-pain-just-gain","text":"As you can see You can switch between Pinferencia and other tools with almost no code changes in client. If you want to use Pinferencia for prototyping and client building, then use other tools in production, you got it supported out of the box. You can use Pinferencia in production with other tools with the same API set. If you're switching from Kserve V1 to Kserve V2 and you need a server supporting both during the transition, you got Pinferencia . So, no pain, just gain.","title":"No Pain, Just Gain"},{"location":"re/reference/restapi/#default-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/predict POST Model Predict /v1/models/{model_name}/versions/{version_name}/predict POST Model Version Predict","title":"Default API"},{"location":"re/reference/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET Healthz /v1/models GET List Models /v1/models/{model_name} GET List Model Versions /v1/models/{model_name}/ready GET Model Is Ready /v1/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v1/models/{model_name}/load POST Load Model /v1/models/{model_name}/versions/{version_name}/load POST Load Version /v1/models/{model_name}/unload POST Unload Model /v1/models/{model_name}/versions/{version_name}/unload POST Unload Version /v1/models/{model_name}/infer POST Model Predict /v1/models/{model_name}/versions/{version_name}/infer POST Model Version Predict /v2/healthz GET Healthz /v2/models GET List Models /v2/models/{model_name} GET List Model Versions /v2/models/{model_name}/ready GET Model Is Ready /v2/models/{model_name}/versions/{version_name}/ready GET Model Version Is Ready /v2/models/{model_name}/load POST Load Model /v2/models/{model_name}/versions/{version_name}/load POST Load Version /v2/models/{model_name}/unload POST Unload Model /v2/models/{model_name}/versions/{version_name}/unload POST Unload Version /v2/models/{model_name}/infer POST Model Predict /v2/models/{model_name}/versions/{version_name}/infer POST Model Version Predict","title":"Kserve API"},{"location":"zh/","text":"","title":"\u9996\u9875"},{"location":"zh/background/models/home/","text":"\u6a21\u578b? \u00b6 \u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6a21\u578b"},{"location":"zh/background/models/home/#_1","text":"\u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u6a21\u578b?"},{"location":"zh/get-started/install/","text":"\u5b89\u88c5 Pinferencia \u00b6 \u63a8\u8350\u65b9\u5f0f \u00b6 \u5efa\u8bae\u4f7f\u7528 streamlit \u5b89\u88c5 Pinferencia \u3002 \u60a8\u5c06\u4f53\u9a8c Pinferencia \u7684\u6240\u6709\u529f\u80fd\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100% \u6216\u8005 \u00b6 \u60a8\u4e5f\u53ef\u4ee5\u9009\u62e9\u5b89\u88c5 Pinferencia \u800c\u4e0d\u4f7f\u7528 streamlit \uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u53ea\u80fd\u8fd0\u884c pinferencia \u7684\u540e\u7aef\u3002 $ pip install pinferencia ---> 100%","title":"\u5b89\u88c5"},{"location":"zh/get-started/install/#pinferencia","text":"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/get-started/install/#_1","text":"\u5efa\u8bae\u4f7f\u7528 streamlit \u5b89\u88c5 Pinferencia \u3002 \u60a8\u5c06\u4f53\u9a8c Pinferencia \u7684\u6240\u6709\u529f\u80fd\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"\u63a8\u8350\u65b9\u5f0f"},{"location":"zh/get-started/install/#_2","text":"\u60a8\u4e5f\u53ef\u4ee5\u9009\u62e9\u5b89\u88c5 Pinferencia \u800c\u4e0d\u4f7f\u7528 streamlit \uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u53ea\u80fd\u8fd0\u884c pinferencia \u7684\u540e\u7aef\u3002 $ pip install pinferencia ---> 100%","title":"\u6216\u8005"},{"location":"zh/get-started/introduction/","text":"\u5feb\u901f\u4e0a\u624b \u00b6 Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ecb\u7ecd"},{"location":"zh/get-started/introduction/#_1","text":"Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u5feb\u901f\u4e0a\u624b"},{"location":"zh/get-started/other-models/","text":"\u4e0b\u4e00\u6b65 \u00b6 \u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"zh/get-started/other-models/#_1","text":"\u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"zh/get-started/pytorch-mnist/","text":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt \u90e8\u7f72\u65b9\u6cd5 \u00b6 \u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570 \u00b6 \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01 \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u524d\u7aef\u754c\u9762 \u00b6 \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f Image to Text \u3002 \u4f7f\u7528\u4e0b\u56fe\uff1a \u4f60\u4f1a\u5f97\u5230\uff1a \u540e\u7aefAPI \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1:8000 \u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84 \u00b6 Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002 \u6700\u540e \u00b6 \u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50 \u548c \u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002","title":"\u542f\u52a8 PyTorch MNIST \u6a21\u578b"},{"location":"zh/get-started/pytorch-mnist/#pytorch-mnist","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002","title":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b"},{"location":"zh/get-started/pytorch-mnist/#_1","text":"\u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"zh/get-started/pytorch-mnist/#_2","text":"\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002","title":"\u90e8\u7f72\u65b9\u6cd5"},{"location":"zh/get-started/pytorch-mnist/#_3","text":"","title":"\u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/pytorch-mnist/#_4","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/get-started/pytorch-mnist/#_5","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_6","text":"\u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_7","text":"\u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f Image to Text \u3002 \u4f7f\u7528\u4e0b\u56fe\uff1a \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u524d\u7aef\u754c\u9762"},{"location":"zh/get-started/pytorch-mnist/#api","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1:8000","title":"\u540e\u7aefAPI"},{"location":"zh/get-started/pytorch-mnist/#handler","text":"Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers","title":"\u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84"},{"location":"zh/get-started/pytorch-mnist/#_8","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/get-started/pytorch-mnist/#_9","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_10","text":"\u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/#_11","text":"\u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50 \u548c \u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002","title":"\u6700\u540e"},{"location":"zh/get-started/pytorch-mnist/bonus/","text":"\u7279\u6b8a\u4efb\u52a1 \u00b6 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002 MNIST \u56fe\u50cf\u6c42\u548c \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002 \u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u5bf9\u5b83\u4eec\u6c42\u548c\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u524d\u7aef\u6a21\u677f\uff0c\u63a5\u53d7\u4e24\u4e2a MNIST \u56fe\u50cf\u5e76\u5c06\u5b83\u4eec\u53d1\u9001\u56de\u6211\u4eec\u7684\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002 \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 \u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u677f\uff1f \u60a8\u53ef\u4ee5\u5728 \u81ea\u5b9a\u4e49\u6a21\u677f \u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002 \u81ea\u5b9a\u4e49\u6a21\u677f \u00b6 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u65b0\u6a21\u677f\uff1a sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) \u8fd9\u91cc\u6211\u4eec\u5c06\u5185\u5bb9\u9762\u677f\u5206\u6210\u4e24\u5217\uff0c\u6bcf\u5217\u63a5\u53d7\u4e00\u4e2a MNIST \u56fe\u50cf\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u5982\u679c\u4e24\u5f20\u56fe\u7247\u90fd\u4e0a\u4f20\u4e86\uff0c\u53d1\u9001\u5230\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002 \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 \u5728\u81ea\u5b9a\u4e49\u6a21\u677f\u6587\u4ef6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u4ee3\u7801\u6765\u81ea\u5b9a\u4e49\u524d\u7aef\u670d\u52a1\u3002 sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, ) \u540e\u7aef \u00b6 \u5728\u6211\u4eec\u81ea\u5b9a\u4e49\u524d\u7aef\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u6a21\u578b\u6ce8\u518c\u65f6\u76f4\u63a5\u4f7f\u7528\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u677f\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u6570\u5b57\u5e76\u8fdb\u884c\u6c42\u548c\u3002 \u5c06\u6211\u4eec\u7684\u65b0\u6a21\u677f\u201cSum Mnist\u201d\u6ce8\u518c\u4e3a\u9ed8\u8ba4\u6a21\u677f\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u73a9\u5f97\u5f00\u5fc3**Pinferencia**\uff01","title":"MNIST \u7279\u522b\u4efb\u52a1"},{"location":"zh/get-started/pytorch-mnist/bonus/#_1","text":"\u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002","title":"\u7279\u6b8a\u4efb\u52a1"},{"location":"zh/get-started/pytorch-mnist/bonus/#mnist","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002 \u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u5bf9\u5b83\u4eec\u6c42\u548c\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u524d\u7aef\u6a21\u677f\uff0c\u63a5\u53d7\u4e24\u4e2a MNIST \u56fe\u50cf\u5e76\u5c06\u5b83\u4eec\u53d1\u9001\u56de\u6211\u4eec\u7684\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002","title":"MNIST \u56fe\u50cf\u6c42\u548c"},{"location":"zh/get-started/pytorch-mnist/bonus/#_2","text":"\u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u677f\uff1f \u60a8\u53ef\u4ee5\u5728 \u81ea\u5b9a\u4e49\u6a21\u677f \u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"zh/get-started/pytorch-mnist/bonus/#_3","text":"\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u65b0\u6a21\u677f\uff1a sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) \u8fd9\u91cc\u6211\u4eec\u5c06\u5185\u5bb9\u9762\u677f\u5206\u6210\u4e24\u5217\uff0c\u6bcf\u5217\u63a5\u53d7\u4e00\u4e2a MNIST \u56fe\u50cf\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u5982\u679c\u4e24\u5f20\u56fe\u7247\u90fd\u4e0a\u4f20\u4e86\uff0c\u53d1\u9001\u5230\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u81ea\u5b9a\u4e49\u6a21\u677f"},{"location":"zh/get-started/pytorch-mnist/bonus/#_4","text":"\u5728\u81ea\u5b9a\u4e49\u6a21\u677f\u6587\u4ef6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u4ee3\u7801\u6765\u81ea\u5b9a\u4e49\u524d\u7aef\u670d\u52a1\u3002 sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, )","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"zh/get-started/pytorch-mnist/bonus/#_5","text":"\u5728\u6211\u4eec\u81ea\u5b9a\u4e49\u524d\u7aef\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u6a21\u578b\u6ce8\u518c\u65f6\u76f4\u63a5\u4f7f\u7528\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u677f\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u6570\u5b57\u5e76\u8fdb\u884c\u6c42\u548c\u3002 \u5c06\u6211\u4eec\u7684\u65b0\u6a21\u677f\u201cSum Mnist\u201d\u6ce8\u518c\u4e3a\u9ed8\u8ba4\u6a21\u677f\u3002","title":"\u540e\u7aef"},{"location":"zh/get-started/pytorch-mnist/bonus/#_6","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/pytorch-mnist/bonus/#_7","text":"\u73a9\u5f97\u5f00\u5fc3**Pinferencia**\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/get-started/serve-a-function/","text":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570 \u00b6 \u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002 \u4efb\u52a1 \u00b6 \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) \u542f\u52a8\u670d\u52a1\u5668 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m. \u6b64\u5916 \u00b6 \u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/serve-a-function/#_1","text":"\u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"zh/get-started/serve-a-function/#_2","text":"\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end","title":"\u4efb\u52a1"},{"location":"zh/get-started/serve-a-function/#_3","text":"\u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"zh/get-started/serve-a-function/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1\u5668"},{"location":"zh/get-started/serve-a-function/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m.","title":"\u6d4b\u8bd5 API"},{"location":"zh/get-started/serve-a-function/#_5","text":"\u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u6b64\u5916"},{"location":"zh/get-started/serve-a-json-model/","text":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b \u00b6 \u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model \u5b9a\u4e49 JSON \u6a21\u578b \u00b6 \u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u60a8\u53ef\u4ee5\u4f7f\u7528 Python 3 Type Hints \u6765\u5b9a\u4e49\u6a21\u578b\u670d\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u5728 Define Request and Response Schema \u4e2d\u67e5\u770b Pinferencia \u5982\u4f55\u5229\u7528 Type Hints \u7684\u3002 \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) model_name\uff0c entrypoint \u548c task \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 task \u6307\u793a\u6a21\u578b\u6b63\u5728\u6267\u884c\u7684\u4efb\u52a1\u7c7b\u578b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86\u6a21\u578b\u7684 task \uff0c\u5c06\u81ea\u52a8\u9009\u62e9\u76f8\u5e94\u7684\u524d\u7aef\u6a21\u677f\u3002 \u6a21\u677f\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728 \u524d\u7aef\u8981\u6c42 \u4e2d\u627e\u5230 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee: http://127.0.0.1:8501 , \u4f60\u62e5\u6709\u4e86\u53ef\u4ee5\u4e0e\u4f60\u6a21\u578b\u4ea4\u4e92\u7684\u56fe\u5f62\u4ecb\u9762\u3002 http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u9ed8\u8ba4\u6587\u6863\u5730\u5740\u5728: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia \u524d\u7aef\u57fa\u4e8e Streamlit . \u9ed8\u8ba4\u90e8\u7f72\u5730\u5740\u5728: http://127.0.0.1:8501 \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01 \u4f7f\u7528\u524d\u7aef\u4ecb\u9762 \u00b6 \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': [1]} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u542f\u52a8\u4e00\u4e2a\u7b80\u5355\u7684 JSON \u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#json","text":"\u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model","title":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#json_1","text":"\u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u60a8\u53ef\u4ee5\u4f7f\u7528 Python 3 Type Hints \u6765\u5b9a\u4e49\u6a21\u578b\u670d\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u5728 Define Request and Response Schema \u4e2d\u67e5\u770b Pinferencia \u5982\u4f55\u5229\u7528 Type Hints \u7684\u3002","title":"\u5b9a\u4e49 JSON \u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#_1","text":"\u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) model_name\uff0c entrypoint \u548c task \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 task \u6307\u793a\u6a21\u578b\u6b63\u5728\u6267\u884c\u7684\u4efb\u52a1\u7c7b\u578b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86\u6a21\u578b\u7684 task \uff0c\u5c06\u81ea\u52a8\u9009\u62e9\u76f8\u5e94\u7684\u524d\u7aef\u6a21\u677f\u3002 \u6a21\u677f\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728 \u524d\u7aef\u8981\u6c42 \u4e2d\u627e\u5230","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"zh/get-started/serve-a-json-model/#_2","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee: http://127.0.0.1:8501 , \u4f60\u62e5\u6709\u4e86\u53ef\u4ee5\u4e0e\u4f60\u6a21\u578b\u4ea4\u4e92\u7684\u56fe\u5f62\u4ecb\u9762\u3002 http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u9ed8\u8ba4\u6587\u6863\u5730\u5740\u5728: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia \u524d\u7aef\u57fa\u4e8e Streamlit . \u9ed8\u8ba4\u90e8\u7f72\u5730\u5740\u5728: http://127.0.0.1:8501 \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/get-started/serve-a-json-model/#_3","text":"","title":"\u4f7f\u7528\u524d\u7aef\u4ecb\u9762"},{"location":"zh/get-started/serve-a-json-model/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': [1]} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u6d4b\u8bd5 API"},{"location":"zh/how-to-guides/custom-frontend/","text":"\u81ea\u5b9a\u4e49\u524d\u7aef\u4fe1\u606f \u00b6 Pinferencia \u524d\u7aef\u652f\u6301\u81ea\u5b9a\u4e49\uff1a \u7f51\u9875\u7684\u6807\u9898 \u4f7f\u7528\u6a21\u578b display_name \u4f5c\u4e3a\u6a21\u677f\u7684\u6807\u9898 \u7b80\u77ed\u7684\u4ecb\u7ecd \u548c\u8be6\u7ec6\u8bf4\u660e \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) \u8fd9\u5c06\u66f4\u6539\u53f3\u4fa7\u5185\u5bb9\u533a\u57df\u663e\u793a\u7684\u9ed8\u8ba4\u6a21\u677f\u6807\u9898\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) \u8fd9\u5c06\u6539\u53d8\u5de6\u4fa7\u9762\u677f\u9876\u90e8\u663e\u793a\u7684\u6807\u9898\u3002 \u8fd9\u5c06\u66f4\u6539\u5de6\u4fa7\u9762\u677f\u6807\u9898\u4e0b\u65b9\u7684\u63cf\u8ff0\u3002 \u8fd9\u5c06\u6539\u53d8\u9875\u9762\u7684\u5173\u4e8e\u4fe1\u606f\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"zh/how-to-guides/custom-frontend/#_1","text":"Pinferencia \u524d\u7aef\u652f\u6301\u81ea\u5b9a\u4e49\uff1a \u7f51\u9875\u7684\u6807\u9898 \u4f7f\u7528\u6a21\u578b display_name \u4f5c\u4e3a\u6a21\u677f\u7684\u6807\u9898 \u7b80\u77ed\u7684\u4ecb\u7ecd \u548c\u8be6\u7ec6\u8bf4\u660e","title":"\u81ea\u5b9a\u4e49\u524d\u7aef\u4fe1\u606f"},{"location":"zh/how-to-guides/custom-frontend/#_2","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) \u8fd9\u5c06\u66f4\u6539\u53f3\u4fa7\u5185\u5bb9\u533a\u57df\u663e\u793a\u7684\u9ed8\u8ba4\u6a21\u677f\u6807\u9898\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u670d\u52a1"},{"location":"zh/how-to-guides/custom-frontend/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) \u8fd9\u5c06\u6539\u53d8\u5de6\u4fa7\u9762\u677f\u9876\u90e8\u663e\u793a\u7684\u6807\u9898\u3002 \u8fd9\u5c06\u66f4\u6539\u5de6\u4fa7\u9762\u677f\u6807\u9898\u4e0b\u65b9\u7684\u63cf\u8ff0\u3002 \u8fd9\u5c06\u6539\u53d8\u9875\u9762\u7684\u5173\u4e8e\u4fe1\u606f\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"zh/how-to-guides/custom-templates/","text":"\u81ea\u5b9a\u4e49\u6a21\u677f \u00b6 \u5c3d\u7ba1\u6709\u5185\u7f6e\u6a21\u677f\uff0c\u4f46\u5b83\u6c38\u8fdc\u4e0d\u8db3\u4ee5\u6db5\u76d6\u6240\u6709\u573a\u666f\u3002 Pinferencia \u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u677f\u3002\u81ea\u5b9a\u4e49\u6a21\u677f\u5e76\u5728\u60a8\u7684\u670d\u52a1\u4e2d\u4f7f\u7528\u5b83\u5f88\u5bb9\u6613\u3002 \u9996\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u677f\uff1a \u8f93\u5165\u6570\u5b57\u5217\u8868\u3002 \u663e\u793a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3001\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002 \u6a21\u578b \u00b6 \u6a21\u578b\u5f88\u7b80\u5355\uff0c\u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" }) \u6a21\u677f \u00b6 Pinferencia \u63d0\u4f9b\u4e86 BaseTemplate \u6765\u6269\u5c55\u4ee5\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u677f\u3002 JSON \u8f93\u5165 \u00b6 \u9996\u5148\uff0c\u6211\u4eec\u5c06\u9875\u9762\u5206\u4e3a\u4e24\u5217\uff0c\u5e76\u5206\u522b\u521b\u5efa\u4e00\u4e2a JSON \u8f93\u5165\u5b57\u6bb5\u548c\u663e\u793a\u5b57\u6bb5\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u542f\u52a8\u670d\u52a1 \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u5e76\u6253\u5f00\u6d4f\u89c8\u5668\u4f60\u4f1a\u770b\u5230\uff1a \u8c03\u7528\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c \u00b6 \u6dfb\u52a0\u4ee5\u4e0b\u9ad8\u4eae\u663e\u793a\u7684\u4ee3\u7801\u4ee5\u5c06\u8bf7\u6c42\u53d1\u9001\u5230\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u63d0\u4f9b\u4e00\u4e2a\u6309\u94ae\u6765\u89e6\u53d1\u9884\u6d4b\u3002 \u53d1\u9001\u8bf7\u6c42\u65f6\u663e\u793a\u4e00\u4e2a\u7b49\u5f85\u6548\u679c\u3002 \u5c06\u6570\u636e\u53d1\u9001\u5230\u540e\u7aef\u3002 \u5c06\u7ed3\u679c\u5206\u4e3a\u4e09\u5217\u5c55\u793a\u3002 \u518d\u6b21\u542f\u52a8\u670d\u52a1\uff0c\u60a8\u5c06\u770b\u5230\uff1a \u00b6","title":"\u81ea\u5b9a\u4e49\u6a21\u7248"},{"location":"zh/how-to-guides/custom-templates/#_1","text":"\u5c3d\u7ba1\u6709\u5185\u7f6e\u6a21\u677f\uff0c\u4f46\u5b83\u6c38\u8fdc\u4e0d\u8db3\u4ee5\u6db5\u76d6\u6240\u6709\u573a\u666f\u3002 Pinferencia \u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u677f\u3002\u81ea\u5b9a\u4e49\u6a21\u677f\u5e76\u5728\u60a8\u7684\u670d\u52a1\u4e2d\u4f7f\u7528\u5b83\u5f88\u5bb9\u6613\u3002 \u9996\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u677f\uff1a \u8f93\u5165\u6570\u5b57\u5217\u8868\u3002 \u663e\u793a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3001\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002","title":"\u81ea\u5b9a\u4e49\u6a21\u677f"},{"location":"zh/how-to-guides/custom-templates/#_2","text":"\u6a21\u578b\u5f88\u7b80\u5355\uff0c\u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" })","title":"\u6a21\u578b"},{"location":"zh/how-to-guides/custom-templates/#_3","text":"Pinferencia \u63d0\u4f9b\u4e86 BaseTemplate \u6765\u6269\u5c55\u4ee5\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u677f\u3002","title":"\u6a21\u677f"},{"location":"zh/how-to-guides/custom-templates/#json","text":"\u9996\u5148\uff0c\u6211\u4eec\u5c06\u9875\u9762\u5206\u4e3a\u4e24\u5217\uff0c\u5e76\u5206\u522b\u521b\u5efa\u4e00\u4e2a JSON \u8f93\u5165\u5b57\u6bb5\u548c\u663e\u793a\u5b57\u6bb5\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, )","title":"JSON \u8f93\u5165"},{"location":"zh/how-to-guides/custom-templates/#_4","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u5e76\u6253\u5f00\u6d4f\u89c8\u5668\u4f60\u4f1a\u770b\u5230\uff1a","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/how-to-guides/custom-templates/#_5","text":"\u6dfb\u52a0\u4ee5\u4e0b\u9ad8\u4eae\u663e\u793a\u7684\u4ee3\u7801\u4ee5\u5c06\u8bf7\u6c42\u53d1\u9001\u5230\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u63d0\u4f9b\u4e00\u4e2a\u6309\u94ae\u6765\u89e6\u53d1\u9884\u6d4b\u3002 \u53d1\u9001\u8bf7\u6c42\u65f6\u663e\u793a\u4e00\u4e2a\u7b49\u5f85\u6548\u679c\u3002 \u5c06\u6570\u636e\u53d1\u9001\u5230\u540e\u7aef\u3002 \u5c06\u7ed3\u679c\u5206\u4e3a\u4e09\u5217\u5c55\u793a\u3002","title":"\u8c03\u7528\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c"},{"location":"zh/how-to-guides/custom-templates/#_6","text":"","title":"\u518d\u6b21\u542f\u52a8\u670d\u52a1\uff0c\u60a8\u5c06\u770b\u5230\uff1a"},{"location":"zh/how-to-guides/huggingface/dependencies/","text":"\u5bf9\u4e8emac\u7528\u6237 \u00b6 \u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh \u5b89\u88c5\u4f9d\u8d56 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses transformers \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/how-to-guides/huggingface/dependencies/#mac","text":"\u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"\u5bf9\u4e8emac\u7528\u6237"},{"location":"zh/how-to-guides/huggingface/dependencies/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses transformers \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/","text":"\u4f60\u4eec\u4e2d\u7684\u8bb8\u591a\u4eba\u4e00\u5b9a\u542c\u8bf4\u8fc7\u201cBert\u201d\u6216\u201ctransformers\u201d\u3002 \u4f60\u53ef\u80fd\u8fd8\u77e5\u9053huggingface\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u8ba9\u6211\u4eec\u4f7f\u7528\u5b83\u7684 pytorch \u8f6c\u6362\u5668\u6a21\u578b\u5e76\u901a\u8fc7 REST API \u4e3a\u5b83\u63d0\u4f9b\u670d\u52a1 \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"Bert"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#_1","text":"\u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#_2","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#_3","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/bert/#_4","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1 \u00b6 \u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f \u5982\u4f55\u4f7f\u7528 \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u5982\u4f55\u90e8\u7f72 \u00b6 \u5b89\u88c5 Pinferencia \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% \u521b\u5efa\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u672c\u751f\u6210 - GPT2"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#gpt2-","text":"\u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f","title":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#_1","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002","title":"\u5982\u4f55\u4f7f\u7528"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#_2","text":"","title":"\u5982\u4f55\u90e8\u7f72"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#pinferencia","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"\u5b89\u88c5Pinferencia"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, )","title":"\u521b\u5efa\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#_4","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/text-generation/#_5","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/","text":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801 \u00b6 \u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f \u5b89\u88c5\u4f9d\u8d56 \u00b6 HuggingFace \u00b6 pip install \"transformers[torch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002 Pinferencia \u00b6 pip install \"pinferencia[streamlit]\" \u5b9a\u4e49\u670d\u52a1 \u00b6 \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION }) \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Translation \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [\"Guten Morgen, liebe Liebe.\"] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u7ffb\u8bd1 - Google T5"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#google-t5-7","text":"\u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f","title":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#_1","text":"","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[torch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002","title":"HuggingFace"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[streamlit]\"","title":"Pinferencia"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#_2","text":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION })","title":"\u5b9a\u4e49\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#_3","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/nlp/translation/#_4","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Translation \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [\"Guten Morgen, liebe Liebe.\"] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528 Hugging Face \u7ba1\u9053\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528 Pinferencia \u4f5c\u4e3a REST API \u90e8\u7f72\u5b83\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u5982\u6b64\u7b80\u5355\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a \u90e8\u7f72\u6a21\u578b \u00b6 \u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f \u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01 \u8fdb\u4e00\u6b65\u6539\u8fdb \u00b6 \u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, ) \u518d\u6b21\u9884\u6d4b \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f \u56fe\u7247\u5206\u7c7b \u3002 curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_2","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u5982\u6b64\u7b80\u5355\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a","title":"\u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_3","text":"\u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f","title":"\u90e8\u7f72\u6a21\u578b"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_4","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u9884\u6d4b"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_5","text":"\u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, )","title":"\u8fdb\u4e00\u6b65\u6539\u8fdb"},{"location":"zh/how-to-guides/huggingface/pipeline/vision/#_6","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f \u56fe\u7247\u5206\u7c7b \u3002 curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u518d\u6b21\u9884\u6d4b"},{"location":"zh/how-to-guides/paddlepaddle/dependencies/","text":"\u5b89\u88c5\u4f9d\u8d56 \u00b6 # \u5b89\u88c5gpu\u7248\u672c\u7684PaddlePaddle pip install paddlepaddle-gpu -U # \u6216\u8005\u5b89\u88c5cpu\u7248\u672c\u7684paddlepaddle pip install paddlepaddle -U pip install paddlehub \u63d0\u793a \u9664\u4e0a\u8ff0\u4f9d\u8d56\u5916\uff0cPaddleHub\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9884\u7f6e\u6570\u636e\u96c6\u9700\u8981\u8fde\u63a5\u670d\u52a1\u7aef\u8fdb\u884c\u4e0b\u8f7d\uff0c\u8bf7\u786e\u4fdd\u673a\u5668\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u3002\u82e5\u672c\u5730\u5df2\u5b58\u5728\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u53ef\u4ee5\u79bb\u7ebf\u8fd0\u884cPaddleHub\u3002","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/dependencies/#_1","text":"# \u5b89\u88c5gpu\u7248\u672c\u7684PaddlePaddle pip install paddlepaddle-gpu -U # \u6216\u8005\u5b89\u88c5cpu\u7248\u672c\u7684paddlepaddle pip install paddlepaddle -U pip install paddlehub \u63d0\u793a \u9664\u4e0a\u8ff0\u4f9d\u8d56\u5916\uff0cPaddleHub\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9884\u7f6e\u6570\u636e\u96c6\u9700\u8981\u8fde\u63a5\u670d\u52a1\u7aef\u8fdb\u884c\u4e0b\u8f7d\uff0c\u8bf7\u786e\u4fdd\u673a\u5668\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u3002\u82e5\u672c\u5730\u5df2\u5b58\u5728\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u53ef\u4ee5\u79bb\u7ebf\u8fd0\u884cPaddleHub\u3002","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 PyramidBox-Lite \u662f\u57fa\u4e8e 2018 \u5e74\u767e\u5ea6\u53d1\u8868\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u9876\u7ea7\u4f1a\u8bae ECCV 2018 \u7684\u8bba\u6587 PyramidBox \u800c\u7814\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6a21\u578b\u57fa\u4e8e\u4e3b\u5e72\u7f51\u7edc FaceBoxes\uff0c\u5bf9\u4e8e\u5149\u7167\u3001\u53e3\u7f69\u906e\u6321\u3001\u8868\u60c5\u53d8\u5316\u3001\u5c3a\u5ea6\u53d8\u5316\u7b49\u5e38\u89c1\u95ee\u9898\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5 PaddleHub Module \u57fa\u4e8e WIDER FACE \u6570\u636e\u96c6\u548c\u767e\u5ea6\u81ea\u91c7\u4eba\u8138\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u9884\u6d4b\uff0c\u53ef\u7528\u4e8e\u4eba\u8138\u68c0\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) ![\u4eba\u8138](/assets/images/examples/paddle/face.jpg){ width=\"300\" } \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001pyramidbox_lite_server \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install pyramidbox_lite_server \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' \u54cd\u5e94 { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"\u4eba\u8138\u68c0\u6d4b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#_1","text":"PyramidBox-Lite \u662f\u57fa\u4e8e 2018 \u5e74\u767e\u5ea6\u53d1\u8868\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u9876\u7ea7\u4f1a\u8bae ECCV 2018 \u7684\u8bba\u6587 PyramidBox \u800c\u7814\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6a21\u578b\u57fa\u4e8e\u4e3b\u5e72\u7f51\u7edc FaceBoxes\uff0c\u5bf9\u4e8e\u5149\u7167\u3001\u53e3\u7f69\u906e\u6321\u3001\u8868\u60c5\u53d8\u5316\u3001\u5c3a\u5ea6\u53d8\u5316\u7b49\u5e38\u89c1\u95ee\u9898\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5 PaddleHub Module \u57fa\u4e8e WIDER FACE \u6570\u636e\u96c6\u548c\u767e\u5ea6\u81ea\u91c7\u4eba\u8138\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u9884\u6d4b\uff0c\u53ef\u7528\u4e8e\u4eba\u8138\u68c0\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) ![\u4eba\u8138](/assets/images/examples/paddle/face.jpg){ width=\"300\" } \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#2pyramidbox_lite_server","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001pyramidbox_lite_server \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#3","text":"hub install pyramidbox_lite_server","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/face_detection/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' \u54cd\u5e94 { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 MobileNet V2 \u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728 MobileNet \u7684\u57fa\u7840\u4e0a\uff0c\u505a\u4e86 Inverted Residuals \u548c Linear bottlenecks \u8fd9\u4e24\u5927\u6539\u8fdb\u3002\u8be5 PaddleHub Module \u662f\u5728\u767e\u5ea6\u81ea\u5efa\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5f53\u524d\u5df2\u652f\u6301 7978 \u79cd\u52a8\u7269\u7684\u5206\u7c7b\u8bc6\u522b\u3002\u6a21\u578b\u7684\u8be6\u60c5\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u677e\u9f20 \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install pyramidbox_lite_server \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' \u54cd\u5e94 { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#_1","text":"MobileNet V2 \u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728 MobileNet \u7684\u57fa\u7840\u4e0a\uff0c\u505a\u4e86 Inverted Residuals \u548c Linear bottlenecks \u8fd9\u4e24\u5927\u6539\u8fdb\u3002\u8be5 PaddleHub Module \u662f\u5728\u767e\u5ea6\u81ea\u5efa\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5f53\u524d\u5df2\u652f\u6301 7978 \u79cd\u52a8\u7269\u7684\u5206\u7c7b\u8bc6\u522b\u3002\u6a21\u578b\u7684\u8be6\u60c5\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u677e\u9f20 \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#2mobilenet_v2_animals","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#3","text":"hub install pyramidbox_lite_server","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_classification/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' \u54cd\u5e94 { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u672c\u6a21\u578b\u5c01\u88c5\u81ea \u5c0f\u89c6\u79d1\u6280 photo2cartoon \u9879\u76ee\u7684 paddlepaddle \u7248\u672c \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon \u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install Photo2Cartoon \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efa app.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' \u54cd\u5e94 { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u56fe\u50cf\u751f\u6210"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#_1","text":"\u672c\u6a21\u578b\u5c01\u88c5\u81ea \u5c0f\u89c6\u79d1\u6280 photo2cartoon \u9879\u76ee\u7684 paddlepaddle \u7248\u672c \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#_2","text":"\u8f93\u5165 \u8f93\u51fa \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#2mobilenet_v2_animals","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#3","text":"hub install Photo2Cartoon","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efa app.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/image_generation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' \u54cd\u5e94 { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u8f66\u8f86\u68c0\u6d4b\u662f\u57ce\u5e02\u4ea4\u901a\u76d1\u63a7\u4e2d\u975e\u5e38\u91cd\u8981\u5e76\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u7684\u96be\u5ea6\u5728\u4e8e\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u76f8\u5bf9\u8f83\u5c0f\u7684\u8f66\u8f86\u8fdb\u884c\u7cbe\u51c6\u5730\u5b9a\u4f4d\u548c\u5206\u7c7b\u3002\u8be5 PaddleHub Module \u7684\u7f51\u7edc\u4e3a YOLOv3, \u5176\u4e2d backbone \u4e3a DarkNet53\uff0c\u91c7\u7528\u767e\u5ea6\u81ea\u5efa\u5927\u89c4\u6a21\u8f66\u8f86\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff0c\u652f\u6301 car (\u6c7d\u8f66)\u3001truck (\u5361\u8f66)\u3001bus (\u516c\u4ea4\u8f66)\u3001motorbike (\u6469\u6258\u8f66)\u3001tricycle (\u4e09\u8f6e\u8f66)\u7b49\u8f66\u578b\u7684\u8bc6\u522b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001yolov3_darknet53_vehicles \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install yolov3_darknet53_vehicles \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' \u54cd\u5e94 { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Index"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#_1","text":"\u8f66\u8f86\u68c0\u6d4b\u662f\u57ce\u5e02\u4ea4\u901a\u76d1\u63a7\u4e2d\u975e\u5e38\u91cd\u8981\u5e76\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u7684\u96be\u5ea6\u5728\u4e8e\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u76f8\u5bf9\u8f83\u5c0f\u7684\u8f66\u8f86\u8fdb\u884c\u7cbe\u51c6\u5730\u5b9a\u4f4d\u548c\u5206\u7c7b\u3002\u8be5 PaddleHub Module \u7684\u7f51\u7edc\u4e3a YOLOv3, \u5176\u4e2d backbone \u4e3a DarkNet53\uff0c\u91c7\u7528\u767e\u5ea6\u81ea\u5efa\u5927\u89c4\u6a21\u8f66\u8f86\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff0c\u652f\u6301 car (\u6c7d\u8f66)\u3001truck (\u5361\u8f66)\u3001bus (\u516c\u4ea4\u8f66)\u3001motorbike (\u6469\u6258\u8f66)\u3001tricycle (\u4e09\u8f6e\u8f66)\u7b49\u8f66\u578b\u7684\u8bc6\u522b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#2yolov3_darknet53_vehicles","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#3","text":"hub install yolov3_darknet53_vehicles","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/object_detection/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' \u54cd\u5e94 { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u57fa\u4e8e ExtremeC3 \u6a21\u578b\u5b9e\u73b0\u7684\u8f7b\u91cf\u5316\u4eba\u50cf\u5206\u5272\u6a21\u578b, \u66f4\u591a\u8be6\u60c5\u8bf7\u53c2\u8003\uff1a ExtremeC3_Portrait_Segmentation \u9879\u76ee\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation \u6837\u4f8b\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001ExtremeC3_Portrait_Segmentation \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install ExtremeC3_Portrait_Segmentation \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' \u54cd\u5e94 { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u8bed\u4e49\u5206\u5272"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_1","text":"\u57fa\u4e8e ExtremeC3 \u6a21\u578b\u5b9e\u73b0\u7684\u8f7b\u91cf\u5316\u4eba\u50cf\u5206\u5272\u6a21\u578b, \u66f4\u591a\u8be6\u60c5\u8bf7\u53c2\u8003\uff1a ExtremeC3_Portrait_Segmentation \u9879\u76ee\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_2","text":"\u8f93\u5165 \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u4f8b\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#2extremec3_portrait_segmentation","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001ExtremeC3_Portrait_Segmentation \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#3","text":"hub install ExtremeC3_Portrait_Segmentation","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' \u54cd\u5e94 { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 chinese_ocr_db_crnn_mobile Module \u7528\u4e8e\u8bc6\u522b\u56fe\u7247\u5f53\u4e2d\u7684\u6c49\u5b57, \u8bc6\u522b\u6587\u672c\u6846\u4e2d\u7684\u4e2d\u6587\u6587\u5b57,\u518d\u5bf9\u68c0\u6d4b\u6587\u672c\u6846\u8fdb\u884c\u89d2\u5ea6\u5206\u7c7b\u3002\u6700\u7ec8\u8bc6\u522b\u6587\u5b57\u7b97\u6cd5\u91c7\u7528 CRNN\uff08Convolutional Recurrent Neural Network\uff09\u5373\u5377\u79ef\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3002\u8be5 Module \u662f\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587 OCR \u6a21\u578b\uff0c\u652f\u6301\u76f4\u63a5\u9884\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 pip3 install shapely pyclipper 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install chinese_ocr_db_crnn_mobile \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) # mkldnn\u52a0\u901f\u4ec5\u5728CPU\u4e0b\u6709\u6548 def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' \u54cd\u5e94 {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Index"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#_1","text":"chinese_ocr_db_crnn_mobile Module \u7528\u4e8e\u8bc6\u522b\u56fe\u7247\u5f53\u4e2d\u7684\u6c49\u5b57, \u8bc6\u522b\u6587\u672c\u6846\u4e2d\u7684\u4e2d\u6587\u6587\u5b57,\u518d\u5bf9\u68c0\u6d4b\u6587\u672c\u6846\u8fdb\u884c\u89d2\u5ea6\u5206\u7c7b\u3002\u6700\u7ec8\u8bc6\u522b\u6587\u5b57\u7b97\u6cd5\u91c7\u7528 CRNN\uff08Convolutional Recurrent Neural Network\uff09\u5373\u5377\u79ef\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3002\u8be5 Module \u662f\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587 OCR \u6a21\u578b\uff0c\u652f\u6301\u76f4\u63a5\u9884\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#2mobilenet_v2_animals","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 pip3 install shapely pyclipper","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#3","text":"hub install chinese_ocr_db_crnn_mobile","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) # mkldnn\u52a0\u901f\u4ec5\u5728CPU\u4e0b\u6709\u6548 def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/image/text_recognition/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' \u54cd\u5e94 {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\uff08Emotion Detection\uff0c\u7b80\u79f0 EmoTect\uff09\u4e13\u6ce8\u4e8e\u8bc6\u522b\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7528\u6237\u7684\u60c5\u7eea\uff0c\u9488\u5bf9\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u7528\u6237\u6587\u672c\uff0c\u81ea\u52a8\u5224\u65ad\u8be5\u6587\u672c\u7684\u60c5\u7eea\u7c7b\u522b\u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u7f6e\u4fe1\u5ea6\uff0c\u60c5\u7eea\u7c7b\u578b\u5206\u4e3a\u79ef\u6781\u3001\u6d88\u6781\u3001\u4e2d\u6027\u3002\u8be5\u6a21\u578b\u57fa\u4e8eTextCNN\uff08\u591a\u5377\u79ef\u6838 CNN \u6a21\u578b\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u5c40\u90e8\u76f8\u5173\u6027\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001emotion_detection_textcnn \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install emotion_detection_textcnn \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efa app.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Raw Request \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' \u54cd\u5e94 { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u60c5\u7eea\u5206\u6790"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_1","text":"\u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\uff08Emotion Detection\uff0c\u7b80\u79f0 EmoTect\uff09\u4e13\u6ce8\u4e8e\u8bc6\u522b\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7528\u6237\u7684\u60c5\u7eea\uff0c\u9488\u5bf9\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u7528\u6237\u6587\u672c\uff0c\u81ea\u52a8\u5224\u65ad\u8be5\u6587\u672c\u7684\u60c5\u7eea\u7c7b\u522b\u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u7f6e\u4fe1\u5ea6\uff0c\u60c5\u7eea\u7c7b\u578b\u5206\u4e3a\u79ef\u6781\u3001\u6d88\u6781\u3001\u4e2d\u6027\u3002\u8be5\u6a21\u578b\u57fa\u4e8eTextCNN\uff08\u591a\u5377\u79ef\u6838 CNN \u6a21\u578b\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u5c40\u90e8\u76f8\u5173\u6027\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_2","text":"\u8f93\u5165 \u8f93\u51fa [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#2emotion_detection_textcnn","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001emotion_detection_textcnn \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#3","text":"hub install emotion_detection_textcnn","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efa app.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Raw Request \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' \u54cd\u5e94 { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u8be5 Module \u662f jieba \u4f7f\u7528 PaddlePaddle \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u642d\u5efa\u7684\u5207\u8bcd\u7f51\u7edc\uff08\u53cc\u5411 GRU\uff09\u3002\u540c\u65f6\u4e5f\u652f\u6301 jieba \u7684\u4f20\u7edf\u5207\u8bcd\u65b9\u6cd5\uff0c\u5982\u7cbe\u786e\u6a21\u5f0f\u3001\u5168\u6a21\u5f0f\u3001\u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u7b49\u5207\u8bcd\u6a21\u5f0f\uff0c\u4f7f\u7528\u65b9\u6cd5\u548c jieba \u4fdd\u6301\u4e00\u81f4\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001jieba_paddle \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install jieba_paddle \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' \u54cd\u5e94 { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u8bed\u6cd5\u5206\u6790"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_1","text":"\u8be5 Module \u662f jieba \u4f7f\u7528 PaddlePaddle \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u642d\u5efa\u7684\u5207\u8bcd\u7f51\u7edc\uff08\u53cc\u5411 GRU\uff09\u3002\u540c\u65f6\u4e5f\u652f\u6301 jieba \u7684\u4f20\u7edf\u5207\u8bcd\u65b9\u6cd5\uff0c\u5982\u7cbe\u786e\u6a21\u5f0f\u3001\u5168\u6a21\u5f0f\u3001\u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u7b49\u5207\u8bcd\u6a21\u5f0f\uff0c\u4f7f\u7528\u65b9\u6cd5\u548c jieba \u4fdd\u6301\u4e00\u81f4\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_2","text":"\u8f93\u5165 \u8f93\u51fa \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#2jieba_paddle","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001jieba_paddle \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#3","text":"hub install jieba_paddle","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' \u54cd\u5e94 { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u540c\u58f0\u4f20\u8bd1\uff08Simultaneous Translation\uff09\uff0c\u5373\u5728\u53e5\u5b50\u5b8c\u6210\u4e4b\u524d\u8fdb\u884c\u7ffb\u8bd1\uff0c\u540c\u58f0\u4f20\u8bd1\u7684\u76ee\u6807\u662f\u5b9e\u73b0\u540c\u58f0\u4f20\u8bd1\u7684\u81ea\u52a8\u5316\uff0c\u5b83\u53ef\u4ee5\u4e0e\u6e90\u8bed\u8a00\u540c\u65f6\u7ffb\u8bd1\uff0c\u5ef6\u8fdf\u65f6\u95f4\u53ea\u6709\u51e0\u79d2\u949f\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1 \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001transformer_nist_wait_1 \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.1.0 paddlehub >= 2.1.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install transformer_nist_wait_1 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' \u54cd\u5e94 \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py # \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Index"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_1","text":"\u540c\u58f0\u4f20\u8bd1\uff08Simultaneous Translation\uff09\uff0c\u5373\u5728\u53e5\u5b50\u5b8c\u6210\u4e4b\u524d\u8fdb\u884c\u7ffb\u8bd1\uff0c\u540c\u58f0\u4f20\u8bd1\u7684\u76ee\u6807\u662f\u5b9e\u73b0\u540c\u58f0\u4f20\u8bd1\u7684\u81ea\u52a8\u5316\uff0c\u5b83\u53ef\u4ee5\u4e0e\u6e90\u8bed\u8a00\u540c\u65f6\u7ffb\u8bd1\uff0c\u5ef6\u8fdf\u65f6\u95f4\u53ea\u6709\u51e0\u79d2\u949f\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#2transformer_nist_wait_1","text":"paddlepaddle >= 2.1.0 paddlehub >= 2.1.0","title":"2\u3001transformer_nist_wait_1 \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#3","text":"hub install transformer_nist_wait_1","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_5","text":"curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' \u54cd\u5e94 \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py # \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 ERNIE-GEN \u662f\u9762\u5411\u751f\u6210\u4efb\u52a1\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u9996\u6b21\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165 span-by-span \u751f\u6210\u4efb\u52a1\uff0c\u8ba9\u6a21\u578b\u6bcf\u6b21\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u8bed\u4e49\u5b8c\u6574\u7684\u7247\u6bb5\u3002\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u901a\u8fc7\u586b\u5145\u5f0f\u751f\u6210\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u673a\u5236\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002\u6b64\u5916, ERNIE-GEN \u91c7\u6837\u591a\u7247\u6bb5-\u591a\u7c92\u5ea6\u76ee\u6807\u6587\u672c\u91c7\u6837\u7b56\u7565, \u589e\u5f3a\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u7684\u5173\u8054\u6027\uff0c\u52a0\u5f3a\u4e86\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4ea4\u4e92\u3002ernie_gen_poetry \u91c7\u7528\u5f00\u6e90\u8bd7\u6b4c\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u8bd7\u6b4c\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001ernie_gen_poetry \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install ernie_gen_poetry \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f TEXT_TO_TEXT \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' \u54cd\u5e94 { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u5b57\u751f\u6210"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#_1","text":"ERNIE-GEN \u662f\u9762\u5411\u751f\u6210\u4efb\u52a1\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u9996\u6b21\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165 span-by-span \u751f\u6210\u4efb\u52a1\uff0c\u8ba9\u6a21\u578b\u6bcf\u6b21\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u8bed\u4e49\u5b8c\u6574\u7684\u7247\u6bb5\u3002\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u901a\u8fc7\u586b\u5145\u5f0f\u751f\u6210\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u673a\u5236\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002\u6b64\u5916, ERNIE-GEN \u91c7\u6837\u591a\u7247\u6bb5-\u591a\u7c92\u5ea6\u76ee\u6807\u6587\u672c\u91c7\u6837\u7b56\u7565, \u589e\u5f3a\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u7684\u5173\u8054\u6027\uff0c\u52a0\u5f3a\u4e86\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4ea4\u4e92\u3002ernie_gen_poetry \u91c7\u7528\u5f00\u6e90\u8bd7\u6b4c\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u8bd7\u6b4c\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#_2","text":"\u8f93\u5165 \u8f93\u51fa [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#2ernie_gen_poetry","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp","title":"2\u3001ernie_gen_poetry \u4f9d\u8d56"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#3","text":"hub install ernie_gen_poetry","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"zh/how-to-guides/paddlepaddle/modules/text/text_generation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f TEXT_TO_TEXT \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' \u54cd\u5e94 { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"zh/how-to-guides/schema/","text":"\u5982\u4f55\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\uff1f \u00b6 \u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u8ba1\u7b97\u6570\u636e\u603b\u548c\u7684\u670d\u52a1\u3002 \u5b83\u63a5\u53d7\u7684\u8bf7\u6c42\u5185\u5bb9\u662f\uff1a [ 1 , 2 , 3 ] \u8fd4\u56de\u7684\u54cd\u5e94\u5185\u5bb9: 6 \u60a8\u5982\u4f55\u8ba9\u7528\u6237\u77e5\u9053\u60a8\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u662f\u4ec0\u4e48\u6837\u7684\uff1f \u5982\u679c\u60a8\u60f3\u81ea\u52a8\u9a8c\u8bc1\u6216\u8005\u89e3\u6790\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u8be5\u600e\u4e48\u529e\uff1f \u672c\u6587\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\u3002 Python 3 Type Hint \u00b6 \u4f60\u542c\u8bf4\u8fc7 python \u4e2d\u7684\u201c\u7c7b\u578b\u63d0\u793a\u201d\u5417\uff1f \u5982\u679c\u6ca1\u6709\uff0c\u60a8\u6700\u597d\u73b0\u5728\u5728 Python Typing \u4e0a\u67e5\u770b\u3002 \u4ece Python 3.5 \u5f00\u59cb\uff0cPython \u5f00\u59cb\u5728\u51fd\u6570\u5b9a\u4e49\u4e2d\u652f\u6301\u7c7b\u578b\u63d0\u793a\u3002 \u60a8\u53ef\u4ee5\u58f0\u660e\u53c2\u6570\u7684\u7c7b\u578b\u5e76\u8fd4\u56de\u3002 Pinferencia \u4f7f\u7528\u51fd\u6570\u7684\u7c7b\u578b\u63d0\u793a\u6765\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\u3002 \u6240\u4ee5\uff0c\u4f60\u4e0d\u9700\u8981\u5b66\u4e60\u53e6\u4e00\u79cd\u683c\u5f0f\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 python\u3002 \u5e76\u975e\u6240\u6709\u7c7b\u578b\u63d0\u793a\u90fd\u53d7\u652f\u6301! \u5e76\u975e\u6240\u6709 python \u4e2d\u7684\u7c7b\u578b\u63d0\u793a\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49schema\u3002 \u7c7b\u578b\u63d0\u793a\u9700\u8981\u80fd\u591f\u5728 json schema\u4e2d\u6b63\u786e\u8868\u793a\u3002 Dummy \u670d\u52a1 \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a Dummy \u670d\u52a1\u6765\u5411\u60a8\u5c55\u793a\u4e00\u5207\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) \u542f\u52a8\u670d\u52a1\uff0c\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u5c06\u627e\u5230\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u793a\u4f8b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u8fd9\u91cc\u51fd\u6570\u53c2\u6570\u7684\u7c7b\u578b\u63d0\u793a list \u5c06\u7528\u4e8e\u5b9a\u4e49\u8bf7\u6c42\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u51fd\u6570\u8fd4\u56de\u7684\u7c7b\u578b\u63d0\u793a str \u5c06\u7528\u4e8e\u5b9a\u4e49\u54cd\u5e94\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u6c42\u548c\u670d\u52a1 \u00b6 \u73b0\u5728\u8ba9\u6211\u4eec\u56de\u5230\u672c\u6587\u5f00\u5934\u63d0\u5230\u7684\u670d\u52a1\uff0c\u4e00\u4e2a\u6c42\u548c\u670d\u52a1\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b [ 1 , 2 , 3 ] 6 \u8ba9\u6211\u4eec\u91cd\u5199\u4e00\u4e0b Dummy \u670d\u52a1\u3002 Python 3.6\u53ca\u4ee5\u4e0a Python 3.9\u53ca\u4ee5\u4e0a dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) \u73b0\u5728\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u793a\u4f8b\u5c06\u662f\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } \u9664\u4e86\u663e\u793a schema \u4e4b\u5916\uff0c Pinferencia \u8fd8\u9a8c\u8bc1\u5e76\u5c1d\u8bd5\u5c06\u6570\u636e\u89e3\u6790\u4e3a\u6240\u9700\u7684\u7c7b\u578b\u3002 \u8ba9\u6211\u4eec\u5728\u540e\u7aef\u6587\u6863\u9875\u9762\u4e0a\u8bd5\u7528 API\u3002 \u6b63\u5e38\u6570\u636e \u7c7b\u578b\u9519\u8bef\u6570\u636e \u9519\u8bef\u6570\u636e \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u5c06\u8bf7\u6c42\u4e2d\u7684\u6570\u5b57\u4e4b\u4e00\u66f4\u6539\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\u3002 \u8be5\u6570\u5b57\u5c06\u6839\u636e schema \u81ea\u52a8\u8f6c\u6362\u4e3a\u6574\u6570\u3002 \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u53d1\u5e03\u4e00\u4e9b\u65e0\u6548\u7684\u6570\u636e\u7c7b\u578b\uff0c\u60a8\u5c06\u6536\u5230\u4e00\u4e2a 422 \u9519\u8bef \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] } \u590d\u6742\u7684Schema \u00b6 \u5728 pydantic \u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u590d\u6742\u7684schema\u3002 \u8ba9\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u670d\u52a1\u63a5\u6536\u5230\u4e2a\u4eba\u4fe1\u606f\uff1a \u8bf7\u6c42 [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u7b80\u5355\u7684\u6b22\u8fce\u95ee\u5019\u3002 \u54cd\u5e94 \"Hello, Will! Hello, Elise!\" \u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e0b\u8fd9\u4e2a\u670d\u52a1: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) \u73b0\u5728\u542f\u52a8\u670d\u52a1\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u4f1a\u53d1\u73b0\u8bf7\u6c42\u548c\u54cd\u5e94\u793a\u4f8b\u5982\u4e0b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u4efb\u52a1\u5b8c\u6210 \u00b6 \u60a8\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema\u3002 \u60a8\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u591a\u60a8\u611f\u5174\u8da3\u7684 Schema\u3002 \u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema"},{"location":"zh/how-to-guides/schema/#schema","text":"\u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u8ba1\u7b97\u6570\u636e\u603b\u548c\u7684\u670d\u52a1\u3002 \u5b83\u63a5\u53d7\u7684\u8bf7\u6c42\u5185\u5bb9\u662f\uff1a [ 1 , 2 , 3 ] \u8fd4\u56de\u7684\u54cd\u5e94\u5185\u5bb9: 6 \u60a8\u5982\u4f55\u8ba9\u7528\u6237\u77e5\u9053\u60a8\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u662f\u4ec0\u4e48\u6837\u7684\uff1f \u5982\u679c\u60a8\u60f3\u81ea\u52a8\u9a8c\u8bc1\u6216\u8005\u89e3\u6790\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u8be5\u600e\u4e48\u529e\uff1f \u672c\u6587\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\u3002","title":"\u5982\u4f55\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\uff1f"},{"location":"zh/how-to-guides/schema/#python-3-type-hint","text":"\u4f60\u542c\u8bf4\u8fc7 python \u4e2d\u7684\u201c\u7c7b\u578b\u63d0\u793a\u201d\u5417\uff1f \u5982\u679c\u6ca1\u6709\uff0c\u60a8\u6700\u597d\u73b0\u5728\u5728 Python Typing \u4e0a\u67e5\u770b\u3002 \u4ece Python 3.5 \u5f00\u59cb\uff0cPython \u5f00\u59cb\u5728\u51fd\u6570\u5b9a\u4e49\u4e2d\u652f\u6301\u7c7b\u578b\u63d0\u793a\u3002 \u60a8\u53ef\u4ee5\u58f0\u660e\u53c2\u6570\u7684\u7c7b\u578b\u5e76\u8fd4\u56de\u3002 Pinferencia \u4f7f\u7528\u51fd\u6570\u7684\u7c7b\u578b\u63d0\u793a\u6765\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\u3002 \u6240\u4ee5\uff0c\u4f60\u4e0d\u9700\u8981\u5b66\u4e60\u53e6\u4e00\u79cd\u683c\u5f0f\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 python\u3002 \u5e76\u975e\u6240\u6709\u7c7b\u578b\u63d0\u793a\u90fd\u53d7\u652f\u6301! \u5e76\u975e\u6240\u6709 python \u4e2d\u7684\u7c7b\u578b\u63d0\u793a\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49schema\u3002 \u7c7b\u578b\u63d0\u793a\u9700\u8981\u80fd\u591f\u5728 json schema\u4e2d\u6b63\u786e\u8868\u793a\u3002","title":"Python 3 Type Hint"},{"location":"zh/how-to-guides/schema/#dummy","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a Dummy \u670d\u52a1\u6765\u5411\u60a8\u5c55\u793a\u4e00\u5207\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) \u542f\u52a8\u670d\u52a1\uff0c\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u5c06\u627e\u5230\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u793a\u4f8b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u8fd9\u91cc\u51fd\u6570\u53c2\u6570\u7684\u7c7b\u578b\u63d0\u793a list \u5c06\u7528\u4e8e\u5b9a\u4e49\u8bf7\u6c42\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u51fd\u6570\u8fd4\u56de\u7684\u7c7b\u578b\u63d0\u793a str \u5c06\u7528\u4e8e\u5b9a\u4e49\u54cd\u5e94\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002","title":"Dummy \u670d\u52a1"},{"location":"zh/how-to-guides/schema/#_1","text":"\u73b0\u5728\u8ba9\u6211\u4eec\u56de\u5230\u672c\u6587\u5f00\u5934\u63d0\u5230\u7684\u670d\u52a1\uff0c\u4e00\u4e2a\u6c42\u548c\u670d\u52a1\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b [ 1 , 2 , 3 ] 6 \u8ba9\u6211\u4eec\u91cd\u5199\u4e00\u4e0b Dummy \u670d\u52a1\u3002 Python 3.6\u53ca\u4ee5\u4e0a Python 3.9\u53ca\u4ee5\u4e0a dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) \u73b0\u5728\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u793a\u4f8b\u5c06\u662f\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } \u9664\u4e86\u663e\u793a schema \u4e4b\u5916\uff0c Pinferencia \u8fd8\u9a8c\u8bc1\u5e76\u5c1d\u8bd5\u5c06\u6570\u636e\u89e3\u6790\u4e3a\u6240\u9700\u7684\u7c7b\u578b\u3002 \u8ba9\u6211\u4eec\u5728\u540e\u7aef\u6587\u6863\u9875\u9762\u4e0a\u8bd5\u7528 API\u3002 \u6b63\u5e38\u6570\u636e \u7c7b\u578b\u9519\u8bef\u6570\u636e \u9519\u8bef\u6570\u636e \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u5c06\u8bf7\u6c42\u4e2d\u7684\u6570\u5b57\u4e4b\u4e00\u66f4\u6539\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\u3002 \u8be5\u6570\u5b57\u5c06\u6839\u636e schema \u81ea\u52a8\u8f6c\u6362\u4e3a\u6574\u6570\u3002 \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u53d1\u5e03\u4e00\u4e9b\u65e0\u6548\u7684\u6570\u636e\u7c7b\u578b\uff0c\u60a8\u5c06\u6536\u5230\u4e00\u4e2a 422 \u9519\u8bef \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] }","title":"\u6c42\u548c\u670d\u52a1"},{"location":"zh/how-to-guides/schema/#schema_1","text":"\u5728 pydantic \u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u590d\u6742\u7684schema\u3002 \u8ba9\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u670d\u52a1\u63a5\u6536\u5230\u4e2a\u4eba\u4fe1\u606f\uff1a \u8bf7\u6c42 [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u7b80\u5355\u7684\u6b22\u8fce\u95ee\u5019\u3002 \u54cd\u5e94 \"Hello, Will! Hello, Elise!\" \u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e0b\u8fd9\u4e2a\u670d\u52a1: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) \u73b0\u5728\u542f\u52a8\u670d\u52a1\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u4f1a\u53d1\u73b0\u8bf7\u6c42\u548c\u54cd\u5e94\u793a\u4f8b\u5982\u4e0b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" }","title":"\u590d\u6742\u7684Schema"},{"location":"zh/how-to-guides/schema/#_2","text":"\u60a8\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema\u3002 \u60a8\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u591a\u60a8\u611f\u5174\u8da3\u7684 Schema\u3002 \u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u4efb\u52a1\u5b8c\u6210"},{"location":"zh/introduction/overview/","text":"\u6b22\u8fce\u4f7f\u7528Pinferencia \u00b6 Pinferencia? \u00b6 \u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 \u8fd8\u5acc\u4e0d\u591f? \u66f4\u591a\u6b22\u4e50\uff0c\u8bf7\u524d\u5f80 \u6b63\u895f\u5371\u5750\u7248\u6587\u6863 \u5f00\u59cb\u5c1d\u9c9c! \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u8fd0\u884c\u670d\u52a1! \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u6982\u8ff0"},{"location":"zh/introduction/overview/#pinferencia","text":"","title":"\u6b22\u8fce\u4f7f\u7528Pinferencia"},{"location":"zh/introduction/overview/#pinferencia_1","text":"\u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 \u8fd8\u5acc\u4e0d\u591f? \u66f4\u591a\u6b22\u4e50\uff0c\u8bf7\u524d\u5f80 \u6b63\u895f\u5371\u5750\u7248\u6587\u6863","title":"Pinferencia?"},{"location":"zh/introduction/overview/#_1","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"\u5f00\u59cb\u5c1d\u9c9c!"},{"location":"zh/introduction/overview/#_2","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"zh/introduction/overview/#_3","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u8fd0\u884c\u670d\u52a1!"},{"location":"zh/introduction/pinferencia-is-different/","text":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c? \u00b6 \u4e0d\u540c? \u00b6 \u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48? \u4e0d\u8981\u554a\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01 \u6a21\u578b\u5728\u4f60\u624b\u91cc\uff0c\u4f60\u7528 Python \u8bad\u7ec3\uff0c\u7528 Python \u9884\u6d4b\uff0c\u751a\u81f3\u5199\u4e86\u5f88\u591a\u590d\u6742\u7684\u4ee3\u7801\uff0c\u53bb\u89e3\u51b3\u56f0\u96be\u53c8\u72ec\u7279\u7684\u9700\u6c42\u3002 \u800c\u5982\u4eca\uff0c\u4f60\u53c8\u8981\u591a\u5199\u591a\u5c11\u4ee3\u7801\uff0c\u591a\u505a\u591a\u5c11\u6539\u53d8\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u6a21\u578b\uff0c\u7528\u8fd9\u4e9b\u5de5\u5177\u6216\u8005\u5e73\u53f0\uff0c\u4ec5\u4ec5\u662f\u542f\u52a8\u4e00\u4e2aAPI\uff1f \u7b54\u6848\u662f\uff1a \u6570\u4e0d\u80dc\u6570 . \u6709\u4e86 Pinferencia \u00b6 \u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002 \u7b80\u5355\uff0c\u4e14\u5f3a\u5927 \u00b6 Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"Pinferencia \u6709\u4f55\u4e0d\u540c?"},{"location":"zh/introduction/pinferencia-is-different/#pinferencia","text":"","title":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c?"},{"location":"zh/introduction/pinferencia-is-different/#_1","text":"\u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48?","title":"\u4e0d\u540c?"},{"location":"zh/introduction/pinferencia-is-different/#pinferencia_1","text":"\u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002","title":"\u6709\u4e86 Pinferencia"},{"location":"zh/introduction/pinferencia-is-different/#_2","text":"Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"\u7b80\u5355\uff0c\u4e14\u5f3a\u5927"},{"location":"zh/reference/cli/","text":"\u547d\u4ee4\u884c\u754c\u9762 \u00b6 Pinfenrecia \u63d0\u4f9b\u547d\u4ee4 pinfer \u6765\u7b80\u5316\u542f\u52a8\u524d\u7aef\u548c\u540e\u7aef\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 pinfer --help \u67e5\u770b\u53ef\u7528\u9009\u9879\uff1a Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"\u547d\u4ee4\u884c\u754c\u9762"},{"location":"zh/reference/cli/#_1","text":"Pinfenrecia \u63d0\u4f9b\u547d\u4ee4 pinfer \u6765\u7b80\u5316\u542f\u52a8\u524d\u7aef\u548c\u540e\u7aef\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 pinfer --help \u67e5\u770b\u53ef\u7528\u9009\u9879\uff1a Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"\u547d\u4ee4\u884c\u754c\u9762"},{"location":"zh/reference/frontend/requirements/","text":"\u8981\u6c42 \u00b6 \u8981\u5c06 Pinferencia \u7684\u524d\u7aef\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u7684\u9884\u6d4b\u529f\u80fd\u6709\u4e00\u4e9b\u8981\u6c42\u3002 \u6a21\u677f \u00b6 \u76ee\u524d\uff0c\u6a21\u677f\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e3b\u8981\u6709\u4e24\u5927\u7c7b\u3002\u5176\u5b83\u7c7b\u578b\u7684\u8f93\u5165\u8f93\u51fa\uff08\u4f8b\u5982\u97f3\u9891\u548c\u89c6\u9891\uff09\uff0c\u4f1a\u5728\u540e\u7eed\u9646\u7eed\u652f\u6301\u3002 \u57fa\u672c\u6a21\u677f \u00b6 \u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u6587\u672c\u8f6c\u6587\u672c \u6587\u672c \u6587\u672c \u6587\u672c\u8f6c\u56fe\u7247 \u6587\u672c \u56fe\u7247 \u56fe\u7247\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u76f8\u673a\u8f93\u5165\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u76f8\u673a\u8f93\u5165\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u4fbf\u6377\u6a21\u677f \u00b6 \u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u7ffb\u8bd1 \u6587\u672c \u6587\u672c \u56fe\u50cf\u5206\u7c7b \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u98ce\u683c\u8f6c\u79fb \u56fe\u7247 \u56fe\u7247 \u8f93\u5165 \u00b6 \u6839\u636e\u8bf7\u6c42\u7684\u6a21\u5f0f\uff0c\u524d\u7aef\u53ef\u4ee5\u5c06\u8f93\u5165\u89e3\u6790\u4e3a\u5217\u8868\u6216\u7b80\u5355\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \uff01\uff01\uff01 \u4fe1\u606f\u201c\u5b9a\u4e49\u67b6\u6784\u201d \u5173\u4e8e\u5982\u4f55\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\uff0c\u8bf7\u8bbf\u95ee \u5982\u4f55\u5b9a\u4e49\u60a8\u7684\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u7684Schema? \u5982\u679c\u5c06\u8bf7\u6c42\u7684schema\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4ee3\u8868 base64 \u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u8868\u793a base64 \u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u8f93\u51fa \u00b6 \u5982\u679c\u5c06\u54cd\u5e94schena\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4e00\u4e2a\u6587\u672c\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u6587\u672c\u8f93\u51fa \u524d\u7aef\u5c06\u5c1d\u8bd5\u5c06\u6587\u672c\u8f93\u51fa\u89e3\u6790\u4e3a\u8868\u683c\u3001json \u6216\u7eaf\u6587\u672c\u3002 \u8868\u683c \u6587\u672c JSON \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a \u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u662f\u4e00\u4e2a\u5217\u8868 [ [ { \"a\" \uff1a 1 \uff0c \"b\" \uff1a 2 } \uff0c { \"a\" \uff1a 3 \uff0c \"b\" \uff1a 4 } \uff0c { \"a\" \uff1a 5 \uff0c \"b\" \uff1a 6 } ] ] \u6216\u8005 json title=\"\u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u4e0d\u662f\u5217\u8868\" [ {\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6} ] \u5b83\u5c06\u663e\u793a\u4e3a\u8868\u683c\u3002 \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u662f\u4e00\u4e2a\u5217\u8868\" [ \"Text output.\" ] or ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u4e0d\u662f\u5217\u8868\" \"Text output.\" \u5b83\u5c06\u663e\u793a\u4e3a\u6587\u672c\u3002 \u6240\u6709\u5176\u4ed6\u683c\u5f0f\u7684\u8f93\u51fa\u90fd\u5c06\u663e\u793a\u4e3a JSON\u3002 \u4f8b\u5982\uff0c [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] \u6216\u8005 { \"a\" : 1 , \"b\" : 2 }","title":"\u524d\u7aef\u4f7f\u7528\u6761\u4ef6"},{"location":"zh/reference/frontend/requirements/#_1","text":"\u8981\u5c06 Pinferencia \u7684\u524d\u7aef\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u7684\u9884\u6d4b\u529f\u80fd\u6709\u4e00\u4e9b\u8981\u6c42\u3002","title":"\u8981\u6c42"},{"location":"zh/reference/frontend/requirements/#_2","text":"\u76ee\u524d\uff0c\u6a21\u677f\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e3b\u8981\u6709\u4e24\u5927\u7c7b\u3002\u5176\u5b83\u7c7b\u578b\u7684\u8f93\u5165\u8f93\u51fa\uff08\u4f8b\u5982\u97f3\u9891\u548c\u89c6\u9891\uff09\uff0c\u4f1a\u5728\u540e\u7eed\u9646\u7eed\u652f\u6301\u3002","title":"\u6a21\u677f"},{"location":"zh/reference/frontend/requirements/#_3","text":"\u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u6587\u672c\u8f6c\u6587\u672c \u6587\u672c \u6587\u672c \u6587\u672c\u8f6c\u56fe\u7247 \u6587\u672c \u56fe\u7247 \u56fe\u7247\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u76f8\u673a\u8f93\u5165\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u76f8\u673a\u8f93\u5165\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247","title":"\u57fa\u672c\u6a21\u677f"},{"location":"zh/reference/frontend/requirements/#_4","text":"\u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u7ffb\u8bd1 \u6587\u672c \u6587\u672c \u56fe\u50cf\u5206\u7c7b \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u98ce\u683c\u8f6c\u79fb \u56fe\u7247 \u56fe\u7247","title":"\u4fbf\u6377\u6a21\u677f"},{"location":"zh/reference/frontend/requirements/#_5","text":"\u6839\u636e\u8bf7\u6c42\u7684\u6a21\u5f0f\uff0c\u524d\u7aef\u53ef\u4ee5\u5c06\u8f93\u5165\u89e3\u6790\u4e3a\u5217\u8868\u6216\u7b80\u5355\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \uff01\uff01\uff01 \u4fe1\u606f\u201c\u5b9a\u4e49\u67b6\u6784\u201d \u5173\u4e8e\u5982\u4f55\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\uff0c\u8bf7\u8bbf\u95ee \u5982\u4f55\u5b9a\u4e49\u60a8\u7684\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u7684Schema? \u5982\u679c\u5c06\u8bf7\u6c42\u7684schema\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4ee3\u8868 base64 \u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u8868\u793a base64 \u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002","title":"\u8f93\u5165"},{"location":"zh/reference/frontend/requirements/#_6","text":"\u5982\u679c\u5c06\u54cd\u5e94schena\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4e00\u4e2a\u6587\u672c\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u6587\u672c\u8f93\u51fa \u524d\u7aef\u5c06\u5c1d\u8bd5\u5c06\u6587\u672c\u8f93\u51fa\u89e3\u6790\u4e3a\u8868\u683c\u3001json \u6216\u7eaf\u6587\u672c\u3002 \u8868\u683c \u6587\u672c JSON \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a \u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u662f\u4e00\u4e2a\u5217\u8868 [ [ { \"a\" \uff1a 1 \uff0c \"b\" \uff1a 2 } \uff0c { \"a\" \uff1a 3 \uff0c \"b\" \uff1a 4 } \uff0c { \"a\" \uff1a 5 \uff0c \"b\" \uff1a 6 } ] ] \u6216\u8005 json title=\"\u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u4e0d\u662f\u5217\u8868\" [ {\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6} ] \u5b83\u5c06\u663e\u793a\u4e3a\u8868\u683c\u3002 \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u662f\u4e00\u4e2a\u5217\u8868\" [ \"Text output.\" ] or ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u4e0d\u662f\u5217\u8868\" \"Text output.\" \u5b83\u5c06\u663e\u793a\u4e3a\u6587\u672c\u3002 \u6240\u6709\u5176\u4ed6\u683c\u5f0f\u7684\u8f93\u51fa\u90fd\u5c06\u663e\u793a\u4e3a JSON\u3002 \u4f8b\u5982\uff0c [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] \u6216\u8005 { \"a\" : 1 , \"b\" : 2 }","title":"\u8f93\u51fa"},{"location":"zh/reference/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002 PickleHandler \u00b6 \u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"zh/reference/handlers/#handlers","text":"","title":"Handlers"},{"location":"zh/reference/handlers/#basehandler","text":"BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002","title":"BaseHandler"},{"location":"zh/reference/handlers/#picklehandler","text":"\u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"zh/reference/models/machine-learning/","text":"\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u00b6 \u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u5176\u5b83\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"zh/reference/models/machine-learning/#_1","text":"\u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"zh/reference/models/register/","text":"\u6ce8\u518c\u6a21\u578b \u00b6 \u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) \u53c2\u6570 \u00b6 \u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b \u4f8b\u5b50 \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version\u540d\u79f0 \u00b6 \u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. Metadata \u00b6 \u9ed8\u8ba4API \u00b6 Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002 Handler \u00b6 \u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"zh/reference/models/register/#_1","text":"\u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"zh/reference/models/register/#_2","text":"\u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b","title":"\u53c2\u6570"},{"location":"zh/reference/models/register/#_3","text":"","title":"\u4f8b\u5b50"},{"location":"zh/reference/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"zh/reference/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"zh/reference/models/register/#version","text":"\u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version\u540d\u79f0"},{"location":"zh/reference/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b.","title":"Entrypoint"},{"location":"zh/reference/models/register/#metadata","text":"","title":"Metadata"},{"location":"zh/reference/models/register/#api","text":"Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"\u9ed8\u8ba4API"},{"location":"zh/reference/models/register/#kserve-api","text":"Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002","title":"Kserve API"},{"location":"zh/reference/models/register/#handler","text":"\u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"zh/reference/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"zh/reference/restapi/","text":"REST API \u00b6 \u6982\u8ff0 \u00b6 Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 \u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7 \u00b6 \u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002 \u9ed8\u8ba4 API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b Kserve API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"REST API"},{"location":"zh/reference/restapi/#rest-api","text":"","title":"REST API"},{"location":"zh/reference/restapi/#_1","text":"Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"\u6982\u8ff0"},{"location":"zh/reference/restapi/#_2","text":"\u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002","title":"\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7"},{"location":"zh/reference/restapi/#api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"\u9ed8\u8ba4 API"},{"location":"zh/reference/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"Kserve API"},{"location":"rc/","text":"","title":"\u9996\u9875"},{"location":"rc/background/models/home/","text":"\u6a21\u578b? \u00b6 \u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u5173\u4e8e\u6a21\u578b"},{"location":"rc/background/models/home/#_1","text":"\u4ec0\u4e48\u662f \u6a21\u578b ? \u6982\u62ec\u7684\u6765\u8bf4\uff0c\u6a21\u578b\u662f\u4e00\u79cd\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u901a\u5e38\u6765\u8bf4\uff0c\u6bd4\u4e00\u4e2a\u65b9\u7a0b\u590d\u6742\u4e00\u70b9\u3002 \u90a3\u6a21\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u6587\u4ef6\u5417\uff1f\u53ef\u4ee5\u662f\u4e00\u4e2aPython\u5bf9\u8c61\u5417\uff1f\u5f53\u7136\u3002 \u5728 Pinferencia \uff0c \u4e00\u4e2a\u6a21\u578b\u5c31\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8c03\u7528\u7684\u4ee3\u7801\uff0c\u5b83\u53ef\u4ee5\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u7c7b\u7684\u5b9e\u4f8b\u3002","title":"\u6a21\u578b?"},{"location":"rc/get-started/install/","text":"\u5b89\u88c5 Pinferencia \u00b6 \u63a8\u8350\u65b9\u5f0f \u00b6 \u5efa\u8bae\u4f7f\u7528 streamlit \u5b89\u88c5 Pinferencia \u3002 \u60a8\u5c06\u4f53\u9a8c Pinferencia \u7684\u6240\u6709\u529f\u80fd\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100% \u6216\u8005 \u00b6 \u60a8\u4e5f\u53ef\u4ee5\u9009\u62e9\u5b89\u88c5 Pinferencia \u800c\u4e0d\u4f7f\u7528 streamlit \uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u53ea\u80fd\u8fd0\u884c pinferencia \u7684\u540e\u7aef\u3002 $ pip install pinferencia ---> 100%","title":"\u5b89\u88c5"},{"location":"rc/get-started/install/#pinferencia","text":"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/get-started/install/#_1","text":"\u5efa\u8bae\u4f7f\u7528 streamlit \u5b89\u88c5 Pinferencia \u3002 \u60a8\u5c06\u4f53\u9a8c Pinferencia \u7684\u6240\u6709\u529f\u80fd\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"\u63a8\u8350\u65b9\u5f0f"},{"location":"rc/get-started/install/#_2","text":"\u60a8\u4e5f\u53ef\u4ee5\u9009\u62e9\u5b89\u88c5 Pinferencia \u800c\u4e0d\u4f7f\u7528 streamlit \uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u60a8\u53ea\u80fd\u8fd0\u884c pinferencia \u7684\u540e\u7aef\u3002 $ pip install pinferencia ---> 100%","title":"\u6216\u8005"},{"location":"rc/get-started/introduction/","text":"\u4ece\u9ad8\u624b\u5230\u5c0f\u767d\u4e4b\u8def \u00b6 Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u4eba\u4eba\u90fd\u60f3\u6210\u4e3a\u9ad8\u624b\uff0c\u5374\u4e0d\u77e5\uff0c\u524d\u8fdb\u53ea\u6709\u4e24\u6761\u817f\u8d70\u3002 \u8fd9\u4e2a\u4e16\u754c\u9ad8\u624b\u90a3\u4e48\u7a00\u5c11\uff0c\u4f46\u6211\u8fd8\u5728\u66ff\u9ad8\u624b\u82e6\u607c\u3002 \u5c0f\u767d\u867d\u7136\u904d\u5730\u90fd\u6709\uff0c\u800c\u6211\u5374\u8ba9\u5c0f\u767d\u6d88\u6101\u501f\u9152\u3002 \u4e0d\u4e0d\u4e0d\uff0c\u8fd9\u6837\u7b80\u76f4\u662f\u4e00\u6761\u6b7b\u8def\u3002 \u7eb5\u4f7f\u5929\u7a7a\u5982\u6b64\u7a7a\u65f7\uff0c\u6211\u5e94\u8be5\u5b89\u5fc3\u8d70\u5728\u8def\u4e0a\u3002\u4e0d\u8981\u56e0\u4e3a\u60f3\u53bb\u98de\u5411\u5929\u7a7a\uff0c\u800c\u5fd8\u8bb0\u81ea\u5df1\u7684\u521d\u8877\u3002 \u6211\u60f3\u8ba9\u5929\u4e0b\u5c0f\u767d\uff0c\u90fd\u4ece\u6b64\u4e00\u626b\u9634\u973e\uff0c \u800c\u6211\u8fd9\u4e2a\u5c0f\u767d\uff0c\u4e5f\u80fd\u591a\u7ed9\u4e16\u754c\u4e00\u70b9\u70b9\u7231\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ecb\u7ecd"},{"location":"rc/get-started/introduction/#_1","text":"Pinferencia\u7684\u76ee\u6807 Pinferencia, \u81f4\u529b\u4e8e\u63d0\u4f9b\u6700\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u5f0f\u8ba9\u4f60\u7684\u6a21\u578b\u62e5\u6709API\u670d\u52a1\u3002 \u8fd9\u91cc\u4e0d\u662f\u4e3a\u4e86\u90a3\u4e9b\u7cbe\u901a\u7f51\u7edc\u7f16\u7a0b\u7684\u4eba\u51c6\u5907\u7684\uff0c\u66f4\u591a\u662f\u4e3a\u4e86\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u670b\u53cb\u3002 \u4e0d\u7ba1\u4f60\u662f\u60f3\u505ademo\uff0c\u8fd8\u662f\u60f3\u7ed9\u516c\u53f8\u6216\u5b66\u6821\u63d0\u4f9b\u5185\u90e8\u63a5\u53e3\uff0c\u751a\u81f3\u66f4\u8fdb\u4e00\u6b65\u96c6\u6210\u5230CICD\uff0c\u76f4\u63a5\u90e8\u7f72\u4e00\u4e2a\u670d\u52a1\u5230\u4e91\u7aef\uff0c\u4f60\u90fd\u80fd\u5728\u8fd9\u91cc\u627e\u5230\u89e3\u7b54\u3002 \u4eba\u4eba\u90fd\u60f3\u6210\u4e3a\u9ad8\u624b\uff0c\u5374\u4e0d\u77e5\uff0c\u524d\u8fdb\u53ea\u6709\u4e24\u6761\u817f\u8d70\u3002 \u8fd9\u4e2a\u4e16\u754c\u9ad8\u624b\u90a3\u4e48\u7a00\u5c11\uff0c\u4f46\u6211\u8fd8\u5728\u66ff\u9ad8\u624b\u82e6\u607c\u3002 \u5c0f\u767d\u867d\u7136\u904d\u5730\u90fd\u6709\uff0c\u800c\u6211\u5374\u8ba9\u5c0f\u767d\u6d88\u6101\u501f\u9152\u3002 \u4e0d\u4e0d\u4e0d\uff0c\u8fd9\u6837\u7b80\u76f4\u662f\u4e00\u6761\u6b7b\u8def\u3002 \u7eb5\u4f7f\u5929\u7a7a\u5982\u6b64\u7a7a\u65f7\uff0c\u6211\u5e94\u8be5\u5b89\u5fc3\u8d70\u5728\u8def\u4e0a\u3002\u4e0d\u8981\u56e0\u4e3a\u60f3\u53bb\u98de\u5411\u5929\u7a7a\uff0c\u800c\u5fd8\u8bb0\u81ea\u5df1\u7684\u521d\u8877\u3002 \u6211\u60f3\u8ba9\u5929\u4e0b\u5c0f\u767d\uff0c\u90fd\u4ece\u6b64\u4e00\u626b\u9634\u973e\uff0c \u800c\u6211\u8fd9\u4e2a\u5c0f\u767d\uff0c\u4e5f\u80fd\u591a\u7ed9\u4e16\u754c\u4e00\u70b9\u70b9\u7231\u3002 \u901a\u8fc7\u8fd9\u4e00\u7cfb\u5217\u6559\u7a0b\uff0c\u4f60\u5c06\u638c\u63e1\u5982\u4f55\uff1a \u542f\u52a8\u4e00\u4e2a JSONModel \u670d\u52a1 \u542f\u52a8\u4e00\u4e2a Function \u670d\u52a1 \u5982\u4f55\u4f7f\u7528\u7528\u4e24\u79cd\u65b9\u6cd5\u542f\u52a8 PyTorch MNIST \u6a21\u578b, \u7528 MNIST \u641e\u70b9\u4e50\u5b50","title":"\u4ece\u9ad8\u624b\u5230\u5c0f\u767d\u4e4b\u8def"},{"location":"rc/get-started/other-models/","text":"\u4e0b\u4e00\u6b65 \u00b6 \u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"rc/get-started/other-models/#_1","text":"\u597d\u5427\uff0c\u6211\u6562\u6253\u8d4c\uff0c\u60a8\u5728\u4e0a\u4e00\u6559\u7a0b\u4e2d\u4f7f\u7528 PyTorch MNIST \u6a21\u578b\u4e00\u5b9a\u4f1a\u5f88\u6709\u8da3\u3002 \u6211\u60f3\u4f60\u73b0\u5728\u5bf9 Pinferencia \u5f88\u719f\u6089\u4e86\u3002 Pinferencia \u53ef\u4ee5\u4ee5\u975e\u5e38\u76f4\u63a5\u7684\u65b9\u5f0f\u4e3a\u4efb\u4f55\u53ef\u8c03\u7528\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\u3002\u975e\u5e38\u7b80\u5355\u76f4\u63a5\u3002 \u5e76\u4e14\u8fd9\u5f88\u5bb9\u6613\u4e0e\u60a8\u73b0\u6709\u7684\u4ee3\u7801\u96c6\u6210\u3002 \u8fd9\u5c31\u662f Pinferencia \u7684\u8bbe\u8ba1\u76ee\u7684\u3002 \u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539 \u3002 \u73b0\u5728\u60a8\u53ef\u4ee5\u4ece \u4efb\u4f55\u6846\u67b6 \u63d0\u4f9b\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4ee5 \u5c06\u5b83\u4eec\u6df7\u5408\u5728\u4e00\u8d77 \u3002\u60a8\u53ef\u4ee5\u5728\u4e00\u4e2aAPI\u91cc \u540c\u65f6 \u4f7f\u7528\u6765\u81ea \u4e0d\u540c\u6846\u67b6 \u7684 \u4e0d\u540c\u6a21\u578b \uff01 \u73a9\u7684\u5f00\u5fc3\uff01 \u5982\u679c\u4f60\u559c\u6b22 Pinferencia \uff0c\u522b\u5fd8\u4e86\u53bb Github \u5e76\u7ed9\u4e00\u4e2a\u661f\u3002\u8c22\u8c22\u4f60\u3002","title":"\u4e0b\u4e00\u6b65"},{"location":"rc/get-started/pytorch-mnist/","text":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt \u90e8\u7f72\u65b9\u6cd5 \u00b6 \u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570 \u00b6 \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01 \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4 \u524d\u7aef\u754c\u9762 \u00b6 \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f Image to Text \u3002 \u4f7f\u7528\u4e0b\u56fe\uff1a \u4f60\u4f1a\u5f97\u5230\uff1a \u540e\u7aefAPI \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1:8000 \u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84 \u00b6 Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers \u521b\u5efa\u5e94\u7528\u7a0b\u5e8f \u00b6 \u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002 \u6700\u540e \u00b6 \u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50 \u548c \u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002","title":"\u542f\u52a8 PyTorch MNIST \u6a21\u578b"},{"location":"rc/get-started/pytorch-mnist/#pytorch-mnist","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63d0\u4f9b PyTorch MNIST \u6a21\u578b\u3002 \u5b83\u63a5\u6536 Base64 \u7f16\u7801\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bf7\u6c42\u6570\u636e\uff0c\u5e76\u5728\u54cd\u5e94\u4e2d\u8fd4\u56de\u9884\u6d4b\u3002","title":"\u4e0a\u7ebf PyTorch MNIST \u6a21\u578b"},{"location":"rc/get-started/pytorch-mnist/#_1","text":"\u8bbf\u95ee PyTorch \u793a\u4f8b - MNIST \uff0c\u4e0b\u8f7d\u6587\u4ef6\u3002 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u5b89\u88c5\u548c\u8bad\u7ec3\u6a21\u578b\uff1a pip install -r requirements.txt python main.py --save-model \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u60a8\u5c06\u62e5\u6709\u5982\u4e0b\u6587\u4ef6\u5939\u7ed3\u6784\u3002\u521b\u5efa\u4e86\u4e00\u4e2a mnist_cnn.pt \u6587\u4ef6 . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mnist_cnn.pt \u2514\u2500\u2500 requirements.txt","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"rc/get-started/pytorch-mnist/#_2","text":"\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u90e8\u7f72\u6a21\u578b\u3002 \u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570\u3002 \u4ec5\u4f7f\u7528\u9644\u52a0\u5904\u7406\u7a0b\u5e8f Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84\u3002 \u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u9010\u6b65\u4ecb\u7ecd\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002","title":"\u90e8\u7f72\u65b9\u6cd5"},{"location":"rc/get-started/pytorch-mnist/#_3","text":"","title":"\u76f4\u63a5\u6ce8\u518c\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/pytorch-mnist/#_4","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 func_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import base64 from io import BytesIO import torch from main import Net # (1) from PIL import Image from torchvision import transforms from pinferencia import Server , task use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) # (2) model = Net () . to ( device ) # (3) model . load_state_dict ( torch . load ( \"mnist_cnn.pt\" )) model . eval () def preprocessing ( img_str ): image = Image . open ( BytesIO ( base64 . b64decode ( img_str ))) tensor = transform ( image ) return torch . stack ([ tensor ]) . to ( device ) def predict ( data ): return model ( preprocessing ( data )) . argmax ( 1 ) . tolist ()[ 0 ] service = Server () # (4) service . register ( model_name = \"mnist\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u786e\u4fdd\u60a8\u53ef\u4ee5\u5bfc\u5165\u7f51\u7edc\u6a21\u578b\u3002 \u9884\u5904\u7406\u8f6c\u6362\u4ee3\u7801\u3002 \u793a\u4f8b\u811a\u672c\u53ea\u4fdd\u5b58 state_dict \u3002\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d state_dict \u3002 \u51c6\u5907\u597d\uff0c3\u30012\u30011\u3002 GO\uff01","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"rc/get-started/pytorch-mnist/#_5","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_6","text":"\u6d4b\u8bd5\u6570\u636e\u90a3\u91cc\u6765? \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u662f base64 \u7f16\u7801\u7684 MNIST \u56fe\u50cf\uff0c\u6211\u4eec\u4ece\u54ea\u91cc\u53ef\u4ee5\u83b7\u5f97\u8fd9\u4e9b\u6570\u636e\uff1f \u60a8\u53ef\u4ee5\u4f7f\u7528 PyTorch \u7684\u6570\u636e\u96c6\u3002\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u540d\u4e3a get-base64-img.oy \u3002 get-base64-img.py import base64 import random from io import BytesIO from PIL import Image from torchvision import datasets dataset = datasets . MNIST ( # (1) \"./data\" , train = True , download = True , transform = None , ) index = random . randint ( 0 , len ( dataset . data )) # (2) img = dataset . data [ index ] img = Image . fromarray ( img . numpy (), mode = \"L\" ) buffered = BytesIO () img . save ( buffered , format = \"JPEG\" ) base64_img_str = base64 . b64encode ( buffered . getvalue ()) . decode () print ( \"Base64 String:\" , base64_img_str ) # (3) print ( \"target:\" , dataset . targets [ index ] . tolist ()) \u8fd9\u662f\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684 MNIST \u6570\u636e\u96c6\u3002 \u8ba9\u6211\u4eec\u4f7f\u7528\u968f\u673a\u56fe\u50cf\u3002 \u5b57\u7b26\u4e32\u548c\u76ee\u6807\u88ab\u6253\u5370\u5230\u6807\u51c6\u8f93\u51fa\u3002 \u8fd0\u884c\u811a\u672c\u5e76\u590d\u5236\u5b57\u7b26\u4e32\u3002 python get-base64-img.py \u8f93\u51fa: Base64 String: /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k = target: 4","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_7","text":"\u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f Image to Text \u3002 \u4f7f\u7528\u4e0b\u56fe\uff1a \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u524d\u7aef\u754c\u9762"},{"location":"rc/get-started/pytorch-mnist/#api","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 test.py test.py 1 2 3 4 5 6 7 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mnist/predict\" , json = { \"data\" : \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+uhfwXqy2Ph25VYnPiB3SzhUkPlXCfNkAAEsCCCeOeKx9RsLjStUu9Ou1C3NpM8Eqg5AdSVIz35FVqK9xl0HXhb/C20sdMubjTLMQXs11AhkRXmmDsCwzgAYPpz+XI/GrSLrTfiVqNzPapbw3xE8AWQNvUAKXOOmWVjg+teeUV2fgXxd4hsPE2hWEGuX8Vh9uhja3Fw3lbGcBhtzjGCad8XI7iL4p68twHDGcMm45+QqCuPbBFcVRRU97fXepXb3d9dT3VzJjfNPIXdsAAZY8nAAH4VBX/9k=\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u66f4\u591a\u56fe\u50cf\u6765\u6d4b\u8bd5\uff0c\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u4ea4\u4e92\u5f0f API \u6587\u6863\u9875\u9762 http://127.0.0.1:8000","title":"\u540e\u7aefAPI"},{"location":"rc/get-started/pytorch-mnist/#handler","text":"Handler \u5982\u679c\u60a8\u66f4\u559c\u6b22\u4f7f\u7528\u6587\u4ef6\u63d0\u4f9b\u6a21\u578b\u7684\u7ecf\u5178\u65b9\u5f0f\uff0c\u5219\u4f7f\u7528\u201cHandlers\u201d\u662f\u60a8\u7684\u9009\u62e9\u3002 \u5904\u7406\u7a0b\u5e8f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u8bbf\u95ee Handlers","title":"\u4f7f\u7528 Handler \u6ce8\u518c\u6a21\u578b\u8def\u5f84"},{"location":"rc/get-started/pytorch-mnist/#_8","text":"\u8ba9\u6211\u4eec\u5728\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 func_app.py \u3002 \u4e0b\u9762\u7684\u4ee3\u7801\u88ab\u91cd\u6784\u4e3a MNISTHandler \u3002\u770b\u8d77\u6765\u66f4\u5e72\u51c0\uff01 path_app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import base64 import pathlib from io import BytesIO import torch from main import Net from PIL import Image from torchvision import transforms from pinferencia import Server , task from pinferencia.handlers import BaseHandler class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): # (1) model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data ): # (2) image = Image . open ( BytesIO ( base64 . b64decode ( data ))) tensor = self . transform ( image ) input_data = torch . stack ([ tensor ]) . to ( self . device ) return self . model ( input_data ) . argmax ( 1 ) . tolist ()[ 0 ] service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) # (3) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , # (4) metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u6211\u4eec\u5c06\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u79fb\u5230 load_model \u51fd\u6570\u4e2d\u3002\u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u8bbf\u95ee\u3002 \u6211\u4eec\u5c06\u9884\u6d4b\u4ee3\u7801\u79fb\u5230 predict \u51fd\u6570\u4e2d\u3002\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u8bbf\u95ee\u3002 model_dir \u662f Pinferencia \u67e5\u627e\u6a21\u578b\u6587\u4ef6\u7684\u5730\u65b9\u3002\u5c06 model_dir \u8bbe\u7f6e\u4e3a\u5305\u542b mnist_cnn.pt \u548c\u6b64\u811a\u672c\u7684\u6587\u4ef6\u5939\u3002 load_now \u786e\u5b9a\u6a21\u578b\u662f\u5426\u4f1a\u5728\u6ce8\u518c\u671f\u95f4\u7acb\u5373\u52a0\u8f7d\u3002\u9ed8\u8ba4\u503c\u4e3a\u201c\u771f\u201d\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a False \uff0c\u5219\u9700\u8981\u8c03\u7528 load API \u52a0\u8f7d\u6a21\u578b\u624d\u80fd\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f"},{"location":"rc/get-started/pytorch-mnist/#_9","text":"Only Backend Frontend and Backend $ uvicorn func_app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer func_app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_10","text":"\u8fd0\u884c\u6d4b\u8bd5: $ python test.py Prediction: 4 \u4e0d\u51fa\u610f\u5916\uff0c\u7ed3\u679c\u4e00\u6837\u3002","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/#_11","text":"\u4f7f\u7528 Pinferencia \uff0c\u60a8\u53ef\u4ee5\u4e3a\u4efb\u4f55\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u81ea\u5df1\u52a0\u8f7d\u6a21\u578b\uff0c\u5c31\u50cf\u60a8\u5728\u8fdb\u884c\u79bb\u7ebf\u9884\u6d4b\u65f6\u6240\u505a\u7684\u90a3\u6837\u3002 \u8fd9\u90e8\u5206\u4ee3\u7801\u4f60\u65e9\u5c31\u5df2\u7ecf\u5199\u597d\u4e86\u3002 \u7136\u540e\uff0c\u53ea\u9700\u4f7f\u7528 Pinferencia \u6ce8\u518c\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u5c31\u4f1a\u751f\u6548\u3002 \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5c06\u4ee3\u7801\u91cd\u6784\u4e3a Handler Class \u3002\u65e7\u7684\u7ecf\u5178\u65b9\u5f0f\u4e5f\u9002\u7528\u4e8e Pinferencia \u3002 \u8fd9\u4e24\u4e2a\u4e16\u754c\u90fd\u9002\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c \u7ecf\u5178\u200b\u200b\u97f3\u4e50 \u548c \u6447\u6eda\u4e50 \u3002 \u662f\u4e0d\u662f\u5f88\u68d2\uff01 \u73b0\u5728\u60a8\u5df2\u7ecf\u638c\u63e1\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u6765\uff1a \u6ce8\u518c\u4efb\u4f55\u6a21\u578b\u3001\u4efb\u4f55\u51fd\u6570\u5e76\u628a\u5b83\u4eec\u4e0a\u7ebf\u3002 \u4f7f\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u5904\u7406\u7a0b\u5e8f\u4e3a\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u670d\u52a1\u3002","title":"\u6700\u540e"},{"location":"rc/get-started/pytorch-mnist/bonus/","text":"\u7279\u6b8a\u4efb\u52a1 \u00b6 \u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002 MNIST \u56fe\u50cf\u6c42\u548c \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002 \u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u5bf9\u5b83\u4eec\u6c42\u548c\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u524d\u7aef\u6a21\u677f\uff0c\u63a5\u53d7\u4e24\u4e2a MNIST \u56fe\u50cf\u5e76\u5c06\u5b83\u4eec\u53d1\u9001\u56de\u6211\u4eec\u7684\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002 \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 \u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u677f\uff1f \u60a8\u53ef\u4ee5\u5728 \u81ea\u5b9a\u4e49\u6a21\u677f \u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002 \u81ea\u5b9a\u4e49\u6a21\u677f \u00b6 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u65b0\u6a21\u677f\uff1a sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) \u8fd9\u91cc\u6211\u4eec\u5c06\u5185\u5bb9\u9762\u677f\u5206\u6210\u4e24\u5217\uff0c\u6bcf\u5217\u63a5\u53d7\u4e00\u4e2a MNIST \u56fe\u50cf\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u5982\u679c\u4e24\u5f20\u56fe\u7247\u90fd\u4e0a\u4f20\u4e86\uff0c\u53d1\u9001\u5230\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002 \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 \u5728\u81ea\u5b9a\u4e49\u6a21\u677f\u6587\u4ef6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u4ee3\u7801\u6765\u81ea\u5b9a\u4e49\u524d\u7aef\u670d\u52a1\u3002 sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, ) \u540e\u7aef \u00b6 \u5728\u6211\u4eec\u81ea\u5b9a\u4e49\u524d\u7aef\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u6a21\u578b\u6ce8\u518c\u65f6\u76f4\u63a5\u4f7f\u7528\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u677f\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u6570\u5b57\u5e76\u8fdb\u884c\u6c42\u548c\u3002 \u5c06\u6211\u4eec\u7684\u65b0\u6a21\u677f\u201cSum Mnist\u201d\u6ce8\u518c\u4e3a\u9ed8\u8ba4\u6a21\u677f\u3002 \u542f\u52a8\u670d\u52a1 \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u73a9\u5f97\u5f00\u5fc3**Pinferencia**\uff01","title":"MNIST \u7279\u522b\u4efb\u52a1"},{"location":"rc/get-started/pytorch-mnist/bonus/#_1","text":"\u5982\u679c\u4f60\u8fd8\u6709\u65f6\u95f4\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e9b\u6709\u8da3\u7684\u4e8b\u60c5\u3002","title":"\u7279\u6b8a\u4efb\u52a1"},{"location":"rc/get-started/pytorch-mnist/bonus/#mnist","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u201csum_mnist.py\u201d\u3002 \u5b83\u63a5\u53d7\u4e00\u7ec4\u56fe\u50cf\uff0c\u9884\u6d4b\u5b83\u4eec\u7684\u6570\u5b57\u5e76\u5bf9\u5b83\u4eec\u6c42\u548c\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u524d\u7aef\u6a21\u677f\uff0c\u63a5\u53d7\u4e24\u4e2a MNIST \u56fe\u50cf\u5e76\u5c06\u5b83\u4eec\u53d1\u9001\u56de\u6211\u4eec\u7684\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002","title":"MNIST \u56fe\u50cf\u6c42\u548c"},{"location":"rc/get-started/pytorch-mnist/bonus/#_2","text":"\u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u677f\uff1f \u60a8\u53ef\u4ee5\u5728 \u81ea\u5b9a\u4e49\u6a21\u677f \u627e\u5230\u66f4\u591a\u4fe1\u606f\u3002","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"rc/get-started/pytorch-mnist/bonus/#_3","text":"\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u65b0\u6a21\u677f\uff1a sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) # (1) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digit:\" ) images = [] if first_number is not None : # (2) image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : # (3) image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : # (4) with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) \u8fd9\u91cc\u6211\u4eec\u5c06\u5185\u5bb9\u9762\u677f\u5206\u6210\u4e24\u5217\uff0c\u6bcf\u5217\u63a5\u53d7\u4e00\u4e2a MNIST \u56fe\u50cf\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u56fe\u50cf\u4e0a\u4f20\u540e\uff0c\u5c06\u5176\u9644\u52a0\u5230\u56fe\u50cf\u6570\u7ec4\u4e2d\u4ee5\u4f9b\u4ee5\u540e\u9884\u6d4b\u3002 \u5982\u679c\u4e24\u5f20\u56fe\u7247\u90fd\u4e0a\u4f20\u4e86\uff0c\u53d1\u9001\u5230\u540e\u7aef\u8fdb\u884c\u9884\u6d4b\u3002","title":"\u81ea\u5b9a\u4e49\u6a21\u677f"},{"location":"rc/get-started/pytorch-mnist/bonus/#_4","text":"\u5728\u81ea\u5b9a\u4e49\u6a21\u677f\u6587\u4ef6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e9b\u989d\u5916\u7684\u4ee3\u7801\u6765\u81ea\u5b9a\u4e49\u524d\u7aef\u670d\u52a1\u3002 sum_mnist_frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import base64 import streamlit as st from PIL import Image from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate from pinferencia.frontend.templates.utils import display_text_prediction class SumMnistTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Sum</span> ' '<span style=\"color:slategray;\">MNIST</span> ' ) def render ( self ): super () . render () col1 , col2 = st . columns ( 2 ) with col1 . form ( \"First Image\" , clear_on_submit = True ): first_number = col1 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"1\" ) with col2 . form ( \"Second Image\" , clear_on_submit = True ): second_number = col2 . file_uploader ( \"Choose an image...\" , type = [ \"jpg\" , \"png\" , \"jpeg\" ], key = \"2\" ) st . markdown ( \"##### Sum of the two digits:\" ) images = [] if first_number is not None : image1 = Image . open ( first_number ) col1 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( first_number . getvalue ()) . decode ()) if second_number is not None : image1 = Image . open ( second_number ) col2 . image ( image1 , use_column_width = True ) images . append ( base64 . b64encode ( second_number . getvalue ()) . decode ()) if first_number and second_number : with st . spinner ( \"Waiting for result\" ): prediction = self . predict ( images ) display_text_prediction ( prediction , component = st ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Sum Mnist\" : SumMnistTemplate }, )","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"rc/get-started/pytorch-mnist/bonus/#_5","text":"\u5728\u6211\u4eec\u81ea\u5b9a\u4e49\u524d\u7aef\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u6a21\u578b\u6ce8\u518c\u65f6\u76f4\u63a5\u4f7f\u7528\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u677f\u3002 sum_mnist.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import base64 import pathlib from io import BytesIO import torch from PIL import Image from pinferencia import Server from pinferencia.handlers import BaseHandler from torchvision import transforms from main import Net class MNISTHandler ( BaseHandler ): transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)), ] ) use_cuda = torch . cuda . is_available () device = torch . device ( \"cuda\" if use_cuda else \"cpu\" ) def load_model ( self ): model = Net () . to ( self . device ) model . load_state_dict ( torch . load ( self . model_path )) model . eval () return model def predict ( self , data : list ) -> int : tensors = [] # (1) for img in data : image = Image . open ( BytesIO ( base64 . b64decode ( img ))) tensors . append ( self . transform ( image )) input_data = torch . stack ( tensors ) . to ( self . device ) return sum ( self . model ( input_data ) . argmax ( 1 ) . tolist ()) service = Server ( model_dir = pathlib . Path ( __file__ ) . parent . resolve ()) service . register ( model_name = \"mnist\" , model = \"mnist_cnn.pt\" , handler = MNISTHandler , load_now = True , metadata = { \"task\" : \"Sum Mnist\" }, # (2) ) \u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u6d4b\u5176\u6570\u5b57\u5e76\u8fdb\u884c\u6c42\u548c\u3002 \u5c06\u6211\u4eec\u7684\u65b0\u6a21\u677f\u201cSum Mnist\u201d\u6ce8\u518c\u4e3a\u9ed8\u8ba4\u6a21\u677f\u3002","title":"\u540e\u7aef"},{"location":"rc/get-started/pytorch-mnist/bonus/#_6","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/pytorch-mnist/bonus/#_7","text":"\u73a9\u5f97\u5f00\u5fc3**Pinferencia**\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/get-started/serve-a-function/","text":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570 \u00b6 \u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002 \u4efb\u52a1 \u00b6 \u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc ) \u542f\u52a8\u670d\u52a1\u5668 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m. \u6b64\u5916 \u00b6 \u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/serve-a-function/#_1","text":"\u597d\u5427\uff0c\u670d\u52a1\u4e00\u4e2a\u51fd\u6570\uff1f\u6709\u7528\u5417\uff1f \u5f53\u7136\u662f\u7684\u3002 \u5982\u679c\u60a8\u6709 \u4e00\u4e2a\u5b8c\u6574\u7684\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b \uff0c\u5b83\u5305\u542b\u8bb8\u591a\u6b65\u9aa4\u3002\u5927\u591a\u6570\u65f6\u5019\uff0c\u60a8\u5c06\u5b9e\u73b0\u4e00\u4e2a\u51fd\u6570\u6765\u5b8c\u6210\u8fd9\u9879\u5de5\u4f5c\u3002\u73b0\u5728\u60a8\u53ef\u4ee5\u7acb\u5373\u6ce8\u518c\u8be5\u51fd\u6570\u3002 \u5982\u679c\u4f60\u60f3\u5206\u4eab\u4e00\u4e9b\u9884\u5904\u7406\u6216\u540e\u5904\u7406\u529f\u80fd\uff0c\u73b0\u5728\u4f60\u6709\u4f60\u7684\u7f57\u5bbe\u4e86\uff0c \u8759\u8760\u4fa0 \uff01 \u6216\u8005\u4e00\u4e2a\u51fd\u6570\u5bf9\u4f60\u7684\u5de5\u4f5c\u6765\u8bf4\u5c31\u8db3\u591f\u4e86\u3002","title":"\u542f\u52a8\u4e00\u4e2a\u51fd\u6570"},{"location":"rc/get-started/serve-a-function/#_2","text":"\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4efd\u5c71\u8109\u9ad8\u5ea6\u7684\u5217\u8868\u3002\u6211\u4eec\u9700\u8981\u627e\u51fa\u6700\u9ad8\u3001\u6700\u4f4e\u4ee5\u53ca\u6700\u9ad8\u548c\u6700\u4f4e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u9898\uff0c\u8ba9\u6211\u4eec\u5728\u4e00\u4e2a\u51fd\u6570\u4e2d\u89e3\u51b3\u5b83\uff0c\u8ba9\u60a8\u66f4\u719f\u6089\u8fd9\u4e2a\u6982\u5ff5\u3002 graph LR heights(\u5c71\u7684\u9ad8\u5ea6) --> max(\u627e\u51fa\u6700\u9ad8\u7684&nbsp&nbsp) heights --> min(\u627e\u51fa\u6700\u4f4e\u7684&nbsp&nbsp) min --> diff(\u8ba1\u7b97\u5dee\u5f02) max --> diff diff --> output(\u8f93\u51fa) subgraph Workflow max min diff end","title":"\u4efb\u52a1"},{"location":"rc/get-started/serve-a-function/#_3","text":"\u5c06\u4ee5\u4e0b\u4ee3\u7801\u4fdd\u5b58\u5728 app.py \u4e2d\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from typing import List from pinferencia import Server def calc ( data : List [ int ]) -> int : highest = max ( data ) lowest = min ( data ) return highest - lowest service = Server () service . register ( model_name = \"mountain\" , model = calc )","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"rc/get-started/serve-a-function/#_4","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u542f\u52a8\u670d\u52a1\u5668"},{"location":"rc/get-started/serve-a-function/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a You need to have requests installed. pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/mountain/predict\" , json = { \"data\" : [ 1000 , 2000 , 3000 ]}, ) difference = response . json ()[ \"data\" ] print ( f \"Difference between the highest and lowest is { difference } m.\" ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py Difference between the highest and lowest is 2000m.","title":"\u6d4b\u8bd5 API"},{"location":"rc/get-started/serve-a-function/#_5","text":"\u73b0\u5728\u4f60\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u4e3a\u201c\u7c7b\u201d\u6216\u201c\u51fd\u6570\u201d\u6a21\u578b\u4e0a\u7ebf\u3002 \u5982\u679c\u60a8\u53ea\u6709\u4e00\u4e2a\u6a21\u578b\u8981\u670d\u52a1\uff0c\u90a3\u5f88\u5bb9\u6613\u3002 \u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u6709\u81ea\u5b9a\u4e49\u4ee3\u7801\uff0c\u4f8b\u5982\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u3002\u6709\u4e9b\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u9884\u6d4b\u52a8\u7269\u7684\u54c1\u79cd\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4ee5\u4e0b\u5de5\u4f5c\u6d41\u7a0b\uff1a graph LR pic(\u56fe\u7247) --> species(\u7269\u79cd\u5206\u7c7b) species --> cat(Cat) --> cat_breed(\u732b\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp) --> Persian(\u6ce2\u65af\u732b) species-->\u72d7(\u72d7)--> dog_breed(\u72d7\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u62c9\u5e03\u62c9\u591a(\u62c9\u5e03\u62c9\u591a) species-->\u7334\u5b50(\u7334\u5b50)-->\u7334\u5b50\u54c1\u79cd(\u7334\u5b50\u54c1\u79cd\u5206\u7c7b&nbsp&nbsp)-->\u8718\u86db(\u8718\u86db\u7334) \u5728\u8bb8\u591a\u5e73\u53f0\u6216\u5de5\u5177\u4e0a\u90e8\u7f72\u5b83\u5e76\u4e0d\u5bb9\u6613\u3002 \u4f46\u662f\uff0c\u73b0\u5728\u60a8\u62e5\u6709 Pinferencia \uff0c\u60a8\u591a\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u9009\u62e9\uff01","title":"\u6b64\u5916"},{"location":"rc/get-started/serve-a-json-model/","text":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b \u00b6 \u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model \u5b9a\u4e49 JSON \u6a21\u578b \u00b6 \u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u60a8\u53ef\u4ee5\u4f7f\u7528 Python 3 Type Hints \u6765\u5b9a\u4e49\u6a21\u578b\u670d\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u5728 Define Request and Response Schema \u4e2d\u67e5\u770b Pinferencia \u5982\u4f55\u5229\u7528 Type Hints \u7684\u3002 \u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b \u00b6 \u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) model_name\uff0c entrypoint \u548c task \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 task \u6307\u793a\u6a21\u578b\u6b63\u5728\u6267\u884c\u7684\u4efb\u52a1\u7c7b\u578b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86\u6a21\u578b\u7684 task \uff0c\u5c06\u81ea\u52a8\u9009\u62e9\u76f8\u5e94\u7684\u524d\u7aef\u6a21\u677f\u3002 \u6a21\u677f\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728 \u524d\u7aef\u8981\u6c42 \u4e2d\u627e\u5230 \u542f\u52a8\u670d\u52a1 \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee: http://127.0.0.1:8501 , \u4f60\u62e5\u6709\u4e86\u53ef\u4ee5\u4e0e\u4f60\u6a21\u578b\u4ea4\u4e92\u7684\u56fe\u5f62\u4ecb\u9762\u3002 http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u9ed8\u8ba4\u6587\u6863\u5730\u5740\u5728: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia \u524d\u7aef\u57fa\u4e8e Streamlit . \u9ed8\u8ba4\u90e8\u7f72\u5730\u5740\u5728: http://127.0.0.1:8501 \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01 \u4f7f\u7528\u524d\u7aef\u4ecb\u9762 \u00b6 \u6d4b\u8bd5 API \u00b6 \u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': [1]} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u542f\u52a8\u4e00\u4e2a\u7b80\u5355\u7684 JSON \u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#json","text":"\u73b0\u5728\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u8ba9\u4f60\u6765\u719f\u6089 Pinferecia . \u592a\u957f\u4e0d\u770b \u719f\u6089\u5982\u4f55\u901a\u8fc7 Pinferencia \u6ce8\u518c\u548c\u4e0a\u7ebf\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u4f60\u60f3\u73b0\u5728\u5c31\u5c1d\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u79fb\u6b65 \u542f\u52a8 Pytorch MNIST Model","title":"\u542f\u52a8\u4e00\u4e2a JSON \u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#json_1","text":"\u8ba9\u6211\u4eec\u5148\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6 app.py . \u4e0b\u9762\u5c31\u662f\u8fd9\u4e2a JSON \u6a21\u578b. \u8f93\u5165\u662f a \u8fd4\u56de 1 , \u8f93\u5165 b \u8fd4\u56de 2 , \u5176\u4ed6\u8f93\u5165\u8fd4\u56de 0 \u3002 app.py 1 2 3 4 class JSONModel : def predict ( self , data : str ) -> int : # (1) knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) \u60a8\u53ef\u4ee5\u4f7f\u7528 Python 3 Type Hints \u6765\u5b9a\u4e49\u6a21\u578b\u670d\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u5728 Define Request and Response Schema \u4e2d\u67e5\u770b Pinferencia \u5982\u4f55\u5229\u7528 Type Hints \u7684\u3002","title":"\u5b9a\u4e49 JSON \u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#_1","text":"\u9996\u5148\u4ece pinferencia \u5bfc\u5165 Server , \u7136\u540e\u521b\u5efa\u4e00\u4e2aserver\u5b9e\u4f8b\u5e76\u6ce8\u518c JSON \u6a21\u578b . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pinferencia import Server , task class JSONModel : def predict ( self , data : str ) -> int : knowledge = { \"a\" : 1 , \"b\" : 2 } return knowledge . get ( data , 0 ) model = JSONModel () service = Server () service . register ( model_name = \"json\" , model = model , entrypoint = \"predict\" , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) model_name\uff0c entrypoint \u548c task \u662f\u4ec0\u4e48\u610f\u601d? model_name \u4f60\u7ed9\u8fd9\u4e2a\u6a21\u578b\u53d6\u7684\u540d\u5b57\u3002 \u8fd9\u91cc\u6211\u4eec\u53d6\u540d json , \u5bf9\u5e94\u7684\u8fd9\u4e2a\u6a21\u578b\u7684\u5730\u5740\u5c31\u662f http://127.0.0.1:8000/v1/models/json . \u5982\u679c\u5173\u4e8eAPI\u4f60\u6709\u4ec0\u4e48\u4e0d\u6e05\u695a\u7684\uff0c\u4f60\u53ef\u4ee5\u968f\u65f6\u8bbf\u95ee\u4e0b\u9762\u5c06\u8981\u63d0\u5230\u7684\u5728\u7ebfAPI\u6587\u6863\u9875\u9762\u3002 entrypoint predict \u610f\u5473\u7740\u6211\u4eec\u4f1a\u4f7f\u7528 JSON \u6a21\u578b \u7684 predict \u51fd\u6570\u6765\u9884\u6d4b\u6570\u636e\u3002 task \u6307\u793a\u6a21\u578b\u6b63\u5728\u6267\u884c\u7684\u4efb\u52a1\u7c7b\u578b\u3002 \u5982\u679c\u63d0\u4f9b\u4e86\u6a21\u578b\u7684 task \uff0c\u5c06\u81ea\u52a8\u9009\u62e9\u76f8\u5e94\u7684\u524d\u7aef\u6a21\u677f\u3002 \u6a21\u677f\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728 \u524d\u7aef\u8981\u6c42 \u4e2d\u627e\u5230","title":"\u521b\u5efa\u670d\u52a1\u5e76\u6ce8\u518c\u6a21\u578b"},{"location":"rc/get-started/serve-a-json-model/#_2","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee: http://127.0.0.1:8501 , \u4f60\u62e5\u6709\u4e86\u53ef\u4ee5\u4e0e\u4f60\u6a21\u578b\u4ea4\u4e92\u7684\u56fe\u5f62\u4ecb\u9762\u3002 http://127.0.0.1:8000 , \u73b0\u5728\u4f60\u62e5\u6709\u4e86\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7684 API \u6587\u6863\u9875\u9762! FastAPI \u548c Starlette Pinferencia \u57fa\u4e8e FastAPI \uff0c\u5176\u53c8\u57fa\u4e8e Starlette . \u591a\u4e8f\u4e86\u4ed6\u4eec\uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5e26\u6709 OpenAPI \u89c4\u8303\u7684 API\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u81ea\u52a8\u6587\u6863\u7f51\u9875\uff0c\u5e76\u4e14\u5ba2\u6237\u7aef\u4ee3\u7801\u4e5f\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u3002 \u9ed8\u8ba4\u6587\u6863\u5730\u5740\u5728: http://127.0.0.1:8000 or http://127.0.0.1:8000/docs Streamlit Pinferencia \u524d\u7aef\u57fa\u4e8e Streamlit . \u9ed8\u8ba4\u90e8\u7f72\u5730\u5740\u5728: http://127.0.0.1:8501 \u60a8\u53ef\u4ee5\u67e5\u770b API \u89c4\u8303\uff0c\u751a\u81f3\u53ef\u4ee5\u81ea\u5df1 \u8bd5\u7528 API\uff01","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/get-started/serve-a-json-model/#_3","text":"","title":"\u4f7f\u7528\u524d\u7aef\u4ecb\u9762"},{"location":"rc/get-started/serve-a-json-model/#api","text":"\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a test.py \u3002 \u63d0\u793a \u4f60\u9700\u8981\u5b89\u88c5 requests . pip install requests test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : \"a\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c. $ python test.py {'model_name': 'json', 'data': [1]} \u73b0\u5728\u8ba9\u6211\u4eec\u518d\u6dfb\u52a0\u4e24\u4e2a\u8f93\u5165\uff0c\u5e76\u8ba9\u6253\u5370\u66f4\u6f02\u4eae. test.py 1 2 3 4 5 6 7 8 9 10 11 import requests print ( \"| {:^10} | {:^15} |\" . format ( \"Input\" , \"Prediction\" )) print ( \"| {:^10} | {:^15} |\" . format ( \"-\" * 10 , \"-\" * 15 )) for character in [ \"a\" , \"b\" , \"c\" ]: response = requests . post ( url = \"http://localhost:8000/v1/models/json/predict\" , json = { \"data\" : character }, ) print ( f \"| { character : ^10 } | { str ( response . json ()[ 'data' ]) : ^15 } |\" ) \u518d\u6b21\u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py | Input | Prediction | |----------|---------------| | a | 1 | | b | 2 | | c | 0 |","title":"\u6d4b\u8bd5 API"},{"location":"rc/how-to-guides/custom-frontend/","text":"\u81ea\u5b9a\u4e49\u524d\u7aef\u4fe1\u606f \u00b6 Pinferencia \u524d\u7aef\u652f\u6301\u81ea\u5b9a\u4e49\uff1a \u7f51\u9875\u7684\u6807\u9898 \u4f7f\u7528\u6a21\u578b display_name \u4f5c\u4e3a\u6a21\u677f\u7684\u6807\u9898 \u7b80\u77ed\u7684\u4ecb\u7ecd \u548c\u8be6\u7ec6\u8bf4\u660e \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) \u8fd9\u5c06\u66f4\u6539\u53f3\u4fa7\u5185\u5bb9\u533a\u57df\u663e\u793a\u7684\u9ed8\u8ba4\u6a21\u677f\u6807\u9898\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a \u81ea\u5b9a\u4e49\u524d\u7aef \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) \u8fd9\u5c06\u6539\u53d8\u5de6\u4fa7\u9762\u677f\u9876\u90e8\u663e\u793a\u7684\u6807\u9898\u3002 \u8fd9\u5c06\u66f4\u6539\u5de6\u4fa7\u9762\u677f\u6807\u9898\u4e0b\u65b9\u7684\u63cf\u8ff0\u3002 \u8fd9\u5c06\u6539\u53d8\u9875\u9762\u7684\u5173\u4e8e\u4fe1\u606f\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"rc/how-to-guides/custom-frontend/#_1","text":"Pinferencia \u524d\u7aef\u652f\u6301\u81ea\u5b9a\u4e49\uff1a \u7f51\u9875\u7684\u6807\u9898 \u4f7f\u7528\u6a21\u578b display_name \u4f5c\u4e3a\u6a21\u677f\u7684\u6807\u9898 \u7b80\u77ed\u7684\u4ecb\u7ecd \u548c\u8be6\u7ec6\u8bf4\u660e","title":"\u81ea\u5b9a\u4e49\u524d\u7aef\u4fe1\u606f"},{"location":"rc/how-to-guides/custom-frontend/#_2","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> float : return sum ( data ) service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"display_name\" : \"Awesome Model\" }, # (1) ) \u8fd9\u5c06\u66f4\u6539\u53f3\u4fa7\u5185\u5bb9\u533a\u57df\u663e\u793a\u7684\u9ed8\u8ba4\u6a21\u677f\u6807\u9898\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u670d\u52a1"},{"location":"rc/how-to-guides/custom-frontend/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pinferencia.frontend.app import Server detail_description = \"\"\" # My Awesome Model This is the service of my awesome model. It is **fast**, **simple**, and **beautiful**. Visit [My Awesome Model Home](/abc) to learn more about it. \"\"\" service = Server ( title = \"My Awesome Model\" , # (1) short_description = \"This is the short description\" , # (2) detail_description = detail_description , # (3) backend_server = \"http://127.0.0.1:8000\" , ) \u8fd9\u5c06\u6539\u53d8\u5de6\u4fa7\u9762\u677f\u9876\u90e8\u663e\u793a\u7684\u6807\u9898\u3002 \u8fd9\u5c06\u66f4\u6539\u5de6\u4fa7\u9762\u677f\u6807\u9898\u4e0b\u65b9\u7684\u63cf\u8ff0\u3002 \u8fd9\u5c06\u6539\u53d8\u9875\u9762\u7684\u5173\u4e8e\u4fe1\u606f\u3002 \u73b0\u5728\u542f\u52a8\u670d\u52a1\uff1a $ pinfer app:service --frontend-script = frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u4f60\u4f1a\u5f97\u5230\uff1a","title":"\u81ea\u5b9a\u4e49\u524d\u7aef"},{"location":"rc/how-to-guides/custom-templates/","text":"\u81ea\u5b9a\u4e49\u6a21\u677f \u00b6 \u5c3d\u7ba1\u6709\u5185\u7f6e\u6a21\u677f\uff0c\u4f46\u5b83\u6c38\u8fdc\u4e0d\u8db3\u4ee5\u6db5\u76d6\u6240\u6709\u573a\u666f\u3002 Pinferencia \u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u677f\u3002\u81ea\u5b9a\u4e49\u6a21\u677f\u5e76\u5728\u60a8\u7684\u670d\u52a1\u4e2d\u4f7f\u7528\u5b83\u5f88\u5bb9\u6613\u3002 \u9996\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u677f\uff1a \u8f93\u5165\u6570\u5b57\u5217\u8868\u3002 \u663e\u793a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3001\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002 \u6a21\u578b \u00b6 \u6a21\u578b\u5f88\u7b80\u5355\uff0c\u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" }) \u6a21\u677f \u00b6 Pinferencia \u63d0\u4f9b\u4e86 BaseTemplate \u6765\u6269\u5c55\u4ee5\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u677f\u3002 JSON \u8f93\u5165 \u00b6 \u9996\u5148\uff0c\u6211\u4eec\u5c06\u9875\u9762\u5206\u4e3a\u4e24\u5217\uff0c\u5e76\u5206\u522b\u521b\u5efa\u4e00\u4e2a JSON \u8f93\u5165\u5b57\u6bb5\u548c\u663e\u793a\u5b57\u6bb5\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u542f\u52a8\u670d\u52a1 \u00b6 $ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u5e76\u6253\u5f00\u6d4f\u89c8\u5668\u4f60\u4f1a\u770b\u5230\uff1a \u8c03\u7528\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c \u00b6 \u6dfb\u52a0\u4ee5\u4e0b\u9ad8\u4eae\u663e\u793a\u7684\u4ee3\u7801\u4ee5\u5c06\u8bf7\u6c42\u53d1\u9001\u5230\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u63d0\u4f9b\u4e00\u4e2a\u6309\u94ae\u6765\u89e6\u53d1\u9884\u6d4b\u3002 \u53d1\u9001\u8bf7\u6c42\u65f6\u663e\u793a\u4e00\u4e2a\u7b49\u5f85\u6548\u679c\u3002 \u5c06\u6570\u636e\u53d1\u9001\u5230\u540e\u7aef\u3002 \u5c06\u7ed3\u679c\u5206\u4e3a\u4e09\u5217\u5c55\u793a\u3002 \u518d\u6b21\u542f\u52a8\u670d\u52a1\uff0c\u60a8\u5c06\u770b\u5230\uff1a \u00b6","title":"\u81ea\u5b9a\u4e49\u6a21\u7248"},{"location":"rc/how-to-guides/custom-templates/#_1","text":"\u5c3d\u7ba1\u6709\u5185\u7f6e\u6a21\u677f\uff0c\u4f46\u5b83\u6c38\u8fdc\u4e0d\u8db3\u4ee5\u6db5\u76d6\u6240\u6709\u573a\u666f\u3002 Pinferencia \u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u677f\u3002\u81ea\u5b9a\u4e49\u6a21\u677f\u5e76\u5728\u60a8\u7684\u670d\u52a1\u4e2d\u4f7f\u7528\u5b83\u5f88\u5bb9\u6613\u3002 \u9996\u5148\u8ba9\u6211\u4eec\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u677f\uff1a \u8f93\u5165\u6570\u5b57\u5217\u8868\u3002 \u663e\u793a\u6570\u5b57\u7684\u5e73\u5747\u503c\u3001\u6700\u5927\u503c\u548c\u6700\u5c0f\u503c\u3002","title":"\u81ea\u5b9a\u4e49\u6a21\u677f"},{"location":"rc/how-to-guides/custom-templates/#_2","text":"\u6a21\u578b\u5f88\u7b80\u5355\uff0c\u670d\u52a1\u53ef\u4ee5\u5b9a\u4e49\u4e3a\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from typing import List from pinferencia import Server def stat ( data : List [ float ]) -> dict : return { \"mean\" : sum ( data ) / len ( data ), \"max\" : max ( data ), \"min\" : min ( data ), } service = Server () service . register ( model_name = \"stat\" , model = stat , metadata = { \"task\" : \"Stat\" })","title":"\u6a21\u578b"},{"location":"rc/how-to-guides/custom-templates/#_3","text":"Pinferencia \u63d0\u4f9b\u4e86 BaseTemplate \u6765\u6269\u5c55\u4ee5\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u677f\u3002","title":"\u6a21\u677f"},{"location":"rc/how-to-guides/custom-templates/#json","text":"\u9996\u5148\uff0c\u6211\u4eec\u5c06\u9875\u9762\u5206\u4e3a\u4e24\u5217\uff0c\u5e76\u5206\u522b\u521b\u5efa\u4e00\u4e2a JSON \u8f93\u5165\u5b57\u6bb5\u548c\u663e\u793a\u5b57\u6bb5\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, )","title":"JSON \u8f93\u5165"},{"location":"rc/how-to-guides/custom-templates/#_4","text":"$ pinfer sum_mnist:service --frontend-script = sum_mnist_frontend.py Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u5e76\u6253\u5f00\u6d4f\u89c8\u5668\u4f60\u4f1a\u770b\u5230\uff1a","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/how-to-guides/custom-templates/#_5","text":"\u6dfb\u52a0\u4ee5\u4e0b\u9ad8\u4eae\u663e\u793a\u7684\u4ee3\u7801\u4ee5\u5c06\u8bf7\u6c42\u53d1\u9001\u5230\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c\u3002 frontend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import json import streamlit as st from pinferencia.frontend.app import Server from pinferencia.frontend.templates.base import BaseTemplate class StatTemplate ( BaseTemplate ): title = ( '<span style=\"color:salmon;\">Numbers</span> ' '<span style=\"color:slategray;\">Statistics</span>' ) def render ( self ): super () . render () json_template = \"[]\" col1 , col2 = st . columns ( 2 ) col2 . write ( \"Request Preview\" ) raw_text = col1 . text_area ( \"Raw Data\" , value = json_template , height = 150 ) col2 . json ( raw_text ) pred_btn = st . button ( \"Run\" ) # (1) if pred_btn : with st . spinner ( \"Wait for result\" ): # (2) prediction = self . predict ( json . loads ( raw_text )) # (3) st . write ( \"Statistics\" ) result_col1 , result_col2 , result_col3 = st . columns ( 3 ) # (4) result_col1 . metric ( label = \"Max\" , value = prediction . get ( \"max\" )) result_col2 . metric ( label = \"Min\" , value = prediction . get ( \"min\" )) result_col3 . metric ( label = \"Mean\" , value = prediction . get ( \"mean\" )) backend_address = \"http://127.0.0.1:8000\" service = Server ( backend_server = f \" { backend_address } \" , custom_templates = { \"Stat\" : StatTemplate }, ) \u63d0\u4f9b\u4e00\u4e2a\u6309\u94ae\u6765\u89e6\u53d1\u9884\u6d4b\u3002 \u53d1\u9001\u8bf7\u6c42\u65f6\u663e\u793a\u4e00\u4e2a\u7b49\u5f85\u6548\u679c\u3002 \u5c06\u6570\u636e\u53d1\u9001\u5230\u540e\u7aef\u3002 \u5c06\u7ed3\u679c\u5206\u4e3a\u4e09\u5217\u5c55\u793a\u3002","title":"\u8c03\u7528\u540e\u7aef\u5e76\u663e\u793a\u7ed3\u679c"},{"location":"rc/how-to-guides/custom-templates/#_6","text":"","title":"\u518d\u6b21\u542f\u52a8\u670d\u52a1\uff0c\u60a8\u5c06\u770b\u5230\uff1a"},{"location":"rc/how-to-guides/huggingface/dependencies/","text":"\u5bf9\u4e8emac\u7528\u6237 \u00b6 \u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh \u5b89\u88c5\u4f9d\u8d56 \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses transformers \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/how-to-guides/huggingface/dependencies/#mac","text":"\u5982\u679c\u4f60\u50cf\u6211\u4e00\u6837\u5728 M1 Mac \u4e0a\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 cmake \u548c rust brew install cmake curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh","title":"\u5bf9\u4e8emac\u7528\u6237"},{"location":"rc/how-to-guides/huggingface/dependencies/#_1","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 pip \u5b89\u88c5\u4f9d\u8d56\u9879\u3002 pip install tqdm boto3 requests regex sentencepiece sacremoses transformers \u6216\u8005\u60a8\u53ef\u4ee5\u6539\u7528 docker \u6620\u50cf\uff1a docker run -it -p 8000 :8000 -v $( pwd ) :/opt/workspace huggingface/transformers-pytorch-cpu:4.18.0 bash","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/","text":"\u4f60\u4eec\u4e2d\u7684\u8bb8\u591a\u4eba\u4e00\u5b9a\u542c\u8bf4\u8fc7\u201cBert\u201d\u6216\u201ctransformers\u201d\u3002 \u4f60\u53ef\u80fd\u8fd8\u77e5\u9053huggingface\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u8ba9\u6211\u4eec\u4f7f\u7528\u5b83\u7684 pytorch \u8f6c\u6362\u5668\u6a21\u578b\u5e76\u901a\u8fc7 REST API \u4e3a\u5b83\u63d0\u4f9b\u670d\u52a1 \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"Bert"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#_1","text":"\u8f93\u5165\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa Paris is the [MASK] of France. Paris is the capital of France. \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#_2","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#_3","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from transformers import pipeline from pinferencia import Server , task bert = pipeline ( \"fill-mask\" , model = \"bert-base-uncased\" ) def predict ( text : str ) -> list : return bert ( text ) service = Server () service . register ( model_name = \"bert\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/bert/#_4","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/bert/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"Paris is the [MASK] of France.\" }' \u54cd\u5e94 { \"model_name\":\"bert\", \"data\":\"Paris is the capital of France.\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/bert/predict\" , json = { \"data\" : \"Paris is the [MASK] of France.\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'bert', 'data': 'Paris is the capital of France.'} \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/","text":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1 \u00b6 \u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f \u5982\u4f55\u4f7f\u7528 \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u5982\u4f55\u90e8\u7f72 \u00b6 \u5b89\u88c5 Pinferencia \u00b6 $ pip install \"pinferencia[streamlit]\" ---> 100% \u521b\u5efa\u670d\u52a1 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u672c\u751f\u6210 - GPT2"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#gpt2-","text":"\u4ec0\u4e48\u662f\u6587\u672c\u751f\u6210\uff1f\u8f93\u5165\u4e00\u4e9b\u6587\u672c\uff0c\u6a21\u578b\u5c06\u9884\u6d4b\u540e\u7eed\u6587\u672c\u4f1a\u662f\u4ec0\u4e48\u3002 \u542c\u8d77\u6765\u4e0d\u9519\u3002\u4e0d\u8fc7\u4e0d\u4eb2\u81ea\u5c1d\u8bd5\u6a21\u578b\u600e\u4e48\u53ef\u80fd\u6709\u8da3\uff1f","title":"GPT2\u200a-\u200a\u6587\u672c\u751f\u6210\u8f6c\u6362\u5668\uff1a\u5982\u4f55\u4f7f\u7528\u548c\u542f\u52a8\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#_1","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d from transformers import pipeline , set_seed generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) \u5c31\u662f\u8fd9\u6837\uff01 \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e0b\uff1a predict ( \"You look amazing today,\" ) \u7ed3\u679c\uff1a [{'generated_text': 'You look amazing today, guys. If you\\'re still in school and you still have a job where you work in the field\u2026 you\\'re going to look ridiculous by now, you\\'re going to look really ridiculous.\"\\n\\nHe turned to his friends'}, {'generated_text': 'You look amazing today, aren\\'t you?\"\\n\\nHe turned and looked at me. He had an expression that was full of worry as he looked at me. Even before he told me I\\'d have sex, he gave up after I told him'}, {'generated_text': 'You look amazing today, and look amazing in the sunset.\"\\n\\nGarry, then 33, won the London Marathon at age 15, and the World Triathlon in 2007, the two youngest Olympians to ride 100-meters. He also'}] \u8ba9\u6211\u4eec\u770b\u770b\u7b2c\u4e00\u4e2a\u7ed3\u679c\u3002 You look amazing today, guys. If you're still in school and you still have a job where you work in the field\u2026 you're going to look ridiculous by now, you're going to look really ridiculous.\" He turned to his friends \ud83e\udd23 \u8fd9\u5c31\u662f\u6211\u4eec\u8981\u627e\u7684\u4e1c\u897f\uff01\u5982\u679c\u518d\u6b21\u8fd0\u884c\u9884\u6d4b\uff0c\u6bcf\u6b21\u90fd\u4f1a\u7ed9\u51fa\u4e0d\u540c\u7684\u7ed3\u679c\u3002","title":"\u5982\u4f55\u4f7f\u7528"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#_2","text":"","title":"\u5982\u4f55\u90e8\u7f72"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#pinferencia","text":"$ pip install \"pinferencia[streamlit]\" ---> 100%","title":"\u5b89\u88c5Pinferencia"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#_3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from transformers import pipeline , set_seed from pinferencia import Server , task generator = pipeline ( \"text-generation\" , model = \"gpt2\" ) set_seed ( 42 ) def predict ( text : str ) -> list : return generator ( text , max_length = 50 , num_return_sequences = 3 ) service = Server () service . register ( model_name = \"gpt2\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, )","title":"\u521b\u5efa\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#_4","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/text-generation/#_5","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://127.0.0.1:8000/v1/models/gpt2/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"string\", \"parameters\": {}, \"data\": \"You look amazing today,\" }' \u7ed3\u679c: { \"id\" : \"string\" , \"model_name\" : \"gpt2\" , \"data\" : [ { \"generated_text\" : \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\" : \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\" : \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : \"You look amazing today,\" }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [ { \"generated_text\": \"You look amazing today, I was in front of my friends. I wanted everyone to see me. But that's all. No one really cares about me in the eyes of the whole world unless I love them.\\\"\\n\\nIn a second Facebook post\" }, { \"generated_text\": \"You look amazing today, and I know I am going to get the job done! So thank you all for all those donations, money, help, and hugs. I hope to see you again soon.\" }, { \"generated_text\": \"You look amazing today, but I will have to wait until early June for what will go down as the first NBA championship (a thing I had been expecting). If it's not the biggest, it is also not great. Now let's look at\" } ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/","text":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801 \u00b6 \u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f \u5b89\u88c5\u4f9d\u8d56 \u00b6 HuggingFace \u00b6 pip install \"transformers[torch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002 Pinferencia \u00b6 pip install \"pinferencia[streamlit]\" \u5b9a\u4e49\u670d\u52a1 \u00b6 \u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION }) \u542f\u52a8\u670d\u52a1 \u00b6 Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Translation \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [\"Guten Morgen, liebe Liebe.\"] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u7ffb\u8bd1 - Google T5"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#google-t5-7","text":"\u4ec0\u4e48\u662fT5\uff1f Google \u7684 Text-To-Text Transfer Transformer (T5) \u63d0\u4f9b\u4e86\u7ffb\u8bd1\u529f\u80fd\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 Google T5 \u6a21\u578b\u90e8\u7f72\u4e3a REST API \u670d\u52a1\u3002 \u96be\u7684\uff1f \u6211\u544a\u8bc9\u4f60\u600e\u4e48\u6837\uff1a\u4f60\u53ea\u9700\u8981\u5199 7 \u884c\u4ee3\u7801\uff1f","title":"Google T5 \u7ffb\u8bd1\u5373\u670d\u52a1\uff0c\u53ea\u9700 7 \u884c\u4ee3\u7801"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#_1","text":"","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#huggingface","text":"pip install \"transformers[torch]\" \u5982\u679c\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bf7\u8bbf\u95ee Installation \u5e76\u67e5\u770b\u5176\u5b98\u65b9\u6587\u6863\u3002","title":"HuggingFace"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#pinferencia","text":"pip install \"pinferencia[streamlit]\"","title":"Pinferencia"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#_2","text":"\u9996\u5148\u8ba9\u6211\u4eec\u521b\u5efa app.py \u6765\u5b9a\u4e49\u670d\u52a1\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server , task t5 = pipeline ( model = \"t5-base\" , tokenizer = \"t5-base\" ) def translate ( text : list ) -> list : return [ res [ \"translation_text\" ] for res in t5 ( text )] service = Server () service . register ( model_name = \"t5\" , model = translate , metadata = { \"task\" : task . TRANSLATION })","title":"\u5b9a\u4e49\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#_3","text":"Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u542f\u52a8\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/nlp/translation/#_4","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Translation \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 curl -X 'POST' \\ 'http://localhost:8000/v1/models/t5/predict' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"parameters\": {}, \"data\": [\"translate English to German: Good morning, my love.\"] }' \u7ed3\u679c: { \"model_name\" : \"t5\" , \"data\" : [ \"translation_text\" : \"Guten Morgen, liebe Liebe.\" ] } test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/gpt2/predict\" , json = { \"data\" : [ \"translate English to German: Good morning, my love.\" ] }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u6253\u5370\u7ed3\u679c\uff1a Prediction: [\"Guten Morgen, liebe Liebe.\"] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528 Hugging Face \u7ba1\u9053\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528 Pinferencia \u4f5c\u4e3a REST API \u90e8\u7f72\u5b83\u3002 \u5148\u51b3\u6761\u4ef6 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 \u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b \u00b6 \u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u5982\u6b64\u7b80\u5355\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a \u90e8\u7f72\u6a21\u578b \u00b6 \u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f \u9884\u6d4b \u00b6 Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01 \u8fdb\u4e00\u6b65\u6539\u8fdb \u00b6 \u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, ) \u518d\u6b21\u9884\u6d4b \u00b6 UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f \u56fe\u7247\u5206\u7c7b \u3002 curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_2","text":"\u6a21\u578b\u5c06\u81ea\u52a8\u4e0b\u8f7d\u3002 1 2 3 4 5 6 from transformers import pipeline vision_classifier = pipeline ( task = \"image-classification\" ) vision_classifier ( images = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) \u7ed3\u679c: [{ 'label' : 'lynx, catamount' , 'score' : 0.4403027892112732 }, { 'label' : 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor' , 'score' : 0.03433405980467796 }, { 'label' : 'snow leopard, ounce, Panthera uncia' , 'score' : 0.032148055732250214 }, { 'label' : 'Egyptian cat' , 'score' : 0.02353910356760025 }, { 'label' : 'tiger cat' , 'score' : 0.023034192621707916 }] \u5982\u6b64\u7b80\u5355\uff01 \u73b0\u5728\u8ba9\u6211\u4eec\u8bd5\u8bd5\uff1a","title":"\u4e0b\u8f7d\u6a21\u578b\u5e76\u9884\u6d4b"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_3","text":"\u6ca1\u6709\u90e8\u7f72\uff0c\u673a\u5668\u5b66\u4e60\u6559\u7a0b\u600e\u4e48\u53ef\u80fd\u5b8c\u6574\uff1f \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u73b0\u5728\u8ba9\u6211\u4eec\u7528\u4ee3\u7801\u521b\u5efa\u4e00\u4e2a app.py \u6587\u4ef6\uff1a app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( data : str ) -> list : return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u5bb9\u6613\uff0c\u5bf9\u5427\uff1f","title":"\u90e8\u7f72\u6a21\u578b"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_4","text":"Curl Python requests curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" }' \u7ed3\u679c: Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \uff0c\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0f ui\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u9884\u6d4b"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_5","text":"\u4f46\u662f\uff0c\u6709\u65f6\u4f7f\u7528\u56fe\u50cf\u7684 url \u6765\u9884\u6d4b\u662f\u4e0d\u5408\u9002\u7684\u3002 \u8ba9\u6211\u4eec\u7a0d\u5fae\u4fee\u6539 app.py \u4ee5\u63a5\u53d7 Base64 Encoded String \u4f5c\u4e3a\u8f93\u5165\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import base64 from io import BytesIO from PIL import Image from transformers import pipeline from pinferencia import Server , task vision_classifier = pipeline ( task = \"image-classification\" ) def classify ( images : list ) -> list : \"\"\"Image Classification Args: images (list): list of base64 encoded image strings Returns: list: list of classification results \"\"\" input_images = [ Image . open ( BytesIO ( base64 . b64decode ( img ))) for img in images ] return vision_classifier ( images = input_images ) service = Server () service . register ( model_name = \"vision\" , model = classify , metadata = { \"task\" : task . IMAGE_CLASSIFICATION }, )","title":"\u8fdb\u4e00\u6b65\u6539\u8fdb"},{"location":"rc/how-to-guides/huggingface/pipeline/vision/#_6","text":"UI Curl Python requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u4f1a\u81ea\u52a8\u9009\u62e9\u6a21\u677f \u56fe\u7247\u5206\u7c7b \u3002 curl --location --request POST 'http://127.0.0.1:8000/v1/models/vision/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"...\" }' \u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ] test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vision/predict\" , json = { \"data\" : \"...\" # noqa }, ) print ( \"Prediction:\" , response . json ()[ \"data\" ]) \u8fd0\u884c python test.py \u5e76\u67e5\u770b\u7ed3\u679c\uff1a Prediction: [ {'score': 0.433499813079834, 'label': 'lynx, catamount'}, {'score': 0.03479616343975067, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.032401904463768005, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.023944756016135216, 'label': 'Egyptian cat'}, {'score': 0.022889181971549988, 'label': 'tiger cat'} ]","title":"\u518d\u6b21\u9884\u6d4b"},{"location":"rc/how-to-guides/paddlepaddle/dependencies/","text":"\u5b89\u88c5\u4f9d\u8d56 \u00b6 # \u5b89\u88c5gpu\u7248\u672c\u7684PaddlePaddle pip install paddlepaddle-gpu -U # \u6216\u8005\u5b89\u88c5cpu\u7248\u672c\u7684paddlepaddle pip install paddlepaddle -U pip install paddlehub \u63d0\u793a \u9664\u4e0a\u8ff0\u4f9d\u8d56\u5916\uff0cPaddleHub\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9884\u7f6e\u6570\u636e\u96c6\u9700\u8981\u8fde\u63a5\u670d\u52a1\u7aef\u8fdb\u884c\u4e0b\u8f7d\uff0c\u8bf7\u786e\u4fdd\u673a\u5668\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u3002\u82e5\u672c\u5730\u5df2\u5b58\u5728\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u53ef\u4ee5\u79bb\u7ebf\u8fd0\u884cPaddleHub\u3002","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/dependencies/#_1","text":"# \u5b89\u88c5gpu\u7248\u672c\u7684PaddlePaddle pip install paddlepaddle-gpu -U # \u6216\u8005\u5b89\u88c5cpu\u7248\u672c\u7684paddlepaddle pip install paddlepaddle -U pip install paddlehub \u63d0\u793a \u9664\u4e0a\u8ff0\u4f9d\u8d56\u5916\uff0cPaddleHub\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9884\u7f6e\u6570\u636e\u96c6\u9700\u8981\u8fde\u63a5\u670d\u52a1\u7aef\u8fdb\u884c\u4e0b\u8f7d\uff0c\u8bf7\u786e\u4fdd\u673a\u5668\u53ef\u4ee5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u3002\u82e5\u672c\u5730\u5df2\u5b58\u5728\u76f8\u5173\u7684\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u53ef\u4ee5\u79bb\u7ebf\u8fd0\u884cPaddleHub\u3002","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 PyramidBox-Lite \u662f\u57fa\u4e8e 2018 \u5e74\u767e\u5ea6\u53d1\u8868\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u9876\u7ea7\u4f1a\u8bae ECCV 2018 \u7684\u8bba\u6587 PyramidBox \u800c\u7814\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6a21\u578b\u57fa\u4e8e\u4e3b\u5e72\u7f51\u7edc FaceBoxes\uff0c\u5bf9\u4e8e\u5149\u7167\u3001\u53e3\u7f69\u906e\u6321\u3001\u8868\u60c5\u53d8\u5316\u3001\u5c3a\u5ea6\u53d8\u5316\u7b49\u5e38\u89c1\u95ee\u9898\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5 PaddleHub Module \u57fa\u4e8e WIDER FACE \u6570\u636e\u96c6\u548c\u767e\u5ea6\u81ea\u91c7\u4eba\u8138\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u9884\u6d4b\uff0c\u53ef\u7528\u4e8e\u4eba\u8138\u68c0\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) ![\u4eba\u8138](/assets/images/examples/paddle/face.jpg){ width=\"300\" } \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001pyramidbox_lite_server \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install pyramidbox_lite_server \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' \u54cd\u5e94 { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"\u4eba\u8138\u68c0\u6d4b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#_1","text":"PyramidBox-Lite \u662f\u57fa\u4e8e 2018 \u5e74\u767e\u5ea6\u53d1\u8868\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u9876\u7ea7\u4f1a\u8bae ECCV 2018 \u7684\u8bba\u6587 PyramidBox \u800c\u7814\u53d1\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6a21\u578b\u57fa\u4e8e\u4e3b\u5e72\u7f51\u7edc FaceBoxes\uff0c\u5bf9\u4e8e\u5149\u7167\u3001\u53e3\u7f69\u906e\u6321\u3001\u8868\u60c5\u53d8\u5316\u3001\u5c3a\u5ea6\u53d8\u5316\u7b49\u5e38\u89c1\u95ee\u9898\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5 PaddleHub Module \u57fa\u4e8e WIDER FACE \u6570\u636e\u96c6\u548c\u767e\u5ea6\u81ea\u91c7\u4eba\u8138\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u9884\u6d4b\uff0c\u53ef\u7528\u4e8e\u4eba\u8138\u68c0\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/tree/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) ![\u4eba\u8138](/assets/images/examples/paddle/face.jpg){ width=\"300\" } \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#2pyramidbox_lite_server","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001pyramidbox_lite_server \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#3","text":"hub install pyramidbox_lite_server","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server face_detector = hub . Module ( name = \"pyramidbox_lite_server\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ): return face_detector . face_detection ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"face_detector\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/face_detection/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/face_detector/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"{base64 encoded image}\" }' \u54cd\u5e94 { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/face_detector/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"{base64 encoded image}\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"face_detector\", \"model_version\": \"default\", \"data\": [ { \"data\": [ { \"confidence\": 0.9984221458435059, \"left\": 519, \"top\": 447, \"right\": 755, \"bottom\": 750 } ], \"path\": \"ndarray_time=1655802174713885.0\" } ] }","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 MobileNet V2 \u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728 MobileNet \u7684\u57fa\u7840\u4e0a\uff0c\u505a\u4e86 Inverted Residuals \u548c Linear bottlenecks \u8fd9\u4e24\u5927\u6539\u8fdb\u3002\u8be5 PaddleHub Module \u662f\u5728\u767e\u5ea6\u81ea\u5efa\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5f53\u524d\u5df2\u652f\u6301 7978 \u79cd\u52a8\u7269\u7684\u5206\u7c7b\u8bc6\u522b\u3002\u6a21\u578b\u7684\u8be6\u60c5\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u677e\u9f20 \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install pyramidbox_lite_server \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' \u54cd\u5e94 { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u56fe\u50cf\u8bc6\u522b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#_1","text":"MobileNet V2 \u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728 MobileNet \u7684\u57fa\u7840\u4e0a\uff0c\u505a\u4e86 Inverted Residuals \u548c Linear bottlenecks \u8fd9\u4e24\u5927\u6539\u8fdb\u3002\u8be5 PaddleHub Module \u662f\u5728\u767e\u5ea6\u81ea\u5efa\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f97\u5230\u7684\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5f53\u524d\u5df2\u652f\u6301 7978 \u79cd\u52a8\u7269\u7684\u5206\u7c7b\u8bc6\u522b\u3002\u6a21\u578b\u7684\u8be6\u60c5\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/face_detection/pyramidbox_lite_server","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u677e\u9f20 \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#2mobilenet_v2_animals","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#3","text":"hub install pyramidbox_lite_server","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 import cv2 import numpy as np import paddlehub as hub from pinferencia import Server , task classifier = hub . Module ( name = \"mobilenet_v2_animals\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( data : list ) -> list : images = [ base64_str_to_cv2 ( base64_img_str ) for base64_img_str in data ] return classifier . classification ( images = images ) service = Server () service . register ( model_name = \"classifier\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_classification/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/classifier/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{\"data\": [\"/9j/4AAQS........\"]}' \u54cd\u5e94 { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/classifier/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : [ \"/9j/4AAQS........\" ]}, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"classifier\", \"data\": [ { \"\u677e\u9f20\": 0.9506056308746338 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u672c\u6a21\u578b\u5c01\u88c5\u81ea \u5c0f\u89c6\u79d1\u6280 photo2cartoon \u9879\u76ee\u7684 paddlepaddle \u7248\u672c \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon \u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install Photo2Cartoon \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efa app.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' \u54cd\u5e94 { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u56fe\u50cf\u751f\u6210"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#_1","text":"\u672c\u6a21\u578b\u5c01\u88c5\u81ea \u5c0f\u89c6\u79d1\u6280 photo2cartoon \u9879\u76ee\u7684 paddlepaddle \u7248\u672c \u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/Image_gan/style_transfer/Photo2Cartoon","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#_2","text":"\u8f93\u5165 \u8f93\u51fa \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#2mobilenet_v2_animals","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#3","text":"hub install Photo2Cartoon","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import base64 from io import BytesIO import paddlehub as hub from PIL import Image from pinferencia import Server , task from pinferencia.tools import base64_str_to_cv2 image_generation = hub . Module ( name = \"Photo2Cartoon\" ) def predict ( base64_img_str : str ) -> str : result = image_generation . Cartoon_GEN ( images = [ base64_str_to_cv2 ( base64_img_str )], visualization = True , output_dir = \"./\" ) pil_img = Image . fromarray ( result [ 0 ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"image_generation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efa app.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/image_generation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/image_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"base64 image string\" }' \u54cd\u5e94 { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/image_generation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"base64 image string\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"image_generation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u8f66\u8f86\u68c0\u6d4b\u662f\u57ce\u5e02\u4ea4\u901a\u76d1\u63a7\u4e2d\u975e\u5e38\u91cd\u8981\u5e76\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u7684\u96be\u5ea6\u5728\u4e8e\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u76f8\u5bf9\u8f83\u5c0f\u7684\u8f66\u8f86\u8fdb\u884c\u7cbe\u51c6\u5730\u5b9a\u4f4d\u548c\u5206\u7c7b\u3002\u8be5 PaddleHub Module \u7684\u7f51\u7edc\u4e3a YOLOv3, \u5176\u4e2d backbone \u4e3a DarkNet53\uff0c\u91c7\u7528\u767e\u5ea6\u81ea\u5efa\u5927\u89c4\u6a21\u8f66\u8f86\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff0c\u652f\u6301 car (\u6c7d\u8f66)\u3001truck (\u5361\u8f66)\u3001bus (\u516c\u4ea4\u8f66)\u3001motorbike (\u6469\u6258\u8f66)\u3001tricycle (\u4e09\u8f6e\u8f66)\u7b49\u8f66\u578b\u7684\u8bc6\u522b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001yolov3_darknet53_vehicles \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install yolov3_darknet53_vehicles \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' \u54cd\u5e94 { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"Index"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#_1","text":"\u8f66\u8f86\u68c0\u6d4b\u662f\u57ce\u5e02\u4ea4\u901a\u76d1\u63a7\u4e2d\u975e\u5e38\u91cd\u8981\u5e76\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u7684\u96be\u5ea6\u5728\u4e8e\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u76f8\u5bf9\u8f83\u5c0f\u7684\u8f66\u8f86\u8fdb\u884c\u7cbe\u51c6\u5730\u5b9a\u4f4d\u548c\u5206\u7c7b\u3002\u8be5 PaddleHub Module \u7684\u7f51\u7edc\u4e3a YOLOv3, \u5176\u4e2d backbone \u4e3a DarkNet53\uff0c\u91c7\u7528\u767e\u5ea6\u81ea\u5efa\u5927\u89c4\u6a21\u8f66\u8f86\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff0c\u652f\u6301 car (\u6c7d\u8f66)\u3001truck (\u5361\u8f66)\u3001bus (\u516c\u4ea4\u8f66)\u3001motorbike (\u6469\u6258\u8f66)\u3001tricycle (\u4e09\u8f6e\u8f66)\u7b49\u8f66\u578b\u7684\u8bc6\u522b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/object_detection/yolov3_darknet53_vehicles","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#2yolov3_darknet53_vehicles","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0","title":"2\u3001yolov3_darknet53_vehicles \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#3","text":"hub install yolov3_darknet53_vehicles","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server import paddlehub as hub import cv2 vehicle_detection = hub . Module ( name = \"yolov3_darknet53_vehicles\" ) def predict ( path : str ): return vehicle_detection . object_detection ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"vehicle_detection\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/object_detection/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/vehicle_detection/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"car.jpg\" }' \u54cd\u5e94 { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/vehicle_detection/predict\" , json = { \"data\" : \"car.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"vehicle_detection\", \"data\": [ { \"data\": [ { \"label\": \"car\", \"confidence\": 0.9332570433616638, \"left\": 832.1240234375, \"top\": 1694.6256103515625, \"right\": 1209.645263671875, \"bottom\": 1972.4195556640625 }, { \"label\": \"car\", \"confidence\": 0.8977782130241394, \"left\": 1476.706787109375, \"top\": 1803.521240234375, \"right\": 1796.732177734375, \"bottom\": 2107.582275390625 }, { \"label\": \"car\", \"confidence\": 0.849329948425293, \"left\": 1319.199462890625, \"top\": 1679.5538330078125, \"right\": 1513.8466796875, \"bottom\": 1851.3421630859375 }, { \"label\": \"car\", \"confidence\": 0.8382290005683899, \"left\": 1665.3941650390625, \"top\": 1754.3929443359375, \"right\": 2237.92138671875, \"bottom\": 2323.58642578125 }, { \"label\": \"car\", \"confidence\": 0.8308005332946777, \"left\": 2576.8466796875, \"top\": 1775.929931640625, \"right\": 4473.15087890625, \"bottom\": 3095.475830078125 }, { \"label\": \"car\", \"confidence\": 0.6374166011810303, \"left\": 2269.047119140625, \"top\": 1852.68994140625, \"right\": 3090.314208984375, \"bottom\": 2686.0478515625 }, { \"label\": \"car\", \"confidence\": 0.5584644079208374, \"left\": 1963.8443603515625, \"top\": 1830.8948974609375, \"right\": 2598.80126953125, \"bottom\": 2392.88818359375 }, { \"label\": \"car\", \"confidence\": 0.28342998027801514, \"left\": 1141.4927978515625, \"top\": 1578.66015625, \"right\": 1272.1849365234375, \"bottom\": 1657.116455078125 }, { \"label\": \"car\", \"confidence\": 0.23879402875900269, \"left\": 1186.671142578125, \"top\": 1590.04052734375, \"right\": 1316.980712890625, \"bottom\": 1683.19970703125 }, { \"label\": \"carplate\", \"confidence\": 0.9311351776123047, \"left\": 3628.1376953125, \"top\": 2832.224853515625, \"right\": 3963.41162109375, \"bottom\": 2924.886962890625 }, { \"label\": \"carplate\", \"confidence\": 0.3726407289505005, \"left\": 1056.91015625, \"top\": 1856.930908203125, \"right\": 1110.511962890625, \"bottom\": 1878.08935546875 } ], \"save_path\": \"./image_numpy_0.jpg\" } ] }","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u57fa\u4e8e ExtremeC3 \u6a21\u578b\u5b9e\u73b0\u7684\u8f7b\u91cf\u5316\u4eba\u50cf\u5206\u5272\u6a21\u578b, \u66f4\u591a\u8be6\u60c5\u8bf7\u53c2\u8003\uff1a ExtremeC3_Portrait_Segmentation \u9879\u76ee\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation \u6837\u4f8b\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001ExtremeC3_Portrait_Segmentation \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install ExtremeC3_Portrait_Segmentation \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' \u54cd\u5e94 { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u8bed\u4e49\u5206\u5272"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_1","text":"\u57fa\u4e8e ExtremeC3 \u6a21\u578b\u5b9e\u73b0\u7684\u8f7b\u91cf\u5316\u4eba\u50cf\u5206\u5272\u6a21\u578b, \u66f4\u591a\u8be6\u60c5\u8bf7\u53c2\u8003\uff1a ExtremeC3_Portrait_Segmentation \u9879\u76ee\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/semantic_segmentation/ExtremeC3_Portrait_Segmentation","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_2","text":"\u8f93\u5165 \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u4f8b\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#2extremec3_portrait_segmentation","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0","title":"2\u3001ExtremeC3_Portrait_Segmentation \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#3","text":"hub install ExtremeC3_Portrait_Segmentation","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import base64 from io import BytesIO import cv2 import numpy as np import paddlehub as hub from PIL import Image from pinferencia import Server , task semantic_segmentation = hub . Module ( name = \"ExtremeC3_Portrait_Segmentation\" ) def base64_str_to_cv2 ( base64_str : str ) -> np . ndarray : return cv2 . imdecode ( np . fromstring ( base64 . b64decode ( base64_str ), np . uint8 ), cv2 . IMREAD_COLOR ) def predict ( base64_img_str : str ) -> str : images = [ base64_str_to_cv2 ( base64_img_str )] result = semantic_segmentation . Segmentation ( images = images , output_dir = \"./\" , visualization = True , ) pil_img = Image . fromarray ( result [ 0 ][ \"result\" ]) buff = BytesIO () pil_img . save ( buff , format = \"JPEG\" ) return base64 . b64encode ( buff . getvalue ()) . decode ( \"utf-8\" ) service = Server () service . register ( model_name = \"semantic_segmentation\" , model = predict , metadata = { \"task\" : task . IMAGE_TO_IMAGE }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/semantic_segmentation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Url Image To Image \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/semantic_segmentation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"/9j/4AAQSkZJRgABAQEA/...\" }' \u54cd\u5e94 { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/semantic_segmentation/predict\" , headers = { \"Content-type\" : \"application/json\" }, json = { \"data\" : \"/9j/4AAQSkZJRgABAQEA/...\" }, ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"semantic_segmentation\", \"model_version\": \"default\", \"data\": \"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRo...\" } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8000 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u5b8c\u6574\u7684 API \u6587\u6863\u3002 \u60a8\u751a\u81f3\u4e5f\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 chinese_ocr_db_crnn_mobile Module \u7528\u4e8e\u8bc6\u522b\u56fe\u7247\u5f53\u4e2d\u7684\u6c49\u5b57, \u8bc6\u522b\u6587\u672c\u6846\u4e2d\u7684\u4e2d\u6587\u6587\u5b57,\u518d\u5bf9\u68c0\u6d4b\u6587\u672c\u6846\u8fdb\u884c\u89d2\u5ea6\u5206\u7c7b\u3002\u6700\u7ec8\u8bc6\u522b\u6587\u5b57\u7b97\u6cd5\u91c7\u7528 CRNN\uff08Convolutional Recurrent Neural Network\uff09\u5373\u5377\u79ef\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3002\u8be5 Module \u662f\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587 OCR \u6a21\u578b\uff0c\u652f\u6301\u76f4\u63a5\u9884\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile \u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001mobilenet_v2_animals \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 pip3 install shapely pyclipper 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install chinese_ocr_db_crnn_mobile \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) # mkldnn\u52a0\u901f\u4ec5\u5728CPU\u4e0b\u6709\u6548 def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 \u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' \u54cd\u5e94 {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"Index"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#_1","text":"chinese_ocr_db_crnn_mobile Module \u7528\u4e8e\u8bc6\u522b\u56fe\u7247\u5f53\u4e2d\u7684\u6c49\u5b57, \u8bc6\u522b\u6587\u672c\u6846\u4e2d\u7684\u4e2d\u6587\u6587\u5b57,\u518d\u5bf9\u68c0\u6d4b\u6587\u672c\u6846\u8fdb\u884c\u89d2\u5ea6\u5206\u7c7b\u3002\u6700\u7ec8\u8bc6\u522b\u6587\u5b57\u7b97\u6cd5\u91c7\u7528 CRNN\uff08Convolutional Recurrent Neural Network\uff09\u5373\u5377\u79ef\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3002\u8be5 Module \u662f\u4e00\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587 OCR \u6a21\u578b\uff0c\u652f\u6301\u76f4\u63a5\u9884\u6d4b\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/image/text_recognition/chinese_ocr_db_crnn_mobile","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u56fe\u7247\u6765\u6e90 ( https://www.pexels.com ) \u8f93\u51fa \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6a21\u578b\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#2mobilenet_v2_animals","text":"paddlepaddle >= 1.6.2 paddlehub >= 1.6.0 pip3 install shapely pyclipper","title":"2\u3001mobilenet_v2_animals \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#3","text":"hub install chinese_ocr_db_crnn_mobile","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server import paddlehub as hub import cv2 ocr = hub . Module ( name = \"chinese_ocr_db_crnn_mobile\" , enable_mkldnn = True ) # mkldnn\u52a0\u901f\u4ec5\u5728CPU\u4e0b\u6709\u6548 def predict ( path : str ): return ocr . recognize_text ( images = [ cv2 . imread ( path )], visualization = True , output_dir = \"./\" ) service = Server () service . register ( model_name = \"ocr\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/image/text_recognition/#_5","text":"\u63d0\u793a \u56fe\u7247\u5b58\u5728\u4e8e service \u673a\u5668\u4e0a\uff0c\u53ef\u8f93\u5165\u5bf9\u4e8e service \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u8005\u662f\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/ocr/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"test.jpg\" }' \u54cd\u5e94 {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]} \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/ocr/predict\" , json = { \"data\" : \"test.jpg\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py {'model_name': 'text_gencognition', 'model_version': 'default', 'data': [{'save_path': './ndarray_1655277391.4650576.jpg', 'data': [{'text': 'photo', 'confidence': 0.9524916410446167, 'text_box_position': [[1145, 1913], [1243, 1913], [1243, 1934], [1145, 1934]]}, {'text': 'AARON TUAN', 'confidence': 0.9474555850028992, 'text_box_position': [[1236, 1909], [1424, 1909], [1424, 1937], [1236, 1937]]}, {'text': '#makeup ANNA LE', 'confidence': 0.8719193339347839, 'text_box_position': [[1168, 1934], [1424, 1930], [1424, 1960], [1168, 1964]]}, {'text': '#ekip MT RYDER', 'confidence': 0.9155644178390503, 'text_box_position': [[1193, 1962], [1421, 1962], [1421, 1984], [1193, 1984]]}]}]}","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\uff08Emotion Detection\uff0c\u7b80\u79f0 EmoTect\uff09\u4e13\u6ce8\u4e8e\u8bc6\u522b\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7528\u6237\u7684\u60c5\u7eea\uff0c\u9488\u5bf9\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u7528\u6237\u6587\u672c\uff0c\u81ea\u52a8\u5224\u65ad\u8be5\u6587\u672c\u7684\u60c5\u7eea\u7c7b\u522b\u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u7f6e\u4fe1\u5ea6\uff0c\u60c5\u7eea\u7c7b\u578b\u5206\u4e3a\u79ef\u6781\u3001\u6d88\u6781\u3001\u4e2d\u6027\u3002\u8be5\u6a21\u578b\u57fa\u4e8eTextCNN\uff08\u591a\u5377\u79ef\u6838 CNN \u6a21\u578b\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u5c40\u90e8\u76f8\u5173\u6027\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001emotion_detection_textcnn \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install emotion_detection_textcnn \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efa app.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Raw Request \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' \u54cd\u5e94 { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u60c5\u7eea\u5206\u6790"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_1","text":"\u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\uff08Emotion Detection\uff0c\u7b80\u79f0 EmoTect\uff09\u4e13\u6ce8\u4e8e\u8bc6\u522b\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7528\u6237\u7684\u60c5\u7eea\uff0c\u9488\u5bf9\u667a\u80fd\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u7528\u6237\u6587\u672c\uff0c\u81ea\u52a8\u5224\u65ad\u8be5\u6587\u672c\u7684\u60c5\u7eea\u7c7b\u522b\u5e76\u7ed9\u51fa\u76f8\u5e94\u7684\u7f6e\u4fe1\u5ea6\uff0c\u60c5\u7eea\u7c7b\u578b\u5206\u4e3a\u79ef\u6781\u3001\u6d88\u6781\u3001\u4e2d\u6027\u3002\u8be5\u6a21\u578b\u57fa\u4e8eTextCNN\uff08\u591a\u5377\u79ef\u6838 CNN \u6a21\u578b\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u5c40\u90e8\u76f8\u5173\u6027\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/sentiment_analysis/emotion_detection_textcnn","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_2","text":"\u8f93\u5165 \u8f93\u51fa [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#2emotion_detection_textcnn","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001emotion_detection_textcnn \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#3","text":"hub install emotion_detection_textcnn","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task emotion_detection_textcnn = hub . Module ( name = \"emotion_detection_textcnn\" ) def predict ( text : list ) -> list : return emotion_detection_textcnn . emotion_classify ( texts = text ) service = Server () service . register ( model_name = \"emotion_detection_textcnn\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efa app.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/emotion_detection_textcnn/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Raw Request \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/emotion_detection_textcnn/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"\u522b\u6765\u5435\u6211\"] }' \u54cd\u5e94 { \"model_name\" : \"emotion_detection_textcnn\" , \"data\" : [ { \"text\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"emotion_label\" : 2 , \"emotion_key\" : \"positive\" , \"positive_probs\" : 0.9267 , \"negative_probs\" : 0.0019 , \"neutral_probs\" : 0.0714 }, { \"text\" : \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"emotion_label\" : 1 , \"emotion_key\" : \"neutral\" , \"positive_probs\" : 0.0062 , \"negative_probs\" : 0.0042 , \"neutral_probs\" : 0.9896 }, { \"text\" : \"\u522b\u6765\u5435\u6211\" , \"emotion_label\" : 0 , \"emotion_key\" : \"negative\" , \"positive_probs\" : 0.0732 , \"negative_probs\" : 0.7791 , \"neutral_probs\" : 0.1477 } ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/emotion_detection_textcnn/predict\" , json = { \"data\" : [ \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" , \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\" , \"\u522b\u6765\u5435\u6211\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"emotion_detection_textcnn\", \"data\": [ { \"text\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\", \"emotion_label\": 2, \"emotion_key\": \"positive\", \"positive_probs\": 0.9267, \"negative_probs\": 0.0019, \"neutral_probs\": 0.0714 }, { \"text\": \"\u6e7f\u7eb8\u5dfe\u662f\u5e72\u5783\u573e\", \"emotion_label\": 1, \"emotion_key\": \"neutral\", \"positive_probs\": 0.0062, \"negative_probs\": 0.0042, \"neutral_probs\": 0.9896 }, { \"text\": \"\u522b\u6765\u5435\u6211\", \"emotion_label\": 0, \"emotion_key\": \"negative\", \"positive_probs\": 0.0732, \"negative_probs\": 0.7791, \"neutral_probs\": 0.1477 } ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u8be5 Module \u662f jieba \u4f7f\u7528 PaddlePaddle \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u642d\u5efa\u7684\u5207\u8bcd\u7f51\u7edc\uff08\u53cc\u5411 GRU\uff09\u3002\u540c\u65f6\u4e5f\u652f\u6301 jieba \u7684\u4f20\u7edf\u5207\u8bcd\u65b9\u6cd5\uff0c\u5982\u7cbe\u786e\u6a21\u5f0f\u3001\u5168\u6a21\u5f0f\u3001\u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u7b49\u5207\u8bcd\u6a21\u5f0f\uff0c\u4f7f\u7528\u65b9\u6cd5\u548c jieba \u4fdd\u6301\u4e00\u81f4\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001jieba_paddle \u4f9d\u8d56 \u00b6 paddlepaddle >= 1.8.0 paddlehub >= 1.8.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install jieba_paddle \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' \u54cd\u5e94 { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u8bed\u6cd5\u5206\u6790"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_1","text":"\u8be5 Module \u662f jieba \u4f7f\u7528 PaddlePaddle \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u642d\u5efa\u7684\u5207\u8bcd\u7f51\u7edc\uff08\u53cc\u5411 GRU\uff09\u3002\u540c\u65f6\u4e5f\u652f\u6301 jieba \u7684\u4f20\u7edf\u5207\u8bcd\u65b9\u6cd5\uff0c\u5982\u7cbe\u786e\u6a21\u5f0f\u3001\u5168\u6a21\u5f0f\u3001\u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u7b49\u5207\u8bcd\u6a21\u5f0f\uff0c\u4f7f\u7528\u65b9\u6cd5\u548c jieba \u4fdd\u6301\u4e00\u81f4\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/lexical_analysis/jieba_paddle","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_2","text":"\u8f93\u5165 \u8f93\u51fa \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" [ \"\u4eca\u5929\" , \"\u5929\u6c14\" , \"\u771f\u597d\" ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#2jieba_paddle","text":"paddlepaddle >= 1.8.0 paddlehub >= 1.8.0","title":"2\u3001jieba_paddle \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#3","text":"hub install jieba_paddle","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import paddlehub as hub from pinferencia import Server , task lexical_analysis = hub . Module ( name = \"jieba_paddle\" ) def predict ( text : str ): return lexical_analysis . cut ( text , cut_all = False , HMM = True ) service = Server () service . register ( model_name = \"lexical_analysis\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT } ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/lexical_analysis/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f Text to Text \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/lexical_analysis/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" }' \u54cd\u5e94 { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/lexical_analysis/predict\" , json = { \"data\" : \"\u4eca\u5929\u5929\u6c14\u771f\u597d\" } ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"lexical_analysis\", \"data\": [ \"\u4eca\u5929\", \"\u5929\u6c14\", \"\u771f\u597d\" ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 \u540c\u58f0\u4f20\u8bd1\uff08Simultaneous Translation\uff09\uff0c\u5373\u5728\u53e5\u5b50\u5b8c\u6210\u4e4b\u524d\u8fdb\u884c\u7ffb\u8bd1\uff0c\u540c\u58f0\u4f20\u8bd1\u7684\u76ee\u6807\u662f\u5b9e\u73b0\u540c\u58f0\u4f20\u8bd1\u7684\u81ea\u52a8\u5316\uff0c\u5b83\u53ef\u4ee5\u4e0e\u6e90\u8bed\u8a00\u540c\u65f6\u7ffb\u8bd1\uff0c\u5ef6\u8fdf\u65f6\u95f4\u53ea\u6709\u51e0\u79d2\u949f\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1 \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001transformer_nist_wait_1 \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.1.0 paddlehub >= 2.1.0 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install transformer_nist_wait_1 \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) \u6d4b\u8bd5\u670d\u52a1 \u00b6 curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' \u54cd\u5e94 \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py # \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"Index"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_1","text":"\u540c\u58f0\u4f20\u8bd1\uff08Simultaneous Translation\uff09\uff0c\u5373\u5728\u53e5\u5b50\u5b8c\u6210\u4e4b\u524d\u8fdb\u884c\u7ffb\u8bd1\uff0c\u540c\u58f0\u4f20\u8bd1\u7684\u76ee\u6807\u662f\u5b9e\u73b0\u540c\u58f0\u4f20\u8bd1\u7684\u81ea\u52a8\u5316\uff0c\u5b83\u53ef\u4ee5\u4e0e\u6e90\u8bed\u8a00\u540c\u65f6\u7ffb\u8bd1\uff0c\u5ef6\u8fdf\u65f6\u95f4\u53ea\u6709\u51e0\u79d2\u949f\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/simultaneous_translation/stacl/transformer_nist_wait_1","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_2","text":"\u8f93\u5165\u6587\u4ef6\u8def\u5f84\uff0c\u6a21\u578b\u5c06\u7ed9\u51fa\u5b83\u7684\u9884\u6d4b\uff1a \u8f93\u5165 \u8f93\u51fa [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#2transformer_nist_wait_1","text":"paddlepaddle >= 2.1.0 paddlehub >= 2.1.0","title":"2\u3001transformer_nist_wait_1 \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#3","text":"hub install transformer_nist_wait_1","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import paddlehub as hub from pinferencia import Server simultaneous_translation = hub . Module ( name = \"transformer_nist_wait_1\" ) def predict ( text : list ): for t in text : print ( f \"input: { t } \" ) result = simultaneous_translation . translate ( t ) print ( f \"model output: { result } \" ) service = Server () service . register ( model_name = \"simultaneous_translation\" , model = predict ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/simultaneous_translation/#_5","text":"curl Python Requests \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/simultaneous_translation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [ \"\u4ed6\", \"\u4ed6\u8fd8\", \"\u4ed6\u8fd8\u8bf4\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\", \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\", ] }' \u54cd\u5e94 \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting . \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import requests requests . post ( url = \"http://localhost:8000/v1/models/simultaneous_translation/predict\" , json = { \"data\" : [ \"\u4ed6\" , \"\u4ed6\u8fd8\" , \"\u4ed6\u8fd8\u8bf4\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\" , \"\u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002\" , ]} ) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py # \u5728server\u9875\u9762 input: \u4ed6 model output: he input: \u4ed6\u8fd8 model output: he also input: \u4ed6\u8fd8\u8bf4 model output: he also said input: \u4ed6\u8fd8\u8bf4\u73b0\u5728 model output: he also said that input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728 model output: he also said that he input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a model output: he also said that he is input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9 model output: he also said that he is making input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00 model output: he also said that he is making preparations input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae model output: he also said that he is making preparations for input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa model output: he also said that he is making preparations for this input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392 model output: he also said that he is making preparations for this meeting input: \u4ed6\u8fd8\u8bf4\u73b0\u5728\u6b63\u5728\u4e3a\u8fd9\u4e00\u4f1a\u8bae\u4f5c\u51fa\u5b89\u6392\u3002 model output: he also said that he is making preparations for this meeting .","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/","text":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f \u00b6 ERNIE-GEN \u662f\u9762\u5411\u751f\u6210\u4efb\u52a1\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u9996\u6b21\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165 span-by-span \u751f\u6210\u4efb\u52a1\uff0c\u8ba9\u6a21\u578b\u6bcf\u6b21\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u8bed\u4e49\u5b8c\u6574\u7684\u7247\u6bb5\u3002\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u901a\u8fc7\u586b\u5145\u5f0f\u751f\u6210\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u673a\u5236\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002\u6b64\u5916, ERNIE-GEN \u91c7\u6837\u591a\u7247\u6bb5-\u591a\u7c92\u5ea6\u76ee\u6807\u6587\u672c\u91c7\u6837\u7b56\u7565, \u589e\u5f3a\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u7684\u5173\u8054\u6027\uff0c\u52a0\u5f3a\u4e86\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4ea4\u4e92\u3002ernie_gen_poetry \u91c7\u7528\u5f00\u6e90\u8bd7\u6b4c\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u8bd7\u6b4c\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry \u6837\u672c\u7ed3\u679c\u793a\u4f8b \u00b6 \u8f93\u5165 \u8f93\u51fa [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427 \u5148\u51b3\u6761\u4ef6 \u00b6 1\u3001\u73af\u5883\u4f9d\u8d56 \u00b6 \u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879 2\u3001ernie_gen_poetry \u4f9d\u8d56 \u00b6 paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp 3\u3001\u4e0b\u8f7d\u6a21\u578b \u00b6 hub install ernie_gen_poetry \u670d\u52a1\u6a21\u578b \u00b6 \u5b89\u88c5 Pinferencia \u00b6 \u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\" \u521b\u5efaapp.py \u00b6 \u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting... \u6d4b\u8bd5\u670d\u52a1 \u00b6 UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f TEXT_TO_TEXT \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' \u54cd\u5e94 { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6587\u5b57\u751f\u6210"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#_1","text":"ERNIE-GEN \u662f\u9762\u5411\u751f\u6210\u4efb\u52a1\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u9996\u6b21\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u52a0\u5165 span-by-span \u751f\u6210\u4efb\u52a1\uff0c\u8ba9\u6a21\u578b\u6bcf\u6b21\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u8bed\u4e49\u5b8c\u6574\u7684\u7247\u6bb5\u3002\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u901a\u8fc7\u586b\u5145\u5f0f\u751f\u6210\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u673a\u5236\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002\u6b64\u5916, ERNIE-GEN \u91c7\u6837\u591a\u7247\u6bb5-\u591a\u7c92\u5ea6\u76ee\u6807\u6587\u672c\u91c7\u6837\u7b56\u7565, \u589e\u5f3a\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u7684\u5173\u8054\u6027\uff0c\u52a0\u5f3a\u4e86\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4ea4\u4e92\u3002ernie_gen_poetry \u91c7\u7528\u5f00\u6e90\u8bd7\u6b4c\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u8bd7\u6b4c\u3002 \u53c2\u8003\uff1a https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.2/modules/text/text_generation/ernie_gen_poetry","title":"\u6a21\u578b\u57fa\u672c\u4fe1\u606f"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#_2","text":"\u8f93\u5165 \u8f93\u51fa [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ] [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\" , \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\" , \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] \u73b0\u5728\u5c31\u6765\u8bd5\u8bd5\u5427","title":"\u6837\u672c\u7ed3\u679c\u793a\u4f8b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#_3","text":"","title":"\u5148\u51b3\u6761\u4ef6"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#1","text":"\u8bf7\u8bbf\u95ee \u4f9d\u8d56\u9879","title":"1\u3001\u73af\u5883\u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#2ernie_gen_poetry","text":"paddlepaddle >= 2.0.0 paddlehub >= 2.0.0 paddlenlp >= 2.0.0 pip3 install paddlenlp","title":"2\u3001ernie_gen_poetry \u4f9d\u8d56"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#3","text":"hub install ernie_gen_poetry","title":"3\u3001\u4e0b\u8f7d\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#_4","text":"","title":"\u670d\u52a1\u6a21\u578b"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#pinferencia","text":"\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 Pinferencia \u3002 pip install \"pinferencia[streamlit]\"","title":"\u5b89\u88c5 Pinferencia"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#apppy","text":"\u8ba9\u6211\u4eec\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u51fd\u6570\u4fdd\u5b58\u5230\u4e00\u4e2a\u6587\u4ef6 app.py \u4e2d\u5e76\u6dfb\u52a0\u4e00\u4e9b\u884c\u6765\u6ce8\u518c\u5b83\u3002 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import paddlehub as hub from pinferencia import Server , task text_generation = hub . Module ( name = \"ernie_gen_poetry\" ) def predict ( texts : list ) -> list : return text_generation . generate ( texts = texts , beam_width = 5 ) service = Server () service . register ( model_name = \"text_generation\" , model = predict , metadata = { \"task\" : task . TEXT_TO_TEXT }, ) \u8fd0\u884c\u670d\u52a1\uff0c\u7b49\u5f85\u5b83\u52a0\u8f7d\u6a21\u578b\u5e76\u542f\u52a8\u670d\u52a1\u5668\uff1a Only Backend Frontend and Backend $ uvicorn app:service --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [xxxxx] using statreload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. $ pinfer app:service --reload Pinferencia: Frontend component streamlit is starting... Pinferencia: Backend component uvicorn is starting...","title":"\u521b\u5efaapp.py"},{"location":"rc/how-to-guides/paddlepaddle/modules/text/text_generation/#_5","text":"UI curl Python Requests \u6253\u5f00http://127.0.0.1:8501\uff0c\u6a21\u677f TEXT_TO_TEXT \u4f1a\u81ea\u52a8\u9009\u4e2d\u3002 \u8bf7\u6c42 curl --location --request POST \\ 'http://127.0.0.1:8000/v1/models/text_generation/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": [\"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\", \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\"] }' \u54cd\u5e94 { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u521b\u5efa test.py \u3002 test.py 1 2 3 4 5 6 7 8 import requests response = requests . post ( url = \"http://localhost:8000/v1/models/text_generation/predict\" , json = { \"data\" : [ \"\u6614\u5e74\u65c5\u5357\u670d\uff0c\u59cb\u8bc6\u738b\u8346\u5dde\u3002\" , \"\u9ad8\u540d\u51fa\u6c49\u9634\uff0c\u7985\u9601\u8de8\u9999\u5c91\u3002\" ]} ) print ( response . json ()) \u8fd0\u884c\u811a\u672c\u5e76\u68c0\u67e5\u7ed3\u679c\u3002 $ python test.py { \"model_name\": \"text_generation\", \"data\": [ [ \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u4fef\u4ef0\u6210\u6625\u79cb\u3002\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u592b\u5b50\uff0c\u76f8\u9022\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u6211\u4e0e\u541b\u522b\uff0c\u98d8\u96f6\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u5404\u5728\", \"\u4e00\u89c1\u4fbf\u503e\u76d6\uff0c\u8bba\u4ea4\u66f4\u7ef8\u7f2a\u3002\u522b\u6765\u4e8c\u5341\u5e74\uff0c\u65e5\u6708\u5982\u5954\u6d41\u3002\u4eba\u751f\u4f1a\u5408\u96be\uff0c\u51b5\u4e43\u5c81\u6708\u9052\u3002\u541b\u5bb6\u5bcc\u6587\u53f2\uff0c\u6211\u8001\u65e0\u7530\u7574\u3002\u76f8\u9022\u4e0d\u76f8\u8bc6\uff0c\u5404\u5728\u5929\u4e00\u966c\u3002\u4eba\u751f\u767e\u5e74\u5185\uff0c\u805a\u6563\u5982\u6d6e\u6ca4\u3002\u51b5\u590d\u5404\u5f02\u4e61\uff0c\u98ce\u96e8\" ], [ \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6811\u9634\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u6797\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u67cf\u6b63\u68ee\u3002\", \"\u5730\u50fb\u65e0\u5c18\u5230\uff0c\u5c71\u9ad8\u89c1\u6c34\u6df1\u3002\u949f\u58f0\u4f20\u8fdc\u5bfa\uff0c\u5854\u5f71\u843d\u524d\u6797\u3002\u6b32\u95ee\u897f\u6765\u610f\uff0c\u5ead\u524d\u6709\u6842\u9634\u3002\" ] ] } \u66f4\u9177\u7684\u662f\uff0c\u8bbf\u95ee http://127.0.0.1:8501 \uff0c\u60a8\u5c06\u62e5\u6709\u4e00\u4e2a\u4ea4\u4e92\u5f0fUI\u3002 \u60a8\u53ef\u4ee5\u5728\u90a3\u91cc\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\uff01","title":"\u6d4b\u8bd5\u670d\u52a1"},{"location":"rc/how-to-guides/schema/","text":"\u5982\u4f55\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\uff1f \u00b6 \u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u8ba1\u7b97\u6570\u636e\u603b\u548c\u7684\u670d\u52a1\u3002 \u5b83\u63a5\u53d7\u7684\u8bf7\u6c42\u5185\u5bb9\u662f\uff1a [ 1 , 2 , 3 ] \u8fd4\u56de\u7684\u54cd\u5e94\u5185\u5bb9: 6 \u60a8\u5982\u4f55\u8ba9\u7528\u6237\u77e5\u9053\u60a8\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u662f\u4ec0\u4e48\u6837\u7684\uff1f \u5982\u679c\u60a8\u60f3\u81ea\u52a8\u9a8c\u8bc1\u6216\u8005\u89e3\u6790\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u8be5\u600e\u4e48\u529e\uff1f \u672c\u6587\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\u3002 Python 3 Type Hint \u00b6 \u4f60\u542c\u8bf4\u8fc7 python \u4e2d\u7684\u201c\u7c7b\u578b\u63d0\u793a\u201d\u5417\uff1f \u5982\u679c\u6ca1\u6709\uff0c\u60a8\u6700\u597d\u73b0\u5728\u5728 Python Typing \u4e0a\u67e5\u770b\u3002 \u4ece Python 3.5 \u5f00\u59cb\uff0cPython \u5f00\u59cb\u5728\u51fd\u6570\u5b9a\u4e49\u4e2d\u652f\u6301\u7c7b\u578b\u63d0\u793a\u3002 \u60a8\u53ef\u4ee5\u58f0\u660e\u53c2\u6570\u7684\u7c7b\u578b\u5e76\u8fd4\u56de\u3002 Pinferencia \u4f7f\u7528\u51fd\u6570\u7684\u7c7b\u578b\u63d0\u793a\u6765\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\u3002 \u6240\u4ee5\uff0c\u4f60\u4e0d\u9700\u8981\u5b66\u4e60\u53e6\u4e00\u79cd\u683c\u5f0f\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 python\u3002 \u5e76\u975e\u6240\u6709\u7c7b\u578b\u63d0\u793a\u90fd\u53d7\u652f\u6301! \u5e76\u975e\u6240\u6709 python \u4e2d\u7684\u7c7b\u578b\u63d0\u793a\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49schema\u3002 \u7c7b\u578b\u63d0\u793a\u9700\u8981\u80fd\u591f\u5728 json schema\u4e2d\u6b63\u786e\u8868\u793a\u3002 Dummy \u670d\u52a1 \u00b6 \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a Dummy \u670d\u52a1\u6765\u5411\u60a8\u5c55\u793a\u4e00\u5207\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) \u542f\u52a8\u670d\u52a1\uff0c\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u5c06\u627e\u5230\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u793a\u4f8b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u8fd9\u91cc\u51fd\u6570\u53c2\u6570\u7684\u7c7b\u578b\u63d0\u793a list \u5c06\u7528\u4e8e\u5b9a\u4e49\u8bf7\u6c42\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u51fd\u6570\u8fd4\u56de\u7684\u7c7b\u578b\u63d0\u793a str \u5c06\u7528\u4e8e\u5b9a\u4e49\u54cd\u5e94\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u6c42\u548c\u670d\u52a1 \u00b6 \u73b0\u5728\u8ba9\u6211\u4eec\u56de\u5230\u672c\u6587\u5f00\u5934\u63d0\u5230\u7684\u670d\u52a1\uff0c\u4e00\u4e2a\u6c42\u548c\u670d\u52a1\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b [ 1 , 2 , 3 ] 6 \u8ba9\u6211\u4eec\u91cd\u5199\u4e00\u4e0b Dummy \u670d\u52a1\u3002 Python 3.6\u53ca\u4ee5\u4e0a Python 3.9\u53ca\u4ee5\u4e0a dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) \u73b0\u5728\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u793a\u4f8b\u5c06\u662f\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } \u9664\u4e86\u663e\u793a schema \u4e4b\u5916\uff0c Pinferencia \u8fd8\u9a8c\u8bc1\u5e76\u5c1d\u8bd5\u5c06\u6570\u636e\u89e3\u6790\u4e3a\u6240\u9700\u7684\u7c7b\u578b\u3002 \u8ba9\u6211\u4eec\u5728\u540e\u7aef\u6587\u6863\u9875\u9762\u4e0a\u8bd5\u7528 API\u3002 \u6b63\u5e38\u6570\u636e \u7c7b\u578b\u9519\u8bef\u6570\u636e \u9519\u8bef\u6570\u636e \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u5c06\u8bf7\u6c42\u4e2d\u7684\u6570\u5b57\u4e4b\u4e00\u66f4\u6539\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\u3002 \u8be5\u6570\u5b57\u5c06\u6839\u636e schema \u81ea\u52a8\u8f6c\u6362\u4e3a\u6574\u6570\u3002 \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u53d1\u5e03\u4e00\u4e9b\u65e0\u6548\u7684\u6570\u636e\u7c7b\u578b\uff0c\u60a8\u5c06\u6536\u5230\u4e00\u4e2a 422 \u9519\u8bef \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] } \u590d\u6742\u7684Schema \u00b6 \u5728 pydantic \u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u590d\u6742\u7684schema\u3002 \u8ba9\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u670d\u52a1\u63a5\u6536\u5230\u4e2a\u4eba\u4fe1\u606f\uff1a \u8bf7\u6c42 [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u7b80\u5355\u7684\u6b22\u8fce\u95ee\u5019\u3002 \u54cd\u5e94 \"Hello, Will! Hello, Elise!\" \u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e0b\u8fd9\u4e2a\u670d\u52a1: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) \u73b0\u5728\u542f\u52a8\u670d\u52a1\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u4f1a\u53d1\u73b0\u8bf7\u6c42\u548c\u54cd\u5e94\u793a\u4f8b\u5982\u4e0b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u4efb\u52a1\u5b8c\u6210 \u00b6 \u60a8\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema\u3002 \u60a8\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u591a\u60a8\u611f\u5174\u8da3\u7684 Schema\u3002 \u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema"},{"location":"rc/how-to-guides/schema/#schema","text":"\u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u8ba1\u7b97\u6570\u636e\u603b\u548c\u7684\u670d\u52a1\u3002 \u5b83\u63a5\u53d7\u7684\u8bf7\u6c42\u5185\u5bb9\u662f\uff1a [ 1 , 2 , 3 ] \u8fd4\u56de\u7684\u54cd\u5e94\u5185\u5bb9: 6 \u60a8\u5982\u4f55\u8ba9\u7528\u6237\u77e5\u9053\u60a8\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u662f\u4ec0\u4e48\u6837\u7684\uff1f \u5982\u679c\u60a8\u60f3\u81ea\u52a8\u9a8c\u8bc1\u6216\u8005\u89e3\u6790\u8bf7\u6c42\u548c\u54cd\u5e94\u6b63\u6587\u8be5\u600e\u4e48\u529e\uff1f \u672c\u6587\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\u3002","title":"\u5982\u4f55\u5b9a\u4e49\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94schema\uff1f"},{"location":"rc/how-to-guides/schema/#python-3-type-hint","text":"\u4f60\u542c\u8bf4\u8fc7 python \u4e2d\u7684\u201c\u7c7b\u578b\u63d0\u793a\u201d\u5417\uff1f \u5982\u679c\u6ca1\u6709\uff0c\u60a8\u6700\u597d\u73b0\u5728\u5728 Python Typing \u4e0a\u67e5\u770b\u3002 \u4ece Python 3.5 \u5f00\u59cb\uff0cPython \u5f00\u59cb\u5728\u51fd\u6570\u5b9a\u4e49\u4e2d\u652f\u6301\u7c7b\u578b\u63d0\u793a\u3002 \u60a8\u53ef\u4ee5\u58f0\u660e\u53c2\u6570\u7684\u7c7b\u578b\u5e76\u8fd4\u56de\u3002 Pinferencia \u4f7f\u7528\u51fd\u6570\u7684\u7c7b\u578b\u63d0\u793a\u6765\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\u3002 \u6240\u4ee5\uff0c\u4f60\u4e0d\u9700\u8981\u5b66\u4e60\u53e6\u4e00\u79cd\u683c\u5f0f\uff0c\u4f60\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528 python\u3002 \u5e76\u975e\u6240\u6709\u7c7b\u578b\u63d0\u793a\u90fd\u53d7\u652f\u6301! \u5e76\u975e\u6240\u6709 python \u4e2d\u7684\u7c7b\u578b\u63d0\u793a\u90fd\u53ef\u4ee5\u7528\u6765\u5b9a\u4e49schema\u3002 \u7c7b\u578b\u63d0\u793a\u9700\u8981\u80fd\u591f\u5728 json schema\u4e2d\u6b63\u786e\u8868\u793a\u3002","title":"Python 3 Type Hint"},{"location":"rc/how-to-guides/schema/#dummy","text":"\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a Dummy \u670d\u52a1\u6765\u5411\u60a8\u5c55\u793a\u4e00\u5207\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 dummy.py from pinferencia import Server service = Server () def dummy ( data : list ) -> str : return data service . register ( model_name = \"dummy\" , model = dummy ) \u542f\u52a8\u670d\u52a1\uff0c\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u5c06\u627e\u5230\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u793a\u4f8b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"string\" ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" } \u8fd9\u91cc\u51fd\u6570\u53c2\u6570\u7684\u7c7b\u578b\u63d0\u793a list \u5c06\u7528\u4e8e\u5b9a\u4e49\u8bf7\u6c42\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002 \u51fd\u6570\u8fd4\u56de\u7684\u7c7b\u578b\u63d0\u793a str \u5c06\u7528\u4e8e\u5b9a\u4e49\u54cd\u5e94\u6b63\u6587\u4e2d\u7684 data \u5b57\u6bb5\u3002","title":"Dummy \u670d\u52a1"},{"location":"rc/how-to-guides/schema/#_1","text":"\u73b0\u5728\u8ba9\u6211\u4eec\u56de\u5230\u672c\u6587\u5f00\u5934\u63d0\u5230\u7684\u670d\u52a1\uff0c\u4e00\u4e2a\u6c42\u548c\u670d\u52a1\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b [ 1 , 2 , 3 ] 6 \u8ba9\u6211\u4eec\u91cd\u5199\u4e00\u4e0b Dummy \u670d\u52a1\u3002 Python 3.6\u53ca\u4ee5\u4e0a Python 3.9\u53ca\u4ee5\u4e0a dummy.py from typing import List from pinferencia import Server service = Server () def dummy ( data : List [ int ]) -> int : return data service . register ( model_name = \"dummy\" , model = dummy ) dummy.py from pinferencia import Server service = Server () def dummy ( data : list [ int ]) -> int : return sum ( data ) service . register ( model_name = \"dummy\" , model = dummy ) \u73b0\u5728\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u793a\u4f8b\u5c06\u662f\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 0 ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : 0 } \u9664\u4e86\u663e\u793a schema \u4e4b\u5916\uff0c Pinferencia \u8fd8\u9a8c\u8bc1\u5e76\u5c1d\u8bd5\u5c06\u6570\u636e\u89e3\u6790\u4e3a\u6240\u9700\u7684\u7c7b\u578b\u3002 \u8ba9\u6211\u4eec\u5728\u540e\u7aef\u6587\u6863\u9875\u9762\u4e0a\u8bd5\u7528 API\u3002 \u6b63\u5e38\u6570\u636e \u7c7b\u578b\u9519\u8bef\u6570\u636e \u9519\u8bef\u6570\u636e \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ 1 , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u5c06\u8bf7\u6c42\u4e2d\u7684\u6570\u5b57\u4e4b\u4e00\u66f4\u6539\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\u3002 \u8be5\u6570\u5b57\u5c06\u6839\u636e schema \u81ea\u52a8\u8f6c\u6362\u4e3a\u6574\u6570\u3002 \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ \"1\" , 2 , 3 ] } { \"id\" : \"string\" , \"model_name\" : \"dummy\" , \"model_version\" : \"default\" , \"data\" : 6 } \u8ba9\u6211\u4eec\u53d1\u5e03\u4e00\u4e9b\u65e0\u6548\u7684\u6570\u636e\u7c7b\u578b\uff0c\u60a8\u5c06\u6536\u5230\u4e00\u4e2a 422 \u9519\u8bef \u8bf7\u6c42 \u54cd\u5e94 { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : 1 } { \"detail\" : [ { \"loc\" : [ \"body\" , \"data\" ], \"msg\" : \"value is not a valid list\" , \"type\" : \"type_error.list\" } ] }","title":"\u6c42\u548c\u670d\u52a1"},{"location":"rc/how-to-guides/schema/#schema_1","text":"\u5728 pydantic \u7684\u5e2e\u52a9\u4e0b\uff0c\u53ef\u4ee5\u5728 Pinferencia \u4e2d\u5b9a\u4e49\u590d\u6742\u7684schema\u3002 \u8ba9\u6211\u4eec\u5047\u8bbe\u4e00\u4e2a\u670d\u52a1\u63a5\u6536\u5230\u4e2a\u4eba\u4fe1\u606f\uff1a \u8bf7\u6c42 [ { \"name\" : \"Will\" , \"age\" : 23 , \"gender\" : \"male\" }, { \"name\" : \"Elise\" , \"age\" : 19 , \"gender\" : \"female\" } ] \u540c\u65f6\u8fd4\u56de\u4e00\u4e2a\u7b80\u5355\u7684\u6b22\u8fce\u95ee\u5019\u3002 \u54cd\u5e94 \"Hello, Will! Hello, Elise!\" \u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e0b\u8fd9\u4e2a\u670d\u52a1: welcome.py from typing import List from pydantic import BaseModel from pinferencia import Server class Person ( BaseModel ): name : str age : int gender : str service = Server () def welcome ( persons : List [ Person ]) -> str : message = \"\" for person in persons : message += \"Hello, \" + person . name + \"!\" return message service . register ( model_name = \"welcome\" , model = welcome ) \u73b0\u5728\u542f\u52a8\u670d\u52a1\u5e76\u8bbf\u95ee\u540e\u7aef\u6587\u6863\u9875\u9762\uff0c\u60a8\u4f1a\u53d1\u73b0\u8bf7\u6c42\u548c\u54cd\u5e94\u793a\u4f8b\u5982\u4e0b\uff1a \u8bf7\u6c42\u793a\u4f8b \u54cd\u5e94\u793a\u4f8b { \"id\" : \"string\" , \"parameters\" : {}, \"data\" : [ { \"name\" : \"string\" , \"age\" : 0 , \"gender\" : \"string\" } ] } { \"id\" : \"string\" , \"model_name\" : \"string\" , \"model_version\" : \"string\" , \"parameters\" : {}, \"data\" : \"string\" }","title":"\u590d\u6742\u7684Schema"},{"location":"rc/how-to-guides/schema/#_2","text":"\u60a8\u5df2\u7ecf\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528 Pinferencia \u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94 Schema\u3002 \u60a8\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u66f4\u591a\u60a8\u611f\u5174\u8da3\u7684 Schema\u3002 \u73a9\u5f97\u5f00\u5fc3\uff01","title":"\u4efb\u52a1\u5b8c\u6210"},{"location":"rc/introduction/overview/","text":"\u6b22\u8fce\u4f7f\u7528Pinferencia \u00b6 Pinferencia? \u00b6 \u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100% High\u8d77\u6765\uff01 \u00b6 \u6b22\u6b22\u4e50\u4e50\uff0c\u641e\u5b9aApp \u00b6 Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u8d70\u4e00\u4e2a\uff5e \u00b6 $ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u6982\u8ff0"},{"location":"rc/introduction/overview/#pinferencia","text":"","title":"\u6b22\u8fce\u4f7f\u7528Pinferencia"},{"location":"rc/introduction/overview/#pinferencia_1","text":"\u6ca1\u542c\u8bf4\u8fc7 Pinferencia \uff0c\u8fd9\u4e0d\u662f\u4f60\u7684\u9519\u3002\u4e3b\u8981\u6211\u7684\u5ba3\u4f20\u7ecf\u8d39\uff0c\u5b9e\u5728\u662f\u4e0d\u591f\u591a\u3002 \u4f60\u662f\u4e0d\u662f\u8bad\u7ec3\u4e86\u4e00\u5806\u6a21\u578b\uff0c\u7136\u800c\u522b\u4eba\u8c01\u7528\u90fd\u4e0d\u884c\u3002\u4e0d\u662f\u73af\u5883\u641e\u4e0d\u5b9a\uff0c\u5c31\u662fbug\u547d\u592a\u786c\u3002 \u4f60\u60f3: \u8981\u662f\u6211\u80fd\u6709\u4e2aAPI\uff0c\u8c01\u80fd\u4e0d\u9677\u5165\u6211\u7684\u7231\u3002\u4e0d\u7528\u5b89\u88c5\u4e0d\u7528\u7b49\u5f85\uff0c\u53d1\u4e2a\u8bf7\u6c42\u7ed3\u679c\u81ea\u5df1\u5230\u6765\u3002 \u53ef\u662f\u4e16\u4e0aAPI\u5343\u767e\u4e07\uff0c\u5374\u6ca1\u6709\u54ea\u4e2a\u6211\u80fd\u73a9\u5f97\u8f6c\u3002\u7528\u6765\u7528\u53bb\uff0c\u770b\u6765\u8fd8\u662f\u6211\u5fc3\u592a\u8f6f\uff0c\u6709\u4e9b\u4ea7\u54c1\u771f\u7684\u4e0d\u80fd\u60ef\u3002 \u6211\u591a\u60f3\u8fd9\u4e2a\u4e16\u754c\u53d8\u5f97\u7b80\u5355\uff0c\u6211\u7684\u6a21\u578b1\u5206\u949f\u5c31\u80fd\u4e0a\u7ebf\u3002\u7136\u800c\u73b0\u5b9e\u8fd9\u4e48\u6b8b\u9177\uff0c\u4e00\u5929\u4e24\u5929\u8fc7\u53bb\uff0c\u6211\u7684\u773c\u6cea\u54d7\u54d7\u6b62\u4e0d\u4f4f\u3002 \u5230\u5e95\u8c01\u80fd\u7ed9\u4e88\u6211\u8fd9\u4e2a\u6069\u8d50\u554a\uff0c\u770b\u6765\u53ea\u6709Pinferencia\u3002 $ pip install \"pinferencia[streamlit]\" ---> 100%","title":"Pinferencia?"},{"location":"rc/introduction/overview/#high","text":"","title":"High\u8d77\u6765\uff01"},{"location":"rc/introduction/overview/#app","text":"Scikit-Learn PyTorch Tensorflow HuggingFace Transformer Any Model Any Function app.py import joblib import uvicorn from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) For more details, please visit https://scikit-learn.org/stable/modules/model_persistence.html entrypoint is the function name of the model to perform predictions. Here the data will be sent to the predict function: model.predict(data) . app.py import torch import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) For more details, please visit https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf import uvicorn from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) For more details, please visit https://www.tensorflow.org/tutorials/keras/save_and_load app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from transformers import pipeline from pinferencia import Server vision_classifier = pipeline ( task = \"image-classification\" ) def predict ( data ): return vision_classifier ( images = data ) service = Server () service . register ( model_name = \"vision\" , model = predict ) app.py import uvicorn from pinferencia import Server # train your models class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py import uvicorn from pinferencia import Server # train your models def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u6b22\u6b22\u4e50\u4e50\uff0c\u641e\u5b9aApp"},{"location":"rc/introduction/overview/#_1","text":"$ uvicorn app:service --reload INFO: Started server process [xxxxx] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)","title":"\u8d70\u4e00\u4e2a\uff5e"},{"location":"rc/introduction/pinferencia-is-different/","text":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c? \u00b6 \u4e0d\u540c? \u00b6 \u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48? \u4e0d\u8981\u554a\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01 \u6a21\u578b\u5728\u4f60\u624b\u91cc\uff0c\u4f60\u7528 Python \u8bad\u7ec3\uff0c\u7528 Python \u9884\u6d4b\uff0c\u751a\u81f3\u5199\u4e86\u5f88\u591a\u590d\u6742\u7684\u4ee3\u7801\uff0c\u53bb\u89e3\u51b3\u56f0\u96be\u53c8\u72ec\u7279\u7684\u9700\u6c42\u3002 \u800c\u5982\u4eca\uff0c\u4f60\u53c8\u8981\u591a\u5199\u591a\u5c11\u4ee3\u7801\uff0c\u591a\u505a\u591a\u5c11\u6539\u53d8\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u6a21\u578b\uff0c\u7528\u8fd9\u4e9b\u5de5\u5177\u6216\u8005\u5e73\u53f0\uff0c\u4ec5\u4ec5\u662f\u542f\u52a8\u4e00\u4e2aAPI\uff1f \u7b54\u6848\u662f\uff1a \u6570\u4e0d\u80dc\u6570 . \u6709\u4e86 Pinferencia \u00b6 \u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002 \u7b80\u5355\uff0c\u4e14\u5f3a\u5927 \u00b6 Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"Pinferencia \u6709\u4f55\u4e0d\u540c?"},{"location":"rc/introduction/pinferencia-is-different/#pinferencia","text":"","title":"Pinferencia \u6709\u4ec0\u4e48\u4e0d\u540c?"},{"location":"rc/introduction/pinferencia-is-different/#_1","text":"\u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u4e0d\u540c\u5e76\u4e0d\u91cd\u8981\uff0c\u91cd\u8981\u7684\u662f\u66f4\u76f4\u63a5\uff0c\u66f4\u7b80\u5355\uff0c\u66f4\u7b26\u5408\u4f60\u7684\u9884\u671f\u3002 \u4f60\u73b0\u5728\u662f\u5982\u4f55\u4e0a\u7ebf\u6a21\u578b\u7684? \u4f60\u662f\u4e0d\u662f\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\uff0c\u5199\u4ee3\u7801\uff0c\u4fdd\u5b58\u6587\u4ef6\uff0c\u4e3a\u4e86\u6ee1\u8db3\u90a3\u4e9b\u90e8\u7f72\u5de5\u5177\u7684\u8981\u6c42\u3002 \u5bf9\uff0c\u4f60\u8fd8\u82b1\u4e86\u5f88\u591a\u65f6\u95f4\u53bb\u7406\u89e3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5f88\u591a\u65f6\u95f4\u77e5\u9053\u600e\u4e48\u505a\u662f\u6b63\u786e\u7684\u3002 \u4e0d\u8fc7\uff0c\u529f\u592b\u4e0d\u8d1f\u6709\u5fc3\u4eba\uff0c\u4f60\u8fd8\u662f\u641e\u5b9a\u4e86\u3002 \u597d\u666f\u4e0d\u957f\uff0c\u8fc7\u4e86\u5927\u534a\u5e74\uff0c\u53c8\u6709\u65b0\u7684\uff0c\u66f4\u590d\u6742\u7684\u6a21\u578b\u8981\u90e8\u7f72\uff0c\u5929\u554a\uff0c\u600e\u4e48\u529e\uff1f \u4f60\u73b0\u5728\u5728\u60f3\u4ec0\u4e48?","title":"\u4e0d\u540c?"},{"location":"rc/introduction/pinferencia-is-different/#pinferencia_1","text":"\u4f60\u4e0d\u7528\u518d\u62c5\u5fc3\u8fd9\u4e9b\u3002\u4f60\u53ea\u9700\u8981\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u8fd8\u662f\u7528\u4f60\u81ea\u5df1\u7684\u4ee3\u7801\u3002 \u65e0\u6240\u8c13\u4f60\u7684\u6a21\u578b\u662f: PyTorch \u6a21\u578b Tensorflow \u6a21\u578b \u4efb\u4f55\u673a\u5668\u5b66\u4e60\u6a21\u578b \u4f60\u81ea\u5df1\u7684\u4ee3\u7801\uff0c\u7b97\u6cd5 \u751a\u81f3\u53ea\u662f\u4e00\u4e2a\u7b80\u7b80\u5355\u5355\u7684\u51fd\u6570 \u53ea\u9700\u8981\u7b80\u5355\u7684\u6ce8\u518c\uff0c Pinferencia \u5c31\u662f\u7acb\u523b\u4e0a\u7ebf\u5b83\u6765\u9884\u6d4b\uff0c\u5982\u4f60\u9884\u671f\uff0c\u6ca1\u6709\u60ca\u5413\u3002","title":"\u6709\u4e86 Pinferencia"},{"location":"rc/introduction/pinferencia-is-different/#_2","text":"Pinferencia \u81f4\u529b\u4e8e\u6210\u4e3a\u6700\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5de5\u5177\u3002 \u90e8\u7f72\u6a21\u578b\u4ece\u6765\u6ca1\u6709\u5982\u6b64\u7b80\u5355\u3002 \u5982\u679c\u4f60\u60f3\uff1a \u627e\u5230\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u4e0a\u7ebf\u4f60\u7684\u6a21\u578b \u7528\u6700\u5c11\u7684\u4ee3\u7801\uff0c\u638c\u63a7\u4f60\u7684\u670d\u52a1 \u8131\u79bb\u90a3\u4e9b\u91cd\u91cf\u7ea7\u3001\u800c\u5f88\u591a\u529f\u80fd\u4f60\u6839\u672c\u4e0d\u5728\u4e4e\u7684\u5de5\u5177\u548c\u5e73\u53f0 \u90a3\u4e48\uff0c\u4f60\u6765\u5bf9\u5730\u65b9\u4e86","title":"\u7b80\u5355\uff0c\u4e14\u5f3a\u5927"},{"location":"rc/reference/cli/","text":"\u547d\u4ee4\u884c\u754c\u9762 \u00b6 Pinfenrecia \u63d0\u4f9b\u547d\u4ee4 pinfer \u6765\u7b80\u5316\u542f\u52a8\u524d\u7aef\u548c\u540e\u7aef\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 pinfer --help \u67e5\u770b\u53ef\u7528\u9009\u9879\uff1a Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"\u547d\u4ee4\u884c\u754c\u9762"},{"location":"rc/reference/cli/#_1","text":"Pinfenrecia \u63d0\u4f9b\u547d\u4ee4 pinfer \u6765\u7b80\u5316\u542f\u52a8\u524d\u7aef\u548c\u540e\u7aef\u670d\u52a1\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 pinfer --help \u67e5\u770b\u53ef\u7528\u9009\u9879\uff1a Usage: pinfer [ OPTIONS ] APP Pinferencia Start backend server and/or frontend server. Argument APP: If mode is all or backend, app should be the backend uvicorn app. If mode is frontend, app should be the backend address Options: --mode TEXT Serving mode: all, frontend, or backend. [ default: all ] --backend-host TEXT Bind socket to this host. [ default: 127 .0.0.1 ] --backend-port INTEGER Bind socket to this port. [ default: 8000 ] --backend-workers INTEGER Number of worker processes. Defaults to the $WEB_CONCURRENCY environment variable if available, or 1 . Not valid with --reload. --backend-env-file PATH Environment configuration file. --backend-log-config PATH Logging configuration file. Supported formats: .ini, .json, .yaml. --backend-log-level [ critical | error | warning | info | debug | trace ] Log level. [ default: info ] --backend-root-path TEXT Set the ASGI 'root_path' for applications submounted below a given URL path. --backend-limit-concurrency INTEGER Maximum number of concurrent connections or tasks to allow, before issuing HTTP 503 responses. --backend-backlog INTEGER Maximum number of connections to hold in backlog --backend-limit-max-requests INTEGER Maximum number of requests to service before terminating the process. --backend-timeout-keep-alive INTEGER Close Keep-Alive connections if no new data is received within this timeout. [ default: 5 ] --ssl-keyfile TEXT SSL key file --ssl-certfile TEXT SSL certificate file --ssl-keyfile-password TEXT SSL keyfile password --ssl-version INTEGER SSL version to use ( see stdlib ssl module 's) [default: 17] --ssl-cert-reqs INTEGER Whether client certificate is required (see stdlib ssl module' s ) [ default: 0 ] --ssl-ca-certs TEXT CA certificates file --ssl-ciphers TEXT Ciphers to use ( see stdlib ssl module ' s ) [ default: TLSv1 ] --backend-app-dir TEXT Look for APP in the specified directory, by adding this to the PYTHONPATH. Defaults to the current working directory. [ default: . ] --frontend-base-url-path TEXT The base path for the URL where Streamlit should be served from. --frontend-port INTEGER The port where the server will listen for browser connections. [ default: 8501 ] --frontend-host TEXT The address where the server will listen for client and browser connections. [ default: 127 .0.0.1 ] --frontend-browser-server-address TEXT Internet address where users should point their browsers in order to connect to the app. Can be IP address or DNS name and path. [ default: localhost ] --frontend-script TEXT Path to the customized frontend script. --reload Enable backend auto-reload. [ default: False ] --help Show this message and exit.","title":"\u547d\u4ee4\u884c\u754c\u9762"},{"location":"rc/reference/frontend/requirements/","text":"\u8981\u6c42 \u00b6 \u8981\u5c06 Pinferencia \u7684\u524d\u7aef\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u7684\u9884\u6d4b\u529f\u80fd\u6709\u4e00\u4e9b\u8981\u6c42\u3002 \u6a21\u677f \u00b6 \u76ee\u524d\uff0c\u6a21\u677f\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e3b\u8981\u6709\u4e24\u5927\u7c7b\u3002\u5176\u5b83\u7c7b\u578b\u7684\u8f93\u5165\u8f93\u51fa\uff08\u4f8b\u5982\u97f3\u9891\u548c\u89c6\u9891\uff09\uff0c\u4f1a\u5728\u540e\u7eed\u9646\u7eed\u652f\u6301\u3002 \u57fa\u672c\u6a21\u677f \u00b6 \u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u6587\u672c\u8f6c\u6587\u672c \u6587\u672c \u6587\u672c \u6587\u672c\u8f6c\u56fe\u7247 \u6587\u672c \u56fe\u7247 \u56fe\u7247\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u76f8\u673a\u8f93\u5165\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u76f8\u673a\u8f93\u5165\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u4fbf\u6377\u6a21\u677f \u00b6 \u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u7ffb\u8bd1 \u6587\u672c \u6587\u672c \u56fe\u50cf\u5206\u7c7b \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u98ce\u683c\u8f6c\u79fb \u56fe\u7247 \u56fe\u7247 \u8f93\u5165 \u00b6 \u6839\u636e\u8bf7\u6c42\u7684\u6a21\u5f0f\uff0c\u524d\u7aef\u53ef\u4ee5\u5c06\u8f93\u5165\u89e3\u6790\u4e3a\u5217\u8868\u6216\u7b80\u5355\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \uff01\uff01\uff01 \u4fe1\u606f\u201c\u5b9a\u4e49\u67b6\u6784\u201d \u5173\u4e8e\u5982\u4f55\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\uff0c\u8bf7\u8bbf\u95ee \u5982\u4f55\u5b9a\u4e49\u60a8\u7684\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u7684Schema? \u5982\u679c\u5c06\u8bf7\u6c42\u7684schema\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4ee3\u8868 base64 \u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u8868\u793a base64 \u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u8f93\u51fa \u00b6 \u5982\u679c\u5c06\u54cd\u5e94schena\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4e00\u4e2a\u6587\u672c\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u6587\u672c\u8f93\u51fa \u524d\u7aef\u5c06\u5c1d\u8bd5\u5c06\u6587\u672c\u8f93\u51fa\u89e3\u6790\u4e3a\u8868\u683c\u3001json \u6216\u7eaf\u6587\u672c\u3002 \u8868\u683c \u6587\u672c JSON \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a \u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u662f\u4e00\u4e2a\u5217\u8868 [ [ { \"a\" \uff1a 1 \uff0c \"b\" \uff1a 2 } \uff0c { \"a\" \uff1a 3 \uff0c \"b\" \uff1a 4 } \uff0c { \"a\" \uff1a 5 \uff0c \"b\" \uff1a 6 } ] ] \u6216\u8005 json title=\"\u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u4e0d\u662f\u5217\u8868\" [ {\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6} ] \u5b83\u5c06\u663e\u793a\u4e3a\u8868\u683c\u3002 \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u662f\u4e00\u4e2a\u5217\u8868\" [ \"Text output.\" ] or ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u4e0d\u662f\u5217\u8868\" \"Text output.\" \u5b83\u5c06\u663e\u793a\u4e3a\u6587\u672c\u3002 \u6240\u6709\u5176\u4ed6\u683c\u5f0f\u7684\u8f93\u51fa\u90fd\u5c06\u663e\u793a\u4e3a JSON\u3002 \u4f8b\u5982\uff0c [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] \u6216\u8005 { \"a\" : 1 , \"b\" : 2 }","title":"\u524d\u7aef\u4f7f\u7528\u6761\u4ef6"},{"location":"rc/reference/frontend/requirements/#_1","text":"\u8981\u5c06 Pinferencia \u7684\u524d\u7aef\u7528\u4e8e\u60a8\u7684\u6a21\u578b\uff0c\u60a8\u7684\u6a21\u578b\u7684\u9884\u6d4b\u529f\u80fd\u6709\u4e00\u4e9b\u8981\u6c42\u3002","title":"\u8981\u6c42"},{"location":"rc/reference/frontend/requirements/#_2","text":"\u76ee\u524d\uff0c\u6a21\u677f\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e3b\u8981\u6709\u4e24\u5927\u7c7b\u3002\u5176\u5b83\u7c7b\u578b\u7684\u8f93\u5165\u8f93\u51fa\uff08\u4f8b\u5982\u97f3\u9891\u548c\u89c6\u9891\uff09\uff0c\u4f1a\u5728\u540e\u7eed\u9646\u7eed\u652f\u6301\u3002","title":"\u6a21\u677f"},{"location":"rc/reference/frontend/requirements/#_3","text":"\u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u6587\u672c\u8f6c\u6587\u672c \u6587\u672c \u6587\u672c \u6587\u672c\u8f6c\u56fe\u7247 \u6587\u672c \u56fe\u7247 \u56fe\u7247\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u76f8\u673a\u8f93\u5165\u8f6c\u6587\u672c \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247 \u76f8\u673a\u8f93\u5165\u8f6c\u56fe\u50cf \u56fe\u7247 \u56fe\u7247","title":"\u57fa\u672c\u6a21\u677f"},{"location":"rc/reference/frontend/requirements/#_4","text":"\u6a21\u677f \u8f93\u5165 \u8f93\u51fa \u7ffb\u8bd1 \u6587\u672c \u6587\u672c \u56fe\u50cf\u5206\u7c7b \u56fe\u7247 \u6587\u672c \u56fe\u50cf\u98ce\u683c\u8f6c\u79fb \u56fe\u7247 \u56fe\u7247","title":"\u4fbf\u6377\u6a21\u677f"},{"location":"rc/reference/frontend/requirements/#_5","text":"\u6839\u636e\u8bf7\u6c42\u7684\u6a21\u5f0f\uff0c\u524d\u7aef\u53ef\u4ee5\u5c06\u8f93\u5165\u89e3\u6790\u4e3a\u5217\u8868\u6216\u7b80\u5355\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \uff01\uff01\uff01 \u4fe1\u606f\u201c\u5b9a\u4e49\u67b6\u6784\u201d \u5173\u4e8e\u5982\u4f55\u5b9a\u4e49\u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u67b6\u6784\uff0c\u8bf7\u8bbf\u95ee \u5982\u4f55\u5b9a\u4e49\u60a8\u7684\u670d\u52a1\u7684\u8bf7\u6c42\u548c\u54cd\u5e94\u7684Schema? \u5982\u679c\u5c06\u8bf7\u6c42\u7684schema\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u4ee3\u8868 base64 \u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u80fd\u591f\u63a5\u53d7\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u8f93\u5165\u5c06\u662f\u8868\u793a base64 \u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002","title":"\u8f93\u5165"},{"location":"rc/reference/frontend/requirements/#_6","text":"\u5982\u679c\u5c06\u54cd\u5e94schena\u5b9a\u4e49\u4e3a\u5217\u8868\uff0c\u4f8b\u5982 List[str]\uff0c\u6216\u8005\u5c31\u662f list\uff1a predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u4e00\u4e2a\u6570\u636e\u5217\u8868\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4e00\u4e2a\u6587\u672c\u5217\u8868\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5fc5\u987b\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5b57\u7b26\u4e32\u5217\u8868\u3002 \u5426\u5219\uff0c predict \u51fd\u6570\u5fc5\u987b\u4ea7\u751f\u5355\u4e2a\u6570\u636e\u4f5c\u4e3a\u8f93\u51fa\u3002 \u5bf9\u4e8e\u6587\u672c\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u5bf9\u4e8e\u56fe\u50cf\u8f93\u51fa\uff0c\u8f93\u51fa\u5e94\u8be5\u662f\u4ee3\u8868base64\u7f16\u7801\u56fe\u50cf\u7684\u5355\u4e2a\u5b57\u7b26\u4e32\u3002 \u6587\u672c\u8f93\u51fa \u524d\u7aef\u5c06\u5c1d\u8bd5\u5c06\u6587\u672c\u8f93\u51fa\u89e3\u6790\u4e3a\u8868\u683c\u3001json \u6216\u7eaf\u6587\u672c\u3002 \u8868\u683c \u6587\u672c JSON \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a \u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u662f\u4e00\u4e2a\u5217\u8868 [ [ { \"a\" \uff1a 1 \uff0c \"b\" \uff1a 2 } \uff0c { \"a\" \uff1a 3 \uff0c \"b\" \uff1a 4 } \uff0c { \"a\" \uff1a 5 \uff0c \"b\" \uff1a 6 } ] ] \u6216\u8005 json title=\"\u5982\u679c\u54cd\u5e94\u7684\u683c\u5f0f\u4e0d\u662f\u5217\u8868\" [ {\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6} ] \u5b83\u5c06\u663e\u793a\u4e3a\u8868\u683c\u3002 \u5982\u679c\u8f93\u51fa\u7c7b\u4f3c\u4e8e\u4ee5\u4e0b\uff1a ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u662f\u4e00\u4e2a\u5217\u8868\" [ \"Text output.\" ] or ```json title=\"\u5982\u679c\u54cd\u5e94\u7684\u6a21\u5f0f\u4e0d\u662f\u5217\u8868\" \"Text output.\" \u5b83\u5c06\u663e\u793a\u4e3a\u6587\u672c\u3002 \u6240\u6709\u5176\u4ed6\u683c\u5f0f\u7684\u8f93\u51fa\u90fd\u5c06\u663e\u793a\u4e3a JSON\u3002 \u4f8b\u5982\uff0c [ [ { \"a\" : 1 , \"b\" : 2 }, 1 , \"a\" ] ] \u6216\u8005 { \"a\" : 1 , \"b\" : 2 }","title":"\u8f93\u51fa"},{"location":"rc/reference/handlers/","text":"Handlers \u00b6 BaseHandler \u00b6 BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002 PickleHandler \u00b6 \u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"Handlers"},{"location":"rc/reference/handlers/#handlers","text":"","title":"Handlers"},{"location":"rc/reference/handlers/#basehandler","text":"BaseHandler \u662f\u4e00\u4e2a\u62bd\u8c61\u57fa\u7840\u7c7b\uff0c\u4f60\u4e0d\u80fd\u76f4\u63a5\u7528\u5b83\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u5b83\u7684\u90e8\u5206\u63a5\u53e3\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u62d3\u5c55\u4f7f\u7528: BaseHandler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class BaseHandler ( abc . ABC ): def preprocess ( self , data : object , parameters : object = None ): return data # (1) def postprocess ( self , data : object , parameters : object = None ): return data # (2) def predict ( self , data : object ): if not getattr ( self , \"model\" , None ): raise Exception ( \"Model is not loaded.\" ) predict_func = ( # (3) getattr ( self . model , self . entrypoint ) if self . entrypoint else self . model ) return predict_func ( data ) @abc . abstractmethod def load_model ( self ): return NotImplemented # (4) \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a pre-processing \u5de5\u4f5c\u3002 \u9ed8\u8ba4\u4ee3\u7801\u5e76\u6ca1\u6709\u505a\u4efb\u4f55\u5904\u7406\uff0c\u4f60\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u903b\u8f91\u6765\u505a post-processing \u5de5\u4f5c\u3002 \u6839\u636e entrypoint \u548c model \u5bf9\u8c61\uff0c\u627e\u5230\u9884\u6d4b\u51fd\u6570\u3002\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 self.model \u83b7\u53d6, entrypoint \u53ef\u4ee5\u901a\u8fc7 self.entrypoint \u83b7\u53d6\u3002 \u4f60\u9700\u8981\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002 \u6a21\u578b\u8def\u5f84\u53ef\u4ee5\u901a\u8fc7 self.model_path \u83b7\u53d6\u3002","title":"BaseHandler"},{"location":"rc/reference/handlers/#picklehandler","text":"\u9ed8\u8ba4\u7684 handler \u662f PickleHandler . PickleHandler 1 2 3 4 5 6 7 8 class PickleHandler ( BaseHandler ): \"\"\"Pickle Handler for Models Saved through Pickle\"\"\" def load_model ( self ): if not getattr ( self , \"model_path\" , None ): raise Exception ( \"Model path not provided.\" ) with open ( self . model_path , \"rb\" ) as f : return pickle . load ( f )","title":"PickleHandler"},{"location":"rc/reference/models/machine-learning/","text":"\u673a\u5668\u5b66\u4e60\u6846\u67b6 \u00b6 \u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u5176\u5b83\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"rc/reference/models/machine-learning/#_1","text":"\u4e0b\u9762\u662f\u9488\u5bf9\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u5e38\u89c1\u6a21\u578b\u8f7d\u5165\u65b9\u6cd5\uff1a Scikit-Learn PyTorch Tensorflow \u4efb\u4f55\u6a21\u578b \u4efb\u610f\u51fd\u6570 app.py import joblib from pinferencia import Server # train your model model = \"...\" # or load your model model = joblib . load ( \"/path/to/model.joblib\" ) # (1) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , # (2) ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://scikit-learn.org/stable/modules/model_persistence.html entrypoint \u662f model \u6267\u884c\u9884\u6d4b\u7684\u51fd\u6570\u540d\u3002 \u8fd9\u91cc\u6570\u636e\u5c06\u88ab\u53d1\u9001\u5230 predict \u51fd\u6570\uff1a model.predict(data) \u3002 app.py import torch from pinferencia import Server # train your models model = \"...\" # or load your models (1) # from state_dict model = TheModelClass ( * args , ** kwargs ) model . load_state_dict ( torch . load ( PATH )) # entire model model = torch . load ( PATH ) # torchscript model = torch . jit . load ( 'model_scripted.pt' ) model . eval () service = Server () service . register ( model_name = \"mymodel\" , model = model , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://pytorch.org/tutorials/beginner/saving_loading_models.html app.py import tensorflow as tf from pinferencia import Server # train your models model = \"...\" # or load your models (1) # saved_model model = tf . keras . models . load_model ( 'saved_model/model' ) # HDF5 model = tf . keras . models . load_model ( 'model.h5' ) # from weights model = create_model () model . load_weights ( './checkpoints/my_checkpoint' ) loss , acc = model . evaluate ( test_images , test_labels , verbose = 2 ) service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u8bbf\u95ee https://www.tensorflow.org/tutorials/keras/save_and_load app.py from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) app.py from pinferencia import Server def model ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = model , )","title":"\u673a\u5668\u5b66\u4e60\u6846\u67b6"},{"location":"rc/reference/models/register/","text":"\u6ce8\u518c\u6a21\u578b \u00b6 \u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, ) \u53c2\u6570 \u00b6 \u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b \u4f8b\u5b50 \u00b6 Model Name \u00b6 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Model \u00b6 Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , ) Version\u540d\u79f0 \u00b6 \u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict Entrypoint \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. Metadata \u00b6 \u9ed8\u8ba4API \u00b6 Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } ) Kserve API \u00b6 Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002 Handler \u00b6 \u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler ) Load Now \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"rc/reference/models/register/#_1","text":"\u6ce8\u518c\u4e00\u4e2a\u6a21\u578b\u975e\u5e38\u7b80\u5355: 1 2 3 4 5 service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict\" , ) \u5982\u679c\u6211\u6709\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u6709\u591a\u4e2a\u7248\u672c\u5462? \u4f60\u53ef\u4ee5\u6ce8\u518c\u591a\u4e2a\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u53ef\u4ee5\u6709\u4e0d\u540c\u7684\u7248\u672c: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service . register ( model_name = \"my-model\" , model = my_model , entrypoint = \"predict\" , ) service . register ( model_name = \"my-model\" , model = my_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model , entrypoint = \"predict\" , ) service . register ( model_name = \"your-model\" , model = your_model_v1 , entrypoint = \"predict\" , version_name = \"v1, ) service . register ( model_name = \"your-model\" , model = your_model_v2 , entrypoint = \"predict\" , version_name = \"v2, )","title":"\u6ce8\u518c\u6a21\u578b"},{"location":"rc/reference/models/register/#_2","text":"\u53c2\u6570 \u7c7b\u4f3c \u9ed8\u8ba4\u503c\uff08\u5982\u6709\uff09 \u7ec6\u8282 model_name str \u6a21\u578b\u540d\u79f0 model object \u6a21\u578bPython\u5bf9\u8c61\uff0c\u6216\u8005\u6a21\u578b\u6587\u4ef6\u8def\u5f84 version_name str None \u7248\u672c\u540d\u79f0 entrypoint str None \u7528\u6765\u9884\u6d4b\u7684\u51fd\u6570\u540d\u79f0 metadata dict None \u6a21\u578b\u57fa\u7840\u4fe1\u606f handler object None Hanlder \u7c7b load_now bool True \u662f\u5426\u7acb\u523b\u8f7d\u5165\u6a21\u578b","title":"\u53c2\u6570"},{"location":"rc/reference/models/register/#_3","text":"","title":"\u4f8b\u5b50"},{"location":"rc/reference/models/register/#model-name","text":"1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model Name"},{"location":"rc/reference/models/register/#model","text":"Model Object Function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pinferencia import Server class MyModel : def predict ( self , data ): return sum ( data ) model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , entrypoint = \"predict ) 1 2 3 4 5 6 7 8 9 10 11 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , )","title":"Model"},{"location":"rc/reference/models/register/#version","text":"\u6ca1\u6709\u63d0\u4f9b\u7248\u672c\u540d\u7684\u6a21\u578b\u4f1a\u7528 default \u7248\u672c\u540d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server def add ( data ): return data [ 0 ] + data [ 1 ] def substract ( data ): return data [ 0 ] + data [ 1 ] service = Server () service . register ( model_name = \"mymodel\" , model = add , version_name = \"add\" , # (1) ) service . register ( model_name = \"mymodel\" , model = substract , version_name = \"substract\" , # (2) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict","title":"Version\u540d\u79f0"},{"location":"rc/reference/models/register/#entrypoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server class MyModel : def add ( self , data ): return data [ 0 ] + data [ 1 ] def substract ( self , data ): return data [ 0 ] - data [ 1 ] model = MyModel () service = Server () service . register ( model_name = \"mymodel\" , model = model , version_name = \"add\" , # (1) entrypoint = \"add\" , # (3) ) service . register ( model_name = \"mymodel\" , model = model , version_name = \"substract\" , # (2) entrypoint = \"substract\" , # (4) ) \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/add/predict \u9884\u6d4b\u5730\u5740\u5728 http://127.0.0.1/v1/models/mymodel/versions/substract/predict add \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b. substract \u51fd\u6570\u4f1a\u88ab\u7528\u6765\u9884\u6d4b.","title":"Entrypoint"},{"location":"rc/reference/models/register/#metadata","text":"","title":"Metadata"},{"location":"rc/reference/models/register/#api","text":"Pinferencia \u9ed8\u8ba4metadata\u67b6\u6784\u652f\u6301 platform \u548c device \u8fd9\u4e9b\u4fe1\u606f\u4ec5\u4f9b\u5c55\u793a\u3002 These are information for display purpose only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"Linux\" , \"device\" : \"CPU+GPU\" , } )","title":"\u9ed8\u8ba4API"},{"location":"rc/reference/models/register/#kserve-api","text":"Pinferencia \u540c\u65f6\u652f\u6301 Kserve API. \u5bf9\u4e8e Kserve V2, \u6a21\u578bmetadata\u652f\u6301: - platform - inputs - outputs inputs \u548c outputs \u4f1a\u51b3\u5b9a\u6a21\u578b\u6536\u5230\u7684\u6570\u636e\u548c\u8fd4\u56de\u7684\u6570\u636e\u7c7b\u578b. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pinferencia import Server def predict ( data ): return sum ( data ) service = Server ( api = \"kserve\" ) # (1) service . register ( model_name = \"mymodel\" , model = predict , metadata = { \"platform\" : \"mac os\" , \"inputs\" : [ { \"name\" : \"integers\" , # (2) \"datatype\" : \"int64\" , \"shape\" : [ 1 ], \"data\" : [ 1 , 2 , 3 ], } ], \"outputs\" : [ { \"name\" : \"sum\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, # (3) { \"name\" : \"product\" , \"datatype\" : \"int64\" , \"shape\" : - 1 , \"data\" : 6 }, ], } ) \u5982\u679c\u8981\u4f7f\u7528 Kserve API \u9700\u8981\u5728\u5b9e\u4f8b\u5316\u670d\u52a1\u65f6\u8bbe\u7f6e api=\"kserve\"\u3002 \u5982\u679c\u8bf7\u6c42\u5305\u542b\u591a\u7ec4\u6570\u636e\uff0c\u53ea\u6709 intergers \u6570\u636e\u4f1a\u88ab\u4f20\u9012\u7ed9\u6a21\u578b\u3002 \u8f93\u51fa\u6570\u636e\u4f1a\u88ab\u8f6c\u6362\u4e3a int64 \u3002 datatype \u5b57\u6bb5\u4ec5\u652f\u6301 numpy \u6570\u636e\u7c7b\u578b. \u5982\u679c\u7c7b\u578b\u8f6c\u6362\u5931\u8d25\uff0c\u54cd\u5e94\u91cc\u4f1a\u591a\u51fa error \u5b57\u6bb5\u3002","title":"Kserve API"},{"location":"rc/reference/models/register/#handler","text":"\u5173\u4e8eHandler\u7684\u7ec6\u8282\uff0c\u8bf7\u67e5\u770b Handlers . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pinferencia import Server from pinferencia.handlers import PickleHandler class MyPrintHandler ( PickleHandler ): def predict ( self , data ): print ( data ) return self . model . predict ( data ) def predict ( data ): return sum ( data ) service = Server () service . register ( model_name = \"mymodel\" , model = predict , handler = MyPrintHandler )","title":"Handler"},{"location":"rc/reference/models/register/#load-now","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import joblib from pinferencia import Server class JoblibHandler ( BaseHandler ): def load_model ( self ): return joblib . load ( self . model_path ) service = Server ( model_dir = \"/opt/models\" ) service . register ( model_name = \"mymodel\" , model = \"/path/to/model.joblib\" , entrypoint = \"predict\" , handler = JoblibHandler , load_now = True , )","title":"Load Now"},{"location":"rc/reference/restapi/","text":"REST API \u00b6 \u6982\u8ff0 \u00b6 Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1 \u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7 \u00b6 \u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002 \u9ed8\u8ba4 API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b Kserve API \u00b6 Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"REST API"},{"location":"rc/reference/restapi/#rest-api","text":"","title":"REST API"},{"location":"rc/reference/restapi/#_1","text":"Pinferencia \u6709\u4e24\u4e2a\u5185\u7f6e API\uff1a \u9ed8\u8ba4 API Kserve API from pinferencia import Server service = Server () # or service = Server ( api = \"default\" ) from pinferencia import Server service = Server ( api = \"kserve\" ) \u60a8\u73b0\u5728\u6b63\u5728\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\u5417\uff1f \u5982\u679c\u60a8\u8fd8\u4f7f\u7528\u5176\u4ed6\u6a21\u578b\u670d\u52a1\u5de5\u5177\uff0c\u4ee5\u4e0b\u662f\u8fd9\u4e9b\u5de5\u5177\u652f\u6301\u7684 Kserve API \u7248\u672c\uff1a \u540d\u79f0 API Pinferencia Kserve V1 & V2 TF Serving Kserve V1 TorchServe Kserve V1 or V2 Triton Kserve V2 KServe Kserve V1","title":"\u6982\u8ff0"},{"location":"rc/reference/restapi/#_2","text":"\u5982\u4f60\u770b\u5230\u7684 \u60a8\u53ef\u4ee5\u5728 Pinferencia \u548c\u5176\u4ed6\u5de5\u5177\u4e4b\u95f4\u5207\u6362\uff0c\u51e0\u4e4e\u65e0\u9700\u5728\u5ba2\u6237\u7aef\u66f4\u6539\u4ee3\u7801\u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u548c\u5ba2\u6237\u7aef\u6784\u5efa\uff0c\u7136\u540e\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u5176\u4ed6\u5de5\u5177\u3002 \u60a8\u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5c06 Pinferencia \u4e0e\u5177\u6709\u76f8\u540c API \u96c6\u7684\u5176\u4ed6\u5de5\u5177\u4e00\u8d77\u4f7f\u7528\u3002 \u5982\u679c\u60a8\u8981\u4ece Kserve V1 \u5207\u6362\u5230 Kserve V2\uff0c\u5e76\u4e14\u5728\u8fc7\u6e21\u671f\u95f4\u9700\u8981\u652f\u6301\u8fd9\u4e24\u8005\u7684\u670d\u52a1\u5668\uff0c\u90a3\u4e48\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528 Pinferencia \u3002 \u6240\u4ee5\uff0c\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7\u3002","title":"\u6ca1\u6709\u75db\u82e6\uff0c\u53ea\u6709\u6536\u83b7"},{"location":"rc/reference/restapi/#api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/predict POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/predict POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"\u9ed8\u8ba4 API"},{"location":"rc/reference/restapi/#kserve-api","text":"Path Method Summary /v1/healthz GET \u670d\u52a1\u5065\u5eb7 /v1/models GET \u6a21\u578b\u5217\u8868 /v1/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v1/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v1/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v1/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v1/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v1/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v1/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b /v2/healthz GET \u670d\u52a1\u5065\u5eb7 /v2/models GET \u6a21\u578b\u5217\u8868 /v2/models/{model_name} GET \u6a21\u578b\u7248\u672c\u5217\u8868 /v2/models/{model_name}/ready GET \u6a21\u578b\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/versions/{version_name}/ready GET \u6a21\u578b\u7248\u672c\u662f\u5426\u53ef\u7528 /v2/models/{model_name}/load POST \u52a0\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/load POST \u52a0\u8f7d\u7248\u672c /v2/models/{model_name}/unload POST \u5378\u8f7d\u6a21\u578b /v2/models/{model_name}/versions/{version_name}/unload POST \u5378\u8f7d\u7248\u672c /v2/models/{model_name}/infer POST \u6a21\u578b\u9884\u6d4b /v2/models/{model_name}/versions/{version_name}/infer POST \u6a21\u578b\u7248\u672c\u9884\u6d4b","title":"Kserve API"}]}